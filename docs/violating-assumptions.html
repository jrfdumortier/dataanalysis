<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Violating Assumptions | Data Analysis for Public Affairs</title>
  <meta name="description" content="6 Violating Assumptions | Data Analysis for Public Affairs" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Violating Assumptions | Data Analysis for Public Affairs" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Violating Assumptions | Data Analysis for Public Affairs" />
  
  
  

<meta name="author" content="Jerome Dumortier" />


<meta name="date" content="2021-03-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="binary-choice.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">dataanalysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="topics-overview.html"><a href="topics-overview.html"><i class="fa fa-check"></i><b>2</b> Topics Overview</a></li>
<li class="chapter" data-level="3" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>3</b> Introduction to R</a><ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#online-resources-and-help"><i class="fa fa-check"></i><b>3.1</b> Online Resources and Help</a></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#opening-rstudio"><i class="fa fa-check"></i><b>3.2</b> Opening RStudio</a><ul>
<li class="chapter" data-level="3.2.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#in-class-exercise-1"><i class="fa fa-check"></i><b>3.2.1</b> In-class Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#functions"><i class="fa fa-check"></i><b>3.3</b> Functions</a></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#data-management"><i class="fa fa-check"></i><b>3.4</b> Data Management</a><ul>
<li class="chapter" data-level="3.4.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#data-in-r"><i class="fa fa-check"></i><b>3.4.1</b> Data in R</a></li>
<li class="chapter" data-level="3.4.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#using-r-as-a-calculator"><i class="fa fa-check"></i><b>3.4.2</b> Using R as a Calculator</a></li>
<li class="chapter" data-level="3.4.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#creating-a-data-frame-from-scratch"><i class="fa fa-check"></i><b>3.4.3</b> Creating a Data Frame from Scratch</a></li>
<li class="chapter" data-level="3.4.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#in-class-exercise-2"><i class="fa fa-check"></i><b>3.4.4</b> In-class Exercise 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="basic-multivariate-regression.html"><a href="basic-multivariate-regression.html"><i class="fa fa-check"></i><b>4</b> Basic Multivariate Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="basic-multivariate-regression.html"><a href="basic-multivariate-regression.html#dummy-variables"><i class="fa fa-check"></i><b>4.1</b> Dummy Variables</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="binary-choice.html"><a href="binary-choice.html"><i class="fa fa-check"></i><b>5</b> Binary Choice</a><ul>
<li class="chapter" data-level="5.1" data-path="binary-choice.html"><a href="binary-choice.html#stuff"><i class="fa fa-check"></i><b>5.1</b> Stuff</a></li>
<li class="chapter" data-level="5.2" data-path="binary-choice.html"><a href="binary-choice.html#including-plots"><i class="fa fa-check"></i><b>5.2</b> Including Plots</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="violating-assumptions.html"><a href="violating-assumptions.html"><i class="fa fa-check"></i><b>6</b> Violating Assumptions</a><ul>
<li class="chapter" data-level="6.1" data-path="violating-assumptions.html"><a href="violating-assumptions.html#heteroscedasticity"><i class="fa fa-check"></i><b>6.1</b> Heteroscedasticity</a><ul>
<li class="chapter" data-level="6.1.1" data-path="violating-assumptions.html"><a href="violating-assumptions.html#detecting-heteroscedasticity"><i class="fa fa-check"></i><b>6.1.1</b> Detecting Heteroscedasticity</a></li>
<li class="chapter" data-level="6.1.2" data-path="violating-assumptions.html"><a href="violating-assumptions.html#correcting-heteroscedasticity"><i class="fa fa-check"></i><b>6.1.2</b> Correcting Heteroscedasticity</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="violating-assumptions.html"><a href="violating-assumptions.html#multicollinearity"><i class="fa fa-check"></i><b>6.2</b> Multicollinearity</a><ul>
<li class="chapter" data-level="6.2.1" data-path="violating-assumptions.html"><a href="violating-assumptions.html#variance-inflated-factors-vif"><i class="fa fa-check"></i><b>6.2.1</b> Variance Inflated Factors (VIF)</a></li>
<li class="chapter" data-level="6.2.2" data-path="violating-assumptions.html"><a href="violating-assumptions.html#examples"><i class="fa fa-check"></i><b>6.2.2</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="violating-assumptions.html"><a href="violating-assumptions.html#autocorrelation"><i class="fa fa-check"></i><b>6.3</b> Autocorrelation</a><ul>
<li class="chapter" data-level="6.3.1" data-path="violating-assumptions.html"><a href="violating-assumptions.html#durbin-watson-d-test"><i class="fa fa-check"></i><b>6.3.1</b> Durbin Watson d-Test</a></li>
<li class="chapter" data-level="6.3.2" data-path="violating-assumptions.html"><a href="violating-assumptions.html#breusch-godfrey-test"><i class="fa fa-check"></i><b>6.3.2</b> Breusch-Godfrey Test</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="violating-assumptions.html"><a href="violating-assumptions.html#other-issues-and-problems-with-data"><i class="fa fa-check"></i><b>6.4</b> Other Issues and Problems with Data</a></li>
<li class="chapter" data-level="6.5" data-path="violating-assumptions.html"><a href="violating-assumptions.html#exercises"><i class="fa fa-check"></i><b>6.5</b> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis for Public Affairs</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="violating-assumptions" class="section level1">
<h1><span class="header-section-number">6</span> Violating Assumptions</h1>
<p>This chapter introduces the detection and correction of problems with the estimation procedure due to the violation of the key assumptions necessary for the OLS model to work. The following R packages are needed for this chapter: <code>car</code>, <code>lmtest</code>, <code>orcutt</code>, and <code>sandwich</code>.</p>
<div id="heteroscedasticity" class="section level2">
<h2><span class="header-section-number">6.1</span> Heteroscedasticity</h2>
<p>A key assumption of the OLS model is homoscedasticity error terms. That is, the error variance is constant:
<span class="math display">\[Var(\epsilon_i) = \sigma^2\]</span>
With heteroscedasticity, the variance of the error term is not constant:
<span class="math display">\[Var(\epsilon_i) = \sigma_i^2\]</span>
For a bivariate regression model with heteroscedastic data, it can be shown that
<span class="math display">\[Var(\hat{\beta_1}) = \frac{\sum x_i^2 \sigma_i^2}{(\sum x_i^2)^2}\]</span>
This is different from the variance of the coefficient estimate under homoscedasticity:
<span class="math display">\[Var(\hat{\beta_1}) = \frac{\sigma^2}{\sum x_i^2}\]</span>
Unbiasedness of the OLS estimator is not affected but the variance of <span class="math inline">\(\beta_1\)</span> will be larger compared to other estimators. Note that the measure of <span class="math inline">\(R^2\)</span> is unaffected by heteroscedasticity. Homoscedasticity is needed to justify the t-test, F-test, and confidence intervals. The F-statistic does no longer have an F-distribution. In short, hypothesis tests on the <span class="math inline">\(\beta\)</span>-coefficients are no longer valid.</p>
<p>If <span class="math inline">\(\sigma_i^2\)</span> was known, the use of a Generalized Least Squares (GLS) model would be appropriate:
<span class="math display">\[y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i\]</span>
Dividing both sides by the known variance:
<span class="math display">\[\frac{y_i}{\sigma_i}=\beta_0 \cdot \frac{1}{\sigma_i}+\beta_1 \frac{x_i}{\sigma_i}+\frac{\epsilon_i}{\sigma_i}\]</span>
If <span class="math inline">\(\epsilon^*_i = \epsilon_i / \sigma_i\)</span>, then it can be shown that <span class="math inline">\(Var(\epsilon^*_i)=1\)</span>, i.e., constant. Under the usual OLS model:
<span class="math display">\[\sum_{i=1}^N e_i^2=\sum_{i=1}^N \left(y_i-\hat{\beta}_0+\hat{\beta}_1 \cdot x_i \right)^2\]</span>
Under GLS model:
<span class="math display">\[\sum_{i=1}^N w_i e_i^2= \sum_{i=1}^N w_i \left(y_i-\hat{\beta}_0+\hat{\beta}_1 \cdot x_i \right)^2\]</span>
That is, GLS minimizes the weighted sum of the residual squares. Since in reality, the variance of <span class="math inline">\(\sigma^2\)</span> is not known, other techniques have to be employed to obtain so-called heteroscedasticity-consistent (HC) standard errors. But first, two tests are introduced to detect heteroscedasticity.</p>
<div id="detecting-heteroscedasticity" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Detecting Heteroscedasticity</h3>
<p>Two test are presented to detect heteroscedasticity:</p>
<ul>
<li>Goldfeld-Quandt Test (1965)</li>
<li>Breusch-Pagan-Godfrey Test (1979)</li>
</ul>
<p>The steps necessary for the <strong>Goldfeld-Quandt Test</strong> are as follows:</p>
<ol style="list-style-type: decimal">
<li>Sort observations by ascending order of the dependent variable.</li>
<li>Pick <em>C</em> as the number of central observations to drop in the middle of the dependent variable.</li>
<li>Run two separate regression equations, i.e., with the “lower” and “upper” part.</li>
<li>Compute
<span class="math display">\[\lambda = \frac{RSS_2/df}{RSS_1/df}\]</span></li>
<li><span class="math inline">\(\lambda\)</span> follows an F-distribution.</li>
</ol>
<p>The Goldfeld-Quandt Test can be illustrated with <code>gqtestdata</code>. In a first step, the data is separated into two groups with <span class="math inline">\(C=6\)</span>. In a second step, both groups are used to run a regression. And lastly, <span class="math inline">\(\lambda\)</span> is calculated.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="violating-assumptions.html#cb10-1"></a>gqtestdata1         =<span class="st"> </span>gqtestdata[<span class="dv">1</span><span class="op">:</span><span class="dv">22</span>,]</span>
<span id="cb10-2"><a href="violating-assumptions.html#cb10-2"></a>gqtestdata2         =<span class="st"> </span>gqtestdata[<span class="dv">29</span><span class="op">:</span><span class="dv">50</span>,]</span>
<span id="cb10-3"><a href="violating-assumptions.html#cb10-3"></a>bhat1               =<span class="st"> </span><span class="kw">lm</span>(price_r<span class="op">~</span>sqft_r,<span class="dt">data=</span>gqtestdata1)</span>
<span id="cb10-4"><a href="violating-assumptions.html#cb10-4"></a>bhat2               =<span class="st"> </span><span class="kw">lm</span>(price_r<span class="op">~</span>sqft_r,<span class="dt">data=</span>gqtestdata2)</span>
<span id="cb10-5"><a href="violating-assumptions.html#cb10-5"></a>lambda              =<span class="st"> </span><span class="kw">sum</span>(bhat2<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>(bhat1<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)</span></code></pre></div>
<p>Of course, there is also a function in R called gqtest which simplifies the procedure.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="violating-assumptions.html#cb11-1"></a><span class="kw">library</span>(lmtest)</span>
<span id="cb11-2"><a href="violating-assumptions.html#cb11-2"></a>bhat                =<span class="st"> </span><span class="kw">lm</span>(price<span class="op">~</span>sqft,<span class="dt">data=</span>gqtestdata)</span>
<span id="cb11-3"><a href="violating-assumptions.html#cb11-3"></a><span class="kw">gqtest</span>(bhat,<span class="dt">fraction =</span> <span class="dv">6</span>)</span></code></pre></div>
<pre><code>## 
##  Goldfeld-Quandt test
## 
## data:  bhat
## GQ = 4.8709, df1 = 20, df2 = 20, p-value = 0.0004223
## alternative hypothesis: variance increases from segment 1 to 2</code></pre>
<p>In any case, the hypothesis of homoscedasticity is rejected for <code>gqtestdata</code>.</p>
<p>The <strong>Breusch-Pagan-Godfrey Test</strong> is an alternative and does not rely on choosing <em>C</em> as the number of central observations to be dropped. The steps include the following:</p>
<ol style="list-style-type: decimal">
<li>Run a regular OLS model and obtain the residuals.</li>
<li>Calculate
<span class="math display">\[\hat{\sigma}^2 = \frac{\sum_{i=1}^N e^2_i}{N}\]</span></li>
<li>Construct the variable <span class="math inline">\(p_i\)</span> as follows: <span class="math inline">\(p_i = e^2_i / \hat{\sigma}^2\)</span></li>
<li>Regress <span class="math inline">\(p_i\)</span> on the X’s as follows
<span class="math display">\[p_i = \alpha_0 + \alpha_1 \cdot x_{i1}+\alpha_2 \cdot x_{i2} + \dots\]</span></li>
<li>Obtain the explained sum of squares (ESS) and define <span class="math inline">\(\Theta = 0.5 \cdot ESS\)</span>. Then <span class="math inline">\(\Theta \sim \chi^2_{m-1}\)</span>.</li>
</ol>
<p>The much simpler procedure is to use the function <code>bptest()</code> in R.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="violating-assumptions.html#cb13-1"></a><span class="kw">library</span>(lmtest)</span>
<span id="cb13-2"><a href="violating-assumptions.html#cb13-2"></a>bhat                =<span class="st"> </span><span class="kw">lm</span>(price<span class="op">~</span>sqft,<span class="dt">data=</span>gqtestdata)</span>
<span id="cb13-3"><a href="violating-assumptions.html#cb13-3"></a><span class="kw">bptest</span>(bhat)</span></code></pre></div>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  bhat
## BP = 8.2672, df = 1, p-value = 0.004037</code></pre>
</div>
<div id="correcting-heteroscedasticity" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Correcting Heteroscedasticity</h3>
<p>To correct for heteroscedasticity, robust standard errors must be obtained.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="violating-assumptions.html#cb15-1"></a>bhat =<span class="st"> </span><span class="kw">lm</span>(price<span class="op">~</span>sqft,<span class="dt">data=</span>gqtestdata)</span>
<span id="cb15-2"><a href="violating-assumptions.html#cb15-2"></a><span class="kw">summary</span>(bhat)</span>
<span id="cb15-3"><a href="violating-assumptions.html#cb15-3"></a>vcov =<span class="st"> </span><span class="kw">vcovHC</span>(bhat)</span>
<span id="cb15-4"><a href="violating-assumptions.html#cb15-4"></a><span class="kw">coeftest</span>(bhat,<span class="dt">vcov.=</span>vcov)</span></code></pre></div>
<p>Note that there are multiple variations to calculate the standard error and thus, it is possible for slight variations among the results from different packages.
<span class="math display">\[Var(\hat{\beta}_1) = \frac{\sum_{i=1}^N (x_i-\bar{x})^2 e_i^2}{\sum_{i=1}^N (x_i-\bar{x})^2}\]</span>
The square root of the following equation is called heteroscedastic robust standard error:
<span class="math display">\[\widehat{Var}(\hat{\beta}_j) = \frac{\sum_{i=1}^N \hat{r}^2_{ij} e_i^2}{\sum_{i=1}^N (x_i-\bar{x})^2}\]</span>
Standard errors can be either larger or smaller. Note that in this example, we do not know whether heteroscedasticity is present or not.</p>
</div>
</div>
<div id="multicollinearity" class="section level2">
<h2><span class="header-section-number">6.2</span> Multicollinearity</h2>
<p>Multicollinearity describes the situation in which two or more independent variables are linearly related. Under perfect multicollinearity:
<span class="math display">\[\lambda_1 x_1 + \lambda_2 x_2 + \dots +\lambda_k x_k = 0\]</span>
where <span class="math inline">\(\lambda_i\)</span> are constants that are not all zero simultaneously. For example, consider <span class="math inline">\(x_1=\{8,12,15,17\}\)</span>, <span class="math inline">\(x_2=\{24,36,45,51\}\)</span>, and <span class="math inline">\(x_3=\{2,3,3.75,4.25\}\)</span>. In this case, <span class="math inline">\(\lambda_1=1\)</span>, <span class="math inline">\(\lambda_2=-1/5\)</span>, and <span class="math inline">\(\lambda_3=2\)</span>. Note, multicollinearity refers to linear relationships! Including a squared or cubed term is not an issue of multicollinearity. It can be shown that the variance of the estimator increases in the presence of multicollinearity. There are various indications that the data suffers from multicollinearity:</p>
<ul>
<li>High <span class="math inline">\(R^2\)</span> but few significant variables</li>
<li>Fail to reject the hypothesis for H<span class="math inline">\(_0\)</span>: <span class="math inline">\(\beta_i=0\)</span> based on t-values but rejection all slopes being simultaneously zero based on F-test.</li>
<li>High correlation among explanatory variables</li>
<li>Variation of statistically significant variables between models.</li>
</ul>
<div id="variance-inflated-factors-vif" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Variance Inflated Factors (VIF)</h3>
<p>Identifies possible correlation among multiple independent variables and not just two as in the case of a simple correlation coefficient. Consider the model:
<span class="math display">\[y_i = \beta_0 + \beta_k x_{ik} + \epsilon_i\]</span>
The estimated variances of the coefficient <span class="math inline">\(\beta_k\)</span> is written as
<span class="math display">\[Var(\beta_k)^* = \frac{\sigma^2}{\sum_{i=1}^N (x_{ik}-\bar{x}_k)^2}\]</span>
Without any multicollinearity, this variance is minimized. If some some independent variables are correlated with the independent variable <span class="math inline">\(k\)</span>, then
<span class="math display">\[Var(\beta_k) = \frac{\sigma^2}{\sum_{i=1}^N (x_{ik}-\bar{x}_k)^2} \cdot \frac{1}{1-R^2_k}\]</span>
where <span class="math inline">\(R^2_k\)</span> is the <span class="math inline">\(R^2\)</span> if variable <span class="math inline">\(x_k\)</span> is taken as the dependent variable. The VIF can be written as
<span class="math display">\[\frac{Var(\beta_k)}{Var(\beta_k)^*}=\frac{1}{1-R^2_k}\]</span>
If <span class="math inline">\(VIF=1\)</span>, then there is no relationship between the variable <span class="math inline">\(x_k\)</span> and the remaining independent variables. Otherwise, <span class="math inline">\(VIF&gt;1\)</span>. In general, the interpretation is as follows:</p>
<ul>
<li>VIF of 4 warrants attention</li>
<li>VIF of 10 indicates a serious problem.</li>
</ul>
</div>
<div id="examples" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Examples</h3>
<p>To illustrate the concept of multicollinearity, the data set from <code>nfl</code> is used (<a href="http://dx.doi.org/10.1016/j.econlet.2011.02.018">Berri et al. (2011)</a>). The first model includes total salary as the dependent variable and the following independent variables: prior season passing yards, pass attempts, experience (squared) in the league, draft round pick, veteran (more than 3 years in the league), pro bowl appearance, and facial symmetry.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="violating-assumptions.html#cb16-1"></a>bhat =<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(total)<span class="op">~</span>yards<span class="op">+</span>att<span class="op">+</span>exp<span class="op">+</span>exp2<span class="op">+</span>draft1<span class="op">+</span>draft2<span class="op">+</span>veteran<span class="op">+</span></span>
<span id="cb16-2"><a href="violating-assumptions.html#cb16-2"></a><span class="st">        </span>changeteam<span class="op">+</span>pbowlever<span class="op">+</span>symm,<span class="dt">data=</span>nfl)</span>
<span id="cb16-3"><a href="violating-assumptions.html#cb16-3"></a><span class="kw">summary</span>(bhat)</span></code></pre></div>
<p>After estimating the results, the function <code>vif()</code> from the package <code>car</code> is used:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="violating-assumptions.html#cb17-1"></a><span class="kw">vif</span>(bhat)</span></code></pre></div>
<pre><code>##      yards        att        exp       exp2     draft1     draft2    veteran changeteam  pbowlever 
##  32.547700  30.920282  39.889877  26.715342   1.621048   1.228091   5.253525   1.194254   1.581753 
##       symm 
##   1.056661</code></pre>
<p>The results indicate multicollinearity for <em>yards</em>, <em>att</em>, and experience. Passings yards and attempts may be correlated and thus, one of them (<em>att</em>) is dropped.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="violating-assumptions.html#cb19-1"></a>bhat =<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(total)<span class="op">~</span>yards<span class="op">+</span>exp<span class="op">+</span>exp2<span class="op">+</span>draft1<span class="op">+</span>draft2<span class="op">+</span>veteran<span class="op">+</span></span>
<span id="cb19-2"><a href="violating-assumptions.html#cb19-2"></a><span class="st">        </span>changeteam<span class="op">+</span>pbowlever<span class="op">+</span>symm,<span class="dt">data=</span>nfl)</span>
<span id="cb19-3"><a href="violating-assumptions.html#cb19-3"></a><span class="kw">summary</span>(bhat)</span></code></pre></div>
<p>This improves the estimation but experience (and its squared term) are still problematic:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="violating-assumptions.html#cb20-1"></a><span class="kw">vif</span>(bhat)</span></code></pre></div>
<pre><code>##      yards        exp       exp2     draft1     draft2    veteran changeteam  pbowlever       symm 
##   1.460849  39.339639  26.162804   1.616171   1.227479   5.253502   1.141435   1.569621   1.052906</code></pre>
<p>The last estimation removes experience and the VIF terms are now in the acceptable range.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="violating-assumptions.html#cb22-1"></a>bhat =<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(total)<span class="op">~</span>yards<span class="op">+</span>draft1<span class="op">+</span>draft2<span class="op">+</span>veteran<span class="op">+</span></span>
<span id="cb22-2"><a href="violating-assumptions.html#cb22-2"></a><span class="st">        </span>changeteam<span class="op">+</span>pbowlever<span class="op">+</span>symm,<span class="dt">data=</span>nfl)</span>
<span id="cb22-3"><a href="violating-assumptions.html#cb22-3"></a><span class="kw">summary</span>(bhat)</span></code></pre></div>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="violating-assumptions.html#cb23-1"></a><span class="kw">vif</span>(bhat)</span></code></pre></div>
<pre><code>##      yards     draft1     draft2    veteran changeteam  pbowlever       symm 
##   1.406241   1.653634   1.229459   1.976506   1.101988   1.406095   1.010855</code></pre>
<p>The important part is that the conclsion of the paper has not changed with regard to facial symmetry.</p>
</div>
</div>
<div id="autocorrelation" class="section level2">
<h2><span class="header-section-number">6.3</span> Autocorrelation</h2>
<p>The correlation of error terms is called autocorrelation. The issue usually arises if there is a time component in the data. Recall the main types of data available for research:</p>
<ul>
<li>Cross-sectional data (multiple observations at same time point)</li>
<li>Time series data (one variable observed over time)</li>
<li>Pooled data (multiple observations at different time points)</li>
<li>Panel data (same observations at different time points)</li>
</ul>
<p>There is a distinction between serial correlation and autocorrelation</p>
<ul>
<li>Serial correlation: Correlation between two series</li>
<li>Autocorrelation: Correlation with lagged variables</li>
</ul>
<p>The OLS estimator is still unbiased but there is no longer minimum variance since <span class="math inline">\(E(\epsilon_i \epsilon_j) \neq 0\)</span>. Unlikely autocorrelation for cross-sectional data except in the case of spatial auto-correlation. One cause of autocorrelation could be inertia in economic variables. For example, variables such as income, production, or employment increase after a recession. But there are a number of other reasons for autocorrelation.</p>
<p>Autocorrelation could be due to specification bias due to excluded variables or incorrect functional forms. In the case of excluded variables, assume that the correct equation is</p>
<p><span class="math display">\[q_{beef}=\beta_0+\beta_1 \cdot p_{beef}+\beta_2 \cdot p_{income} + \beta_3 \cdot p_{pork}+\epsilon_t\]</span>
The estimated equation is:
<span class="math display">\[q_{beef}=\beta_0+\beta_1 \cdot p_{beef}+\beta_2 \cdot p_{income}+\upsilon_t\]</span></p>
This results in a systematic patters of <span class="math inline">\(\upsilon_t\)</span>:
<span class="math display">\[\upsilon_t= \beta_3 \cdot p_{pork}+\epsilon_t\]</span>
Incorrect functional form

<p>Cobweb phenomenon
<span class="math display">\[\begin{equation*}
            supply_t = \beta_0 + \beta_1 \cdot p_{t-1}
        \end{equation*}\]</span>
Lags
<span class="math display">\[\begin{equation*}
            consumption_t = \beta_0 + \beta_1 \cdot income_t+\beta_3 \cdot consumption_{t-1}+\epsilon_t
        \end{equation*}\]</span></p>
<p>First-order Autoregressive Scheme
Consider the model:
<span class="math display">\[y_t=\beta_0+\beta_1 x_t + \upsilon_t\]</span></p>
<p>Assume the following form of <span class="math inline">\(\upsilon\)</span>:
<span class="math display">\[\upsilon_t = \rho \upsilon_{t-1} + \epsilon_t\]</span></p>
<pre><code>This is called a first-order autoregressive AR(1) scheme. An AR(2) would be written as
    \begin{equation*}
        \upsilon_t = \rho_1 \upsilon_{t-1} + \rho_2 \upsilon_{t-2} + \epsilon_t
    \end{equation*}</code></pre>
<p>This can be illustrated with simulated data. Consider the following model:
<span class="math display">\[y_t=1+0.8 \cdot x_t + \upsilon_t\]</span>
Assume the following form of <span class="math inline">\(\upsilon\)</span>:
<span class="math display">\[\upsilon_t = 0.7 \cdot \upsilon_{t-1} + \epsilon_t\]</span>
Procedure</p>
<ol style="list-style-type: decimal">
<li>Simulate the above model 100 times</li>
<li>Compare variance of coefficients under different two different methods: (1) OLS and (2) Cochrane-Orcutt</li>
</ol>
<div id="durbin-watson-d-test" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Durbin Watson d-Test</h3>
<p>The test statistic of the Durbin-Watson test is written as:
<span class="math display">\[d=\frac{\sum_{t=2}^N (e_t-e_{t-1})^2}{\sum_{t=1}^N e_t^2}\]</span>
Assumptions underlying the test are</p>
<ul>
<li>No intercept</li>
<li>AR(1) process, i.e., <span class="math inline">\(\upsilon_t = \rho \upsilon_{t-1} + \epsilon_t\)</span></li>
<li>No lagged independent variables</li>
</ul>
<p>Original papers derive lower (<span class="math inline">\(d_L\)</span>) and upper (<span class="math inline">\(d_U\)</span>) bounds, i.e., critical values, that depend on <span class="math inline">\(N\)</span> and <span class="math inline">\(k\)</span> only.</p>
<ul>
<li><span class="math inline">\(d \approx 2 \cdot (1-\rho)\)</span> and since <span class="math inline">\(-1 \leq \rho \leq 1\)</span>, we have <span class="math inline">\(0 \leq d \leq 4\)</span>.</li>
</ul>
<p>Rule of thumb indicates that <span class="math inline">\(d=2\)</span> signals no problems.</p>
</div>
<div id="breusch-godfrey-test" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Breusch-Godfrey Test</h3>
<p>Consider the following model <span class="math inline">\(y_t = \beta_0 + \beta_1 x_t + \upsilon_t\)</span> with the following error term structure:
<span class="math display">\[\upsilon_t = \rho_1 \upsilon_{t-1} + \rho_2 \upsilon_{t-2} + \dots + \rho_p \upsilon_{t-p} + \epsilon_t\]</span>
The null hypothesis for the test is expressed as follows:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: <span class="math inline">\(\rho_1 = \rho_2 = \dots = \rho_p =0\)</span></li>
</ul>
<p>When the following regression is executed:</p>
<p><span class="math display">\[\hat{\upsilon}_t = \alpha_0 + \alpha_1 x_t + \hat{\rho}_1 \hat{\upsilon}_{t-1} + \hat{\rho}_2 \hat{\upsilon}_{t-2} + \dots + \hat{\rho}_p \hat{\upsilon}_{t-p} + \epsilon_t\]</span>
Then
<span class="math display">\[(n-p)R^2 \sim \chi^2_p\]</span></p>

</div>
</div>
<div id="other-issues-and-problems-with-data" class="section level2">
<h2><span class="header-section-number">6.4</span> Other Issues and Problems with Data</h2>
<p>More serious problems than heteroscedasticity:</p>
<ul>
<li>Functional form mis-specification</li>
<li>Measurement error</li>
<li>Missing data: Estimating a standard regression model is not possible with missing values. All statistical software packages ignore missing data. Missing data is a minor problem if it is due to random error. Missing data can be problematic if it is systematically missing, e.g., missing education data for people with lower education</li>
<li>Non-random samples</li>
<li>Outliers</li>
</ul>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">6.5</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><p><em>WDI and Heteroscedasticity</em>: Using the data in <code>wdi</code>, estimate the following equation for the year 2018 and report the results:
<span class="math display">\[\begin{equation*}
fertrate = \beta_0 + \beta_1 \cdot gdp+ \beta_2 \cdot litrate
\end{equation*}\]</span>
The variable  represents fertility rate (birth per woman),  represents the GDP per capita in in real terms, and  is the literacy rate.</p></li>
<li><p><em>Indy Homes</em>: The data <code>indyhomes</code> contains home values of two ZIP codes in Indianapolis. The model estimates the home value (dependent variable) based on a set of independent variables. The variables <em>levels</em> and <em>garage</em> refers to the number of stories and garage spots, respectively. The remaining variables are self-explanatory.</p>
<ol style="list-style-type: lower-alpha">
<li>Create a dummy variable called <em>northwest</em> for the 46268 ZIP code.</li>
<li>Report and interpret the results of the following regression equation:
<span class="math display">\[\ln(price)=\beta_0+\beta_1 \cdot \ln(sqft)+\beta_2 \cdot northwest+\beta_3 \cdot \ln(lot)+\beta_4 \cdot bed+\beta_5 \cdot garage+\beta_6 \cdot levels+\beta_7 \cdot northwest \cdot levels\]</span></li>
<li>What is the expected home value of a house in the 46228 ZIP code area with the following characteristics: 1900 sqft, 0.65 acres lot, 3 bedrooms, 3 bathrooms, 2 garage spots, and 2 story.</li>
<li>Conduct a Breusch-Pagan-Godfrey test for heteroscedasticity. What do you conclude?</li>
<li>Estimate the above model with heteroscedasticity-consistent (HC) standard errors. What changes compared to the model from Part b?</li>
</ol></li>
<li><p><em>WDI and Multicollinearity</em>: Use the command <code>subset()</code> on the WDI data and to select the variables <em>fertrate</em>, <em>gdp</em>, <em>litrate</em>, <em>lifeexp</em>, and <em>mortrate</em> for the year 2015. Estimate the following model
<span class="math display">\[fertrate=\beta_0+\beta_1 \cdot gdp+\beta_2 \cdot litrate+\beta_3 \cdot lifeexp+\beta_4 \cdot mortrate\]</span>
Interpret the results. What do you conclude in terms of statistical significance and the value of <span class="math inline">\(R^2\)</span>? Use the function <code>vif</code> from the package <code>car</code>. What can you say about the issue of multicollinearity in this case? Correct the issue of multicollinearity by adjusting your model.</p></li>
</ol>

<p> In this exercise, you will estimate the per-capita pork demand as a function of pork prices and the prices of substitutes (beef and chicken) as well as real disposable income. Estimate the following equation and interpret the coefficients. Are the signs of the coefficients what you would expect?
<span class="math display">\[\begin{equation*}
\ln(q_{pork}) = \beta_0 + \beta_1 \cdot \ln(p_{pork}) + \beta_2 \cdot \ln(p_{chicken}) + \beta_3 \cdot \ln(p_{beef}) + \beta_4 \cdot \ln(rdi)
\end{equation*}\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="binary-choice.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrfdumortier/dataanalysis/edit/master/violatingassumptions.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/jrfdumortier/dataanalysis/blob/master/violatingassumptions.Rmd",
"text": null
},
"download": ["bookdownproj.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
