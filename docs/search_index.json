[["index.html", "Data Analysis for Public Affairs with R 1 Preface", " Data Analysis for Public Affairs with R Jerome Dumortier 2021-04-21 1 Preface This book serves as an introduction to data analysis for public affairs with R and RStudio. Most examples are drawn from public affairs and economics. Throughout the book, the following notation is used \\(variablename\\): Variable names which represent the columns in a data frame. dataset: Name of the data set in the file DataAnalysisPAData.RData. Exercise Name: Title of the exercise in the various chapters. package: Link to the package documentation. The example link is associated with Linear Models for Panel Data. "],["introduction.html", "2 Introduction", " 2 Introduction The purpose of this chapter is to provide an introduction and overview about the various aspects of uncertainty: Usefulness of learning about probability, statistics, and regression models Installing R and RStudio "],["overview.html", "2.1 Overview", " 2.1 Overview We are surrounded by probability and statistics on a daily basis because the world around us is uncertain. The purpose of probability theory and statistics is to explain and model stochastic processes such that predictions can be made. Probability and the application of statistics occur basically everywhere. For example, if you order something online, other products are suggested to you. Those suggestions are not random but are based on how you compare to other shoppers interested in similar items. Consider the following examples: Grades: If you take a university class then the grade you receive in the class is uncertain at the beginning of the semester. You may attach different probabilities associated with the various grades based on your knowledge about the material. 911 calls: While getting my graduate degree at Iowa State University, I was standing at a red light one morning which had a fire station down the road. Two fire trucks with their sirens on arrived at the red light and departed in opposite directions. Thus, two 911 calls must have come in at the same time requiring two trucks from the same station. The fire station has three trucks and as a public safety manager, you may be interested in the probability of more than three trucks being requested. Basketball free throws: Just because there are two outcomes does not mean that the probability is 50%/50%. Stephen Curry is the career leader in terms of free throw percentage (90.56%). Either he scores or misses, and his success rate is far from 50%. Polls: Especially before elections, polls are very popular to determine which candidate is favored. The polling results usually include a so-called margin of error which is an indicator of confidence in the results. The chapter on confidence intervals explains how the margin of error is calculated. Hurricanes: Projected pathways of Hurricanes, e.g., Sandy in 2012, produced by the National Hurricane Center (NHC) include so-called cones of uncertainty. The NHC defines the cone of uncertainty as follows: The cone represents the probable track of the center of a tropical cyclone, and is formed by enclosing the area swept out by a set of circles along the forecast track (at 12, 24, 36 hours, etc.). The size of each circle is set so that two-thirds of historical official forecast errors over a 5-year sample fall within the circle. Based on forecasts over the previous 5 years, the entire track of a tropical cyclone can be expected to remain within the cone roughly 60-70% of the time. It is important to note that the area affected by a tropical cyclone can extend well beyond the confines of the cone enclosing the most likely track area of the center. COVID-19: A recent example is the COVID-19 risk assessment chart developed by the Texas Medical Association. The risk categories can be thought of as probabilities of contracting COVID-19 for the activities listed. They also updated their chart to account for COVID Risks of Various Holiday Activities. If you are working and receiving retirement benefits, you are likely investing those in mutual fund. The saying do not put all your eggs in one basket applies in this context. The figure below shows the evolution of Vanguard 500 Index Fund Investor Shares (VFINX) and the Fidelity Select Retailing Portfolio (FSRPX). Although not perfectly, the funds generally move in the same direction. Scatter plot and indication of frequency of the daily returns of VFINX and FSRPX. Both graphs indicate a certain degree of positive association between the returns, i.e., if one of the mutual funds increases, the other tends to increase as well (and vice versa). This course can be subdivided into three large topics: probability, statistics, and regression. The basics of probability provide means for modeling populations, experiments, and any other random phenomena. You will be introduced to probability distributions that allow you to model random outcomes. Probability theory is also the foundation for statistics. Statistics allows us to learn something about the population based on a sample. Sampling distributions, confidence intervals, and hypothesis testing will be important concepts. The last part will cover regression analysis which states mathematical relationships among variables. For example, the price of a used car can be expressed as a function of model, year, mileage, and cylinders. To illustrate the difference between probability and statistics let us consider two buckets. The first bucket illustrates the concept of probability and the second bucket illustrates the concept of statistics. Suppose that in the first bucket, you have a bunch of balls of different colors: Green, black, and red. And you also know how many of each color are in the bucket. Probability theory tells you the expected number of green, red, and black balls in your hand after pulling out a bunch of balls from the bucket. It will tell you the likely distribution of colors in your hand. Statistics is different. Again, you have a bucket but you do not know what is in the bucket. Think about the content of the bucket as your population with unknown characteristics. To learn about the characteristics of the population, you pull out a sample from the bucket. Based on the distribution of colored balls in your hand, you can use statistics to say something of the characteristics of your population, that is, the content of the bucket in this case. "],["r-and-rstudio.html", "2.2 R and RStudio", " 2.2 R and RStudio Throughout the semester, we are going to use the software to conduct statistical analysis. We will use RStudio to interact with R and you can think of RStudio as a graphical user interface for R. R/RStudio has a similar data setup to Excel but instead of seeing the spreadsheet all the time, the spreadsheet with your data is in the background. R/RStudio requires that your columns be the variables and that the rows are your observations. For convenience, we are going to use Excel for data setup. For R/RStudio, there are great online resources: R/RStudio has several advantages over Excel. 2.2.1 Preparation for R/RStudio The next lecture will introduce you to the use of R and RStudio. We will use RStudio to interact with R and you can think of RStudio as a graphical user interface for R. To focus on the use of R and RStudio during the lecture, some easy preparatory steps are necessary for you to perform before class. Those are mostly related to installing R and RStudio and to load sample data into the software. With this document, you should have downloaded the small dataset and the R-script file . In preparation for the lecture, you will load into R and RStudio. 2.2.2 Installing R and RStudio You must first install R on your computer by doing the following: Go to The R Project for Statistical Computing and download the R version that is appropriate for your computer. This is either the base version if you have a Windows computer or the Latest release .pkg file if you are using Mac OS. Once you have downloaded the file, install R on your computer. Go to RStudio and download the RStudio version that is appropriate for your computer. Note that the various Installers for Supported Platforms are at the bottom of the page. Once you have downloaded the file, install RStudio on your computer. Note that we will only be using RStudio which runs R in the background. You cannot use RStudio without having R installed first. Throughout the lecture, I will be referring to R/RStudio. 2.2.3 Locating Files on your Computer To import data into R/RStudio, you must know (1) where files are located on your computer and (2) what the current working directory of R/RStudio is. On a windows computer, the directory where files are stored is like C:\\Users\\Jerome\\Documents\\R Lecture and for Mac OS it is similar to /Users/Jerome/R Lecture Think of the working directory as the folder on your computer in which R/RStudio is looking for files by default. After opening R/RStudio, you can type getwd() in the console window and R/RStudio will return the current working directory. Usually, you have project specific working directories. For this class, create a directory on your computer in which you are going to store all the files associated with this class. You should download the dataset honda.csv into the directory you have created for this class. You can use the command setwd() to change the R/RStudio to the new working directory. Note - and this is an oddity with R/RStudio - you have to replace the \\textbackslash'' with/ if you are a Windows user, i.e., use . Assuming the file is in the directory you have set above, use to load the file into R/RStudio (see ). The data should appear in the Environment tab on the right side in R/RStudio. It is important that you can do the steps described above before the lecture to alleviate any issues at the beginning. "],["introduction-to-r.html", "3 Introduction to R", " 3 Introduction to R Topics covered in this lecture Introduction to R and RStudio Data Management Plotting and Graphs with R Basic Statistics In-class exercises will be conducted throughout the Lecture. "],["online-resources-and-help.html", "3.1 Online Resources and Help", " 3.1 Online Resources and Help Very large user community for R. Google search for ``Some topic R usually leads quickly to the desired help. Here are the links to a few online tutorials UCLA Institute for Digital Research and Education StatMethods Statistical tools for high-throughput data analysis Two online resources will provide you the solution to the vast majority of your R questions. Getting on those websites is usually the result of a Google search. Statistical Data Analysis R: This resource contains the function manual for R/RStudio including all packages. Example for a function boxplot. The most helpful part are the examples at the bottom of the page. Stack Overflow: Resources for developers. For example, a Google search for r ggplot two y axis may give you the following result Note that all questions on Stack Overflow have to be accompanied by a re-creatable dataset. Besides many online resources, there are also two useful textbooks Applied Econometrics with R by Christian Kleiber and Achim Zeileis. Introductory Statistics with R by Peter Dalgaard. An additional online tutorial is Using R for Introductory Statistics by John Verzani. "],["opening-rstudio.html", "3.2 Opening RStudio", " 3.2 Opening RStudio Work in RStudio is done in four windows: Script Window This is were you type your R Script (.R) and where you execute commands. Comparable to do-file/editor in Stata. This window needs to be opened by File \\(\\Rightarrow\\) New File \\(\\Rightarrow\\) R Script. Console window Use of R interactively. Should only be used for quick calculations and not part of an analysis. Environment Lists all the variables, data frames, and user-created functions. It is tempting to use the ``Import Dataset function Dont. Plots/Packages/Help There is a base version of R that allows doing many calculations but the power of R comes through its packages. To use functions associated with a particular package, click ``Install in the packages window of RStudio and type in the name of the package. Or alternatively, use install.packages(\"ggplot2\") To use a package, you have to activate it by either checking the box in the window Packages or by including library(packagename). Those packages are updated on a regular basis by users. The \\# allows you to include comments in your script file that are not read by R. It is good practice to start any new script with clearing the memory using the command rm(list=ls()). Use the command get() to determine the current working directory or set the new working directory with the command setwd(), e.g., setwd(\"E:/\"). For file paths, replace \\(\\backslash\\) with \\(/\\). Next, you want to load all libraries necessary for your entire script file with the command library(). It is also good practice to save your R-script on a regular basis. The frontmatter, i.e., the top of a R-script file, could look as follows rm(list=ls()) load(&quot;DataAnalysisPAData.RData&quot;) library(openxlsx) 3.2.1 In-class Exercise 1 Create a R-script file with the following components: Two lines for the title and the date (use #) Clearing all current contents Setting the correct working directory This should be a folder to which you have downloaded all materials. Installing and loading the package openxlsx. "],["functions.html", "3.3 Functions", " 3.3 Functions At the core of R are functions that do things based on your input. The basic structure is object = functionname(argument1=value,argument2=value,...) The structure has the following components object: Output of the function will be assigned to object. functionname: Name of the system function. You can also create and use your own functions. More about this later. argument: Arguments are function specific. value: The value you want a particular argument to take. If a function is executed without an specific assignment, the output will be displayed in the console window. Before using a function, read the documentation. Many functions have default settings. Be aware of default values. In most cases, those defaults are set to values that satisfy most uses. For example, consider the help file for the function t.test. -t.test(x,y=NULL,...,mu=0,conf.level=0.95,...) For this function we have the following default values y=NULL mu=0 conf.level=0.95 "],["data-management.html", "3.4 Data Management", " 3.4 Data Management 3.4.1 Data in R The main data types which can appear in the Environment window of R are: Vectors preselection = seq(1788,2016,4) midterm = seq(by=4,to=2018,from=1790) Matrix somematrix = matrix(8,10,4) Only numerical values are allowed. Data frames By far, the most common data type in R. Comparable to an Excel sheet. More on this later. Lists Collection of objects from of various types. myfirstlist = list(preselection,midterm,somematrix) 3.4.2 Using R as a Calculator Entering heights of people and storing it in a vector named : height = c(71,77,70,73,66,69,73,73,75,76) Calculating the sum, product, natural log, mean, and (element-wise) squaring is done with the following commands: sum(height) prod(height) log(height) # Default is the natural log meanheight = mean(height) heightsq = height^2 Removing (i.e., deleting) unused elements: rm(heightsq,meanheight) 3.4.3 Creating a Data Frame from Scratch Data frames are the most commonly used tables in R/RStudio. They are similar to an Excel sheet. Column names represent the variables and rows represent observations. Column names must be unique and without spaces. Suggestion: Use only lower-case variable names and objects. studentid = 1:10 studentnames = c(&quot;Andrew&quot;,&quot;Linda&quot;,&quot;William&quot;,&quot;Daniel&quot;,&quot;Gina&quot;, &quot;Mick&quot;,&quot;Sonny&quot;,&quot;Wilbur&quot;,&quot;Elisabeth&quot;,&quot;James&quot;) students = data.frame(studentid,studentnames,height) rm(studentid,height,studentnames) 3.4.4 In-class Exercise 2 Create a data frame called students containing the following information: Name Economics English Mindy 80.0 52.5 Gregory 60.0 60.0 Shubra 95.0 77.5 Keith 77.5 30.0 Louisa 97.5 95.0 Notes: Use \\(name\\) as the column header for the students names. Once you have created the data frame, remove the unused vectors. "],["basic-multivariate-regression.html", "4 Basic Multivariate Regression", " 4 Basic Multivariate Regression Extension of the bivariate model to multivariate regression One dependent variable but multiple independent variables Topics associated with multivariate regression models covered in this lecture: Multicollinearity Dummy Variables Natural logarithm Functional forms Interaction Terms Bivariate regression model (one independent and one dependent variable) \\[\\begin{equation*} y = \\beta_0 + \\beta_1 x_1 + \\epsilon \\end{equation*}\\] Multivariate linear regression model (multiple independent variables) \\[\\begin{equation*} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + \\epsilon \\end{equation*}\\] Whether we consider the univariate or multivariate regression model, the objective is always to minimize the sum of squared errors, hence the name ordinary least square (OLS) model. The equation of a line can be determined using slope and intercept, we can write: \\[\\begin{equation*} E(y|x) = \\beta_0 + \\beta_1 x \\end{equation*}\\] A model with two independent variables (predictors) describes a plane. "],["dummy-variables.html", "4.1 Dummy Variables", " 4.1 Dummy Variables Dummy variables represent a single qualitative characteristic such as religion, gender, or nationality. Dummy variables are independent variables coded as either 0 or 1. For example, consider Example: Price (\\(Y_i\\)) of a car depending on miles (\\(X_i\\)) and AWD (\\(D_i\\)) \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 D_i + \\epsilon_i \\end{equation*}\\] with \\(D_i = 1\\) if AWD and \\(D_i = 0\\) if RWD. \\end{itemize} This regression can theoretically be separated into two single equations: Interpretation: "],["anova.html", "5 ANOVA", " 5 ANOVA Analysis of Variance (ANOVA) models (also know as Dummy Variable Regression models) are regressions with only dummy variables. An ANOVA model with two independent variables can be written as follows: \\[y_i = \\beta_0 + \\beta_1 \\cdot d_1 + \\beta_2 \\cdot d_2\\] where \\(d_1\\) and \\(d_2\\) are dummy variables. Consider the following model using the gss2018 data: \\[income = \\beta_0 + \\beta_1 \\cdot fulltime + \\beta_2 \\cdot government\\] where fulltime and government are dummy variables. That is, if \\(fulltime=1\\), then the person has a fulltime job. If \\(government=1\\), then the respondent works for the government. To distinguish j categories only j-1 dummy variables are needed. Otherwise, we have perfect multicollinearity. The category without a dummy variable is the base category. bhat = lm(income~fulltime+government,data=gss2018) summary(bhat) ## ## Call: ## lm(formula = income ~ fulltime + government, data = gss2018) ## ## Residuals: ## Min 1Q Median 3Q Max ## -51074 -23574 -6074 16426 152479 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17521 3377 5.189 2.71e-07 *** ## fulltime 33553 3691 9.091 &lt; 2e-16 *** ## government -6840 3640 -1.879 0.0606 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 39230 on 769 degrees of freedom ## Multiple R-squared: 0.099, Adjusted R-squared: 0.09665 ## F-statistic: 42.25 on 2 and 769 DF, p-value: &lt; 2.2e-16 The income for a person who is working neither for the government nor has a full time job makes on average $17,521. A person not working for the government but with a full time job is earning $17,521+$33,553=$51,074. Note that both dummy variables are statistically significant. Note that the R-squared is very low. "],["exercises.html", "5.1 Exercises", " 5.1 Exercises Consider the data in indyhomes. A real estate agent once mentioned that homes built in the eighties are of lower quality and thus, are cheaper. One possibility to asses this claim is with an ANOVA model. Add three new dummy variables to the data frame: eighties (equal to 1 if the home was built in any year between 1980 to 1989), northwest (equal to 1 if the home is in the 46268 ZIP code), and singlestory (equal to 1 if the home has one story). Estimate the following ANOVA model: \\[price = \\beta_0+\\beta_1 \\cdot d_{80s}+\\beta_2 \\cdot d_{northwest} + \\beta_3 \\cdot d_{single}\\] What do you conclude? Next, add square footage to the model. What has changed in terms of coefficicents? How do you interpret that change? "],["violating-assumptions.html", "6 Violating Assumptions", " 6 Violating Assumptions This chapter introduces the detection and correction of problems with the estimation procedure due to the violation of the key assumptions necessary for the OLS model to work. The following R packages are needed for this chapter: car, lmtest, orcutt, and sandwich. "],["heteroscedasticity.html", "6.1 Heteroscedasticity", " 6.1 Heteroscedasticity A key assumption of the OLS model is homoscedasticity error terms. That is, the error variance is constant: \\[Var(\\epsilon_i) = \\sigma^2\\] With heteroscedasticity, the variance of the error term is not constant: \\[Var(\\epsilon_i) = \\sigma_i^2\\] For a bivariate regression model with heteroscedastic data, it can be shown that \\[Var(\\hat{\\beta_1}) = \\frac{\\sum x_i^2 \\sigma_i^2}{(\\sum x_i^2)^2}\\] This is different from the variance of the coefficient estimate under homoscedasticity: \\[Var(\\hat{\\beta_1}) = \\frac{\\sigma^2}{\\sum x_i^2}\\] Unbiasedness of the OLS estimator is not affected but the variance of \\(\\beta_1\\) will be larger compared to other estimators. Note that the measure of \\(R^2\\) is unaffected by heteroscedasticity. Homoscedasticity is needed to justify the t-test, F-test, and confidence intervals. The F-statistic does no longer have an F-distribution. In short, hypothesis tests on the \\(\\beta\\)-coefficients are no longer valid. If \\(\\sigma_i^2\\) was known, the use of a Generalized Least Squares (GLS) model would be appropriate: \\[y_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i\\] Dividing both sides by the known variance: \\[\\frac{y_i}{\\sigma_i}=\\beta_0 \\cdot \\frac{1}{\\sigma_i}+\\beta_1 \\frac{x_i}{\\sigma_i}+\\frac{\\epsilon_i}{\\sigma_i}\\] If \\(\\epsilon^*_i = \\epsilon_i / \\sigma_i\\), then it can be shown that \\(Var(\\epsilon^*_i)=1\\), i.e., constant. Under the usual OLS model: \\[\\sum_{i=1}^N e_i^2=\\sum_{i=1}^N \\left(y_i-\\hat{\\beta}_0+\\hat{\\beta}_1 \\cdot x_i \\right)^2\\] Under GLS model: \\[\\sum_{i=1}^N w_i e_i^2= \\sum_{i=1}^N w_i \\left(y_i-\\hat{\\beta}_0+\\hat{\\beta}_1 \\cdot x_i \\right)^2\\] That is, GLS minimizes the weighted sum of the residual squares. Since in reality, the variance of \\(\\sigma^2\\) is not known, other techniques have to be employed to obtain so-called heteroscedasticity-consistent (HC) standard errors. But first, two tests are introduced to detect heteroscedasticity. 6.1.1 Detecting Heteroscedasticity Two test are presented to detect heteroscedasticity: Goldfeld-Quandt Test (1965) Breusch-Pagan-Godfrey Test (1979) The steps necessary for the Goldfeld-Quandt Test are as follows: Sort observations by ascending order of the dependent variable. Pick C as the number of central observations to drop in the middle of the dependent variable. Run two separate regression equations, i.e., with the lower and upper part. Compute \\[\\lambda = \\frac{RSS_2/df}{RSS_1/df}\\] \\(\\lambda\\) follows an F-distribution. The Goldfeld-Quandt Test can be illustrated with gqtestdata. In a first step, the data is separated into two groups with \\(C=6\\). In a second step, both groups are used to run a regression. And lastly, \\(\\lambda\\) is calculated. gqtestdata1 = gqtestdata[1:22,] gqtestdata2 = gqtestdata[29:50,] bhat1 = lm(price~sqft,data=gqtestdata1) bhat2 = lm(price~sqft,data=gqtestdata2) lambda = sum(bhat2$residuals^2)/sum(bhat1$residuals^2) Of course, there is also a function in R called gqtest which simplifies the procedure. library(lmtest) bhat = lm(price~sqft,data=gqtestdata) gqtest(bhat,fraction = 6) ## ## Goldfeld-Quandt test ## ## data: bhat ## GQ = 14.912, df1 = 20, df2 = 20, p-value = 5.235e-08 ## alternative hypothesis: variance increases from segment 1 to 2 In any case, the hypothesis of homoscedasticity is rejected for gqtestdata. The Breusch-Pagan-Godfrey Test is an alternative and does not rely on choosing C as the number of central observations to be dropped. The steps include the following: Run a regular OLS model and obtain the residuals. Calculate \\[\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^N e^2_i}{N}\\] Construct the variable \\(p_i\\) as follows: \\(p_i = e^2_i / \\hat{\\sigma}^2\\) Regress \\(p_i\\) on the Xs as follows \\[p_i = \\alpha_0 + \\alpha_1 \\cdot x_{i1}+\\alpha_2 \\cdot x_{i2} + \\dots\\] Obtain the explained sum of squares (ESS) and define \\(\\Theta = 0.5 \\cdot ESS\\). Then \\(\\Theta \\sim \\chi^2_{m-1}\\). The much simpler procedure is to use the function bptest() in R. library(lmtest) bhat = lm(price~sqft,data=gqtestdata) bptest(bhat) ## ## studentized Breusch-Pagan test ## ## data: bhat ## BP = 11.639, df = 1, p-value = 0.0006458 6.1.2 Correcting Heteroscedasticity To correct for heteroscedasticity, robust standard errors must be obtained. library(sandwich) bhat = lm(price~sqft,data=gqtestdata) summary(bhat) vcov = vcovHC(bhat) coeftest(bhat,vcov.=vcov) Note that there are multiple variations to calculate the standard error and thus, it is possible for slight variations among the results from different packages. \\[Var(\\hat{\\beta}_1) = \\frac{\\sum_{i=1}^N (x_i-\\bar{x})^2 e_i^2}{\\sum_{i=1}^N (x_i-\\bar{x})^2}\\] The square root of the following equation is called heteroscedastic robust standard error: \\[\\widehat{Var}(\\hat{\\beta}_j) = \\frac{\\sum_{i=1}^N \\hat{r}^2_{ij} e_i^2}{\\sum_{i=1}^N (x_i-\\bar{x})^2}\\] Standard errors can be either larger or smaller. Note that in this example, we do not know whether heteroscedasticity is present or not. "],["multicollinearity.html", "6.2 Multicollinearity", " 6.2 Multicollinearity Multicollinearity describes the situation in which two or more independent variables are linearly related. Under perfect multicollinearity: \\[\\lambda_1 x_1 + \\lambda_2 x_2 + \\dots +\\lambda_k x_k = 0\\] where \\(\\lambda_i\\) are constants that are not all zero simultaneously. For example, consider \\(x_1=\\{8,12,15,17\\}\\), \\(x_2=\\{24,36,45,51\\}\\), and \\(x_3=\\{2,3,3.75,4.25\\}\\). In this case, \\(\\lambda_1=1\\), \\(\\lambda_2=-1/5\\), and \\(\\lambda_3=2\\). Note, multicollinearity refers to linear relationships! Including a squared or cubed term is not an issue of multicollinearity. It can be shown that the variance of the estimator increases in the presence of multicollinearity. There are various indications that the data suffers from multicollinearity: High \\(R^2\\) but few significant variables Fail to reject the hypothesis for H\\(_0\\): \\(\\beta_i=0\\) based on t-values but rejection all slopes being simultaneously zero based on F-test. High correlation among explanatory variables Variation of statistically significant variables between models. 6.2.1 Variance Inflated Factors (VIF) Identifies possible correlation among multiple independent variables and not just two as in the case of a simple correlation coefficient. Consider the model: \\[y_i = \\beta_0 + \\beta_k x_{ik} + \\epsilon_i\\] The estimated variances of the coefficient \\(\\beta_k\\) is written as \\[Var(\\beta_k)^* = \\frac{\\sigma^2}{\\sum_{i=1}^N (x_{ik}-\\bar{x}_k)^2}\\] Without any multicollinearity, this variance is minimized. If some some independent variables are correlated with the independent variable \\(k\\), then \\[Var(\\beta_k) = \\frac{\\sigma^2}{\\sum_{i=1}^N (x_{ik}-\\bar{x}_k)^2} \\cdot \\frac{1}{1-R^2_k}\\] where \\(R^2_k\\) is the \\(R^2\\) if variable \\(x_k\\) is taken as the dependent variable. The VIF can be written as \\[\\frac{Var(\\beta_k)}{Var(\\beta_k)^*}=\\frac{1}{1-R^2_k}\\] If \\(VIF=1\\), then there is no relationship between the variable \\(x_k\\) and the remaining independent variables. Otherwise, \\(VIF&gt;1\\). In general, the interpretation is as follows: VIF of 4 warrants attention VIF of 10 indicates a serious problem. 6.2.2 Examples To illustrate the concept of multicollinearity, the data set from nfl is used (Berri et al. (2011)). The first model includes total salary as the dependent variable and the following independent variables: prior season passing yards, pass attempts, experience (squared) in the league, draft round pick, veteran (more than 3 years in the league), pro bowl appearance, and facial symmetry. bhat = lm(log(total)~yards+att+exp+exp2+draft1+draft2+veteran+changeteam+pbowlever+symm,data=nfl) summary(bhat) After estimating the results, the function vif() from the package car is used: library(car) vif(bhat) ## yards att exp exp2 draft1 draft2 veteran changeteam pbowlever symm ## 32.547700 30.920282 39.889877 26.715342 1.621048 1.228091 5.253525 1.194254 1.581753 1.056661 The results indicate multicollinearity for yards, att, and experience. Passings yards and attempts may be correlated and thus, one of them (att) is dropped. bhat = lm(log(total)~yards+exp+exp2+draft1+draft2+veteran+changeteam+pbowlever+symm,data=nfl) summary(bhat) vif(bhat) ## yards exp exp2 draft1 draft2 veteran changeteam pbowlever symm ## 1.460849 39.339639 26.162804 1.616171 1.227479 5.253502 1.141435 1.569621 1.052906 This improves the estimation but experience (and its squared term) are still problematic. The last estimation removes experience and the VIF terms are now in the acceptable range. bhat = lm(log(total)~yards+draft1+draft2+veteran+changeteam+pbowlever+symm,data=nfl) summary(bhat) vif(bhat) ## yards draft1 draft2 veteran changeteam pbowlever symm ## 1.406241 1.653634 1.229459 1.976506 1.101988 1.406095 1.010855 The important part is that the conclusion of the paper has not changed with regard to facial symmetry. "],["other-issues-and-problems-with-data.html", "6.3 Other Issues and Problems with Data", " 6.3 Other Issues and Problems with Data More serious problems than heteroscedasticity: Functional form mis-specification Measurement error Missing data: Estimating a standard regression model is not possible with missing values. All statistical software packages ignore missing data. Missing data is a minor problem if it is due to random error. Missing data can be problematic if it is systematically missing, e.g., missing education data for people with lower education Non-random samples Outliers "],["autocorrelation.html", "6.4 Autocorrelation", " 6.4 Autocorrelation The correlation of error terms is called autocorrelation. The issue usually arises if there is a time component in the data. Recall the main types of data available for research: Cross-sectional data (multiple observations at same time point) Time series data (one variable observed over time) Pooled data (multiple observations at different time points) Panel data (same observations at different time points) There is a distinction between serial correlation and autocorrelation: Serial correlation: Correlation between two series Autocorrelation: Correlation with lagged variables The OLS estimator is still unbiased but there is no longer minimum variance since \\(E(\\epsilon_i \\epsilon_j) \\neq 0\\). Autocorrelation is unlikely for cross-sectional data except in the case of spatial auto-correlation. One cause of autocorrelation could be inertia in economic variables. For example, variables such as income, production, or employment increase after a recession. But there are a number of other reasons for autocorrelation. Autocorrelation could be caused by specification bias due to excluded variables or incorrect functional forms. For example, assume that the correct equation is \\[q_{beef}=\\beta_0+\\beta_1 \\cdot p_{beef}+\\beta_2 \\cdot p_{income} + \\beta_3 \\cdot p_{pork}+\\epsilon_t\\] The estimated equation is: \\[q_{beef}=\\beta_0+\\beta_1 \\cdot p_{beef}+\\beta_2 \\cdot p_{income}+\\upsilon_t\\] The error terms in both equations are denoted \\(\\epsilon_t\\) and \\(\\upsilon_t\\), respectively. This results in a systematic patters of \\(\\upsilon_t\\): \\[\\upsilon_t= \\beta_3 \\cdot p_{pork}+\\epsilon_t\\] Correlation between the error terms can also be caused by specifying an incorrect functional form. Assume that the correct equation is written as follows: \\[y_i = \\beta_0 +\\beta_1 \\cdot x_i +\\beta_2 \\cdot x_i^2 +\\epsilon_i\\] But the estimated equation is \\[y_i = \\beta_0 +\\beta_1 x_i +\\epsilon_i\\] Serial correlation is caused by lagged terms in the regression equation: \\[consumption_t = \\beta_0 + \\beta_1 \\cdot income_t+\\beta_3 \\cdot consumption_{t-1}+\\epsilon_t\\] The issues of lagged terms will be covered in the part on dynamic regression and time series and this section serves only as an introduction to first-order autoregressive schemes. Consider the model: \\[y_t=\\beta_0+\\beta_1 \\cdot x_t + \\upsilon_t\\] Assume the following form of \\(\\upsilon\\): \\[\\upsilon_t = \\rho \\cdot \\upsilon_{t-1} + \\epsilon_t\\] his is called a first-order autoregressive AR(1) scheme. An AR(2) would be written as \\[\\upsilon_t = \\rho_1 \\cdot \\upsilon_{t-1} + \\rho_2 \\cdot \\upsilon_{t-2} + \\epsilon_t\\] This can be illustrated with simulated data. Consider the following model: \\[y_t=1+0.8 \\cdot x_t + \\upsilon_t\\] and assume the following form of \\(\\upsilon\\): \\[\\upsilon_t = 0.7 \\cdot \\upsilon_{t-1} + \\epsilon_t\\] 1. Simulate the above model 100 times 2. Compare variance of coefficients under different two different methods: (1) OLS and (2) Cochrane-Orcutt 6.4.1 Durbin Watson d-Test The test statistic of the Durbin-Watson test is written as: \\[d=\\frac{\\sum_{t=2}^N (e_t-e_{t-1})^2}{\\sum_{t=1}^N e_t^2}\\] Assumptions underlying the test are No intercept AR(1) process, i.e., \\(\\upsilon_t = \\rho \\upsilon_{t-1} + \\epsilon_t\\) No lagged independent variables Original papers derive lower (\\(d_L\\)) and upper (\\(d_U\\)) bounds, i.e., critical values, that depend on \\(N\\) and \\(k\\) only. \\(d \\approx 2 \\cdot (1-\\rho)\\) and since \\(-1 \\leq \\rho \\leq 1\\), we have \\(0 \\leq d \\leq 4\\). Rule of thumb indicates that \\(d=2\\) signals no problems. 6.4.2 Breusch-Godfrey Test Consider the following model \\(y_t = \\beta_0 + \\beta_1 x_t + \\upsilon_t\\) with the following error term structure: \\[\\upsilon_t = \\rho_1 \\upsilon_{t-1} + \\rho_2 \\upsilon_{t-2} + \\dots + \\rho_p \\upsilon_{t-p} + \\epsilon_t\\] The null hypothesis for the test is expressed as follows: \\(H_0\\): \\(\\rho_1 = \\rho_2 = \\dots = \\rho_p =0\\) When the following regression is executed: \\[\\hat{\\upsilon}_t = \\alpha_0 + \\alpha_1 \\cdot x_t + \\hat{\\rho}_1 \\cdot \\hat{\\upsilon}_{t-1} + \\hat{\\rho}_2 \\cdot \\hat{\\upsilon}_{t-2} + \\dots + \\hat{\\rho}_p \\cdot \\hat{\\upsilon}_{t-p} + \\epsilon_t\\] Then \\[(n-p) \\cdot R^2 \\sim \\chi^2_p\\] "],["exercises-1.html", "6.5 Exercises", " 6.5 Exercises WDI and Heteroscedasticity: Using the data in wdi, estimate the following equation for the year 2018 and report the results: \\[\\begin{equation*} fertrate = \\beta_0 + \\beta_1 \\cdot gdp+ \\beta_2 \\cdot litrate \\end{equation*}\\] The variable represents fertility rate (birth per woman), represents the GDP per capita in in real terms, and is the literacy rate. Indy Homes: The data indyhomes contains home values of two ZIP codes in Indianapolis. The model estimates the home value (dependent variable) based on a set of independent variables. The variables levels and garage refers to the number of stories and garage spots, respectively. The remaining variables are self-explanatory. Create a dummy variable called northwest for the 46268 ZIP code. Report and interpret the results of the following regression equation: \\[\\ln(price)=\\beta_0+\\beta_1 \\cdot \\ln(sqft)+\\beta_2 \\cdot northwest+\\beta_3 \\cdot \\ln(lot)+\\beta_4 \\cdot bed+\\beta_5 \\cdot garage+\\beta_6 \\cdot levels+\\beta_7 \\cdot northwest \\cdot levels\\] What is the expected home value of a house in the 46228 ZIP code area with the following characteristics: 1900 sqft, 0.65 acres lot, 3 bedrooms, 3 bathrooms, 2 garage spots, and 2 story. Conduct a Breusch-Pagan-Godfrey test for heteroscedasticity. What do you conclude? Estimate the above model with heteroscedasticity-consistent (HC) standard errors. What changes compared to the model from Part b? WDI and Multicollinearity: Use the command subset() on the WDI data and to select the variables fertrate, gdp, litrate, lifeexp, and mortrate for the year 2015. Estimate the following model \\[fertrate=\\beta_0+\\beta_1 \\cdot gdp+\\beta_2 \\cdot litrate+\\beta_3 \\cdot lifeexp+\\beta_4 \\cdot mortrate\\] Interpret the results. What do you conclude in terms of statistical significance and the value of \\(R^2\\)? Use the function vif from the package car. What can you say about the issue of multicollinearity in this case? Correct the issue of multicollinearity by adjusting your model. In this exercise, you will estimate the per-capita pork demand as a function of pork prices and the prices of substitutes (beef and chicken) as well as real disposable income. Estimate the following equation and interpret the coefficients. Are the signs of the coefficients what you would expect? \\[\\begin{equation*} \\ln(q_{pork}) = \\beta_0 + \\beta_1 \\cdot \\ln(p_{pork}) + \\beta_2 \\cdot \\ln(p_{chicken}) + \\beta_3 \\cdot \\ln(p_{beef}) + \\beta_4 \\cdot \\ln(rdi) \\end{equation*}\\] "],["binary-choice.html", "7 Binary Choice", " 7 Binary Choice Binary choice models are part of a large class of so-called qualitative choice models which are used for qualitative dependent variables. Consider the following outcomes of interest: Is a person in the labor force? Will an individual vote yes on a particular issue? Did a person watch the last Super Bowl? Have you purchased a new car in the past year Did you do any charitable contributions in the past year? Did you vote during the last election? Does an individual recidivate after being released from prison? For those questions, the dependent variable is either 0 (no) or 1 (yes). For binary choice models, the outcome is interpreted as a probability, i.e., what is the probability of a person to answer yes to those questions. In the next chapter, the model is expanded to consider more than binary outcomes. Those models include categorical dependent variable that are either naturally ordered or have no ordering. Examples of naturally ordered categorical variables are: Level of happiness: Very happy, happy, ok, or sad. Intention to get a COIVID-19 vaccine: Definitely yes, probably yes, probably no, or definitely no Two examples about categorical dependent variable which have no ordering are: Commute to campus: Bike, car, walk, or bus Voter registration: Democrat, Republican, or independent For all those models, the outcome of interest is the probability to fall into a particular category. For binary choice models which are considered in this chapter, the outcome of interest is the probability to fall into the 1 (yes) category. For binary choice models, y takes one of two values: 0 or 1. And the model will specify \\(Pr(y=1|x)\\) where x are the independent variables. Consider the decision to purchase organic food. Assume that you have data about the income of respondents as well as information if they purchase organic food. The purchase decision (yes or no) is on the vertical axis and the income is on the horizontal axis. Remember that the probability has be bounded between 0 and 1. Hence, we need to find a function \\(G(z)\\) such that \\(0 \\leq G(z) \\leq 1\\) for all values of \\(z\\) and \\(P(y=1|x)=G(z)\\). Popular choices for \\(G(z)\\) are the cumulative normal distribution function (Probit Model) and the logistic function (Logit Model). For what follows, let \\(z=\\beta_0+\\beta_1 \\cdot x_1 + \\cdots + \\beta_k \\cdot x_k\\). For the probit model, \\(G(z)\\) is written as \\[Pr(y = 1) = G(z)=\\Phi(z)\\] where \\(\\Phi\\) represents the cumulative normal. And for the logit model, \\(G(z)\\) is written as \\[Pr(y = 1) = G(z)=\\frac{e^z}{1+e^z}=\\frac{1}{1+e^{-z}}\\] The interpretation of the logit and probit estimates is not as straightforward as in the multivariate regression case. In general, we care about the effect of \\(x\\) on \\(P(y=1|x)\\). The sign of the coefficient shows the direction of the change in probability. The approximation to the marginal effect if \\(x\\) is roughly continuous: \\[\\Delta P(y=1|x) \\approx g(\\hat{\\beta}_0 + x \\cdot \\hat{\\beta}) \\cdot \\beta_j \\cdot \\Delta x_j\\] To obtain the marginal effects in R, an additional step is necessary. Let us illustrate the binary choice model using the data set organic and a logit model. The results of interest for the binary choice model are the (1) coefficient estimates, (2) marginal effects, and (3) predicted probabilities. "],["binary-choice-estimation-in-r.html", "7.1 Binary Choice Estimation in R", " 7.1 Binary Choice Estimation in R There are (at least) two possibilities to obtain the coefficient estimates in R. The first is using the built in R command glm(): bhat_glm_logit = glm(buying~income,family=binomial(link=&quot;logit&quot;),data=organic) summary(bhat_glm_logit) ## ## Call: ## glm(formula = buying ~ income, family = binomial(link = &quot;logit&quot;), ## data = organic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8451 -0.5293 -0.1423 0.4093 1.9154 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.87557 1.13842 -5.161 2.45e-07 *** ## income 0.11709 0.02247 5.211 1.87e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 138.469 on 99 degrees of freedom ## Residual deviance: 70.931 on 98 degrees of freedom ## AIC: 74.931 ## ## Number of Fisher Scoring iterations: 6 Note that interpretation of the coefficients is slightly different from the regular linear model. The sign of the coefficient estimate for income is interpreted as the direction in which the probability changes. In this case, the coefficient is positive and thus, an increase (decrease) in income leads to an increase (decrease) in the probability of purchasing organic food. In addition, the coefficients are statistically significant. As aforementioned, the coefficients do not indicate the marginal effects though. To calculate the marginal effects, a slightly different approach is necessary. Let us first look at the second approach of obtaining the coefficient estimates and the R package mfx is required to do so. bhat = logitmfx(buying~income,data=organic) summary(bhat$fit) ## ## Call: ## glm(formula = formula, family = binomial(link = &quot;logit&quot;), data = data, ## start = start, control = control, x = T) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8451 -0.5293 -0.1423 0.4093 1.9154 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.87557 1.13842 -5.161 2.45e-07 *** ## income 0.11709 0.02247 5.211 1.87e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 138.469 on 99 degrees of freedom ## Residual deviance: 70.931 on 98 degrees of freedom ## AIC: 74.931 ## ## Number of Fisher Scoring iterations: 6 The results are identical as before using the but the command allows for the calculation of the marginal effects as well. This is done with the command the bhat\\$mfxest. bhat$mfxest ## dF/dx Std. Err. z P&gt;|z| ## income 0.02919553 0.005634262 5.181785 2.197728e-07 It is important to note that the marginal effects are taken at the mean of the independent variables. To calculate the marginal effects at specific points, the command margins() must be used. Before, we used the command glm() to calculate the logit coefficients. The reason for using the glm() is that it allows us to calculate the predicted probabilities. Consider the example to purchase organic food and assume that there are three new respondents with income levels $25,000, $50,000, and $75,000. To predict the probability of those individuals purchasing organic food, the following functions can be used: datablock = data.frame(income=c(25,50,75)) predict(bhat_glm_logit,newdata=datablock,type=&quot;response&quot;) ## 1 2 3 ## 0.0498116 0.4946870 0.9481377 "],["additional-examples-and-probit-model.html", "7.2 Additional Examples and Probit Model", " 7.2 Additional Examples and Probit Model Here are two additional examples on working full-time and voting behavior. Both examples use the gss2018 data. Note that the previous chapter have introduced the logit model. To estimate a Probit model, the term logit has to be replaced with probit. The results are almost identical. The variables are defined as follows: \\(fulltime\\): Does the respondent work full time. \\(government\\): Does the respondent work for the government \\(education\\): Less than high school (0), high school (1), associate/junior college (2), bachelor (3), and graduate (4) \\(vote\\): Voted in the 2016 election \\(married\\), \\(age\\), \\(childs\\), and \\(income\\) are self-explanatory. Income is for 2016. 7.2.1 Full-Time Work equation = c(&quot;fulltime~age+childs+government+education+married&quot;) bhat_glm = glm(equation,family=binomial(link=&quot;logit&quot;),data=gss2018) summary(bhat_glm) ## ## Call: ## glm(formula = equation, family = binomial(link = &quot;logit&quot;), data = gss2018) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3019 0.4298 0.5638 0.6692 0.9954 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.877519 0.335655 5.594 2.22e-08 *** ## age -0.021941 0.006985 -3.141 0.00168 ** ## childs -0.015773 0.063675 -0.248 0.80435 ## government 0.323365 0.272825 1.185 0.23592 ## education 0.264400 0.086471 3.058 0.00223 ** ## married 0.353223 0.205331 1.720 0.08539 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 724.91 on 771 degrees of freedom ## Residual deviance: 699.31 on 766 degrees of freedom ## AIC: 711.31 ## ## Number of Fisher Scoring iterations: 4 bhat = logitmfx(equation,data=gss2018) summary(bhat$fit) ## ## Call: ## glm(formula = formula, family = binomial(link = &quot;logit&quot;), data = data, ## start = start, control = control, x = T) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3019 0.4298 0.5638 0.6692 0.9954 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.877519 0.335655 5.594 2.22e-08 *** ## age -0.021941 0.006985 -3.141 0.00168 ** ## childs -0.015773 0.063675 -0.248 0.80435 ## government 0.323365 0.272825 1.185 0.23592 ## education 0.264400 0.086471 3.058 0.00223 ** ## married 0.353223 0.205331 1.720 0.08539 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 724.91 on 771 degrees of freedom ## Residual deviance: 699.31 on 766 degrees of freedom ## AIC: 711.31 ## ## Number of Fisher Scoring iterations: 4 bhat_glm = glm(equation,family=binomial(link=&quot;probit&quot;),data=gss2018) summary(bhat_glm) ## ## Call: ## glm(formula = equation, family = binomial(link = &quot;probit&quot;), data = gss2018) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3240 0.4189 0.5671 0.6720 0.9792 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.087820 0.187423 5.804 6.47e-09 *** ## age -0.011979 0.003992 -3.001 0.00269 ** ## childs -0.008966 0.036310 -0.247 0.80496 ## government 0.172189 0.148316 1.161 0.24566 ## education 0.154238 0.047653 3.237 0.00121 ** ## married 0.196650 0.114850 1.712 0.08686 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 724.91 on 771 degrees of freedom ## Residual deviance: 699.29 on 766 degrees of freedom ## AIC: 711.29 ## ## Number of Fisher Scoring iterations: 5 7.2.2 Voting Behavior equation = c(&quot;vote~age+childs+government+education+married+income&quot;) bhat_glm = glm(equation,family=binomial(link=&quot;logit&quot;),data=gss2018) summary(bhat_glm) ## ## Call: ## glm(formula = equation, family = binomial(link = &quot;logit&quot;), data = gss2018) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3291 -1.0702 0.5551 0.8683 1.6618 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.924e+00 2.983e-01 -6.449 1.13e-10 *** ## age 4.145e-02 6.944e-03 5.970 2.37e-09 *** ## childs -7.944e-02 5.677e-02 -1.399 0.162 ## government 4.536e-01 2.325e-01 1.951 0.051 . ## education 4.923e-01 8.326e-02 5.913 3.36e-09 *** ## married 1.282e-01 1.779e-01 0.721 0.471 ## income 2.039e-06 2.470e-06 0.826 0.409 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 978.09 on 771 degrees of freedom ## Residual deviance: 858.90 on 765 degrees of freedom ## AIC: 872.9 ## ## Number of Fisher Scoring iterations: 4 bhat = logitmfx(equation,data=gss2018) summary(bhat$fit) ## ## Call: ## glm(formula = formula, family = binomial(link = &quot;logit&quot;), data = data, ## start = start, control = control, x = T) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3291 -1.0702 0.5551 0.8683 1.6618 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.924e+00 2.983e-01 -6.449 1.13e-10 *** ## age 4.145e-02 6.944e-03 5.970 2.37e-09 *** ## childs -7.944e-02 5.677e-02 -1.399 0.162 ## government 4.536e-01 2.325e-01 1.951 0.051 . ## education 4.923e-01 8.326e-02 5.913 3.36e-09 *** ## married 1.282e-01 1.779e-01 0.721 0.471 ## income 2.039e-06 2.470e-06 0.826 0.409 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 978.09 on 771 degrees of freedom ## Residual deviance: 858.90 on 765 degrees of freedom ## AIC: 872.9 ## ## Number of Fisher Scoring iterations: 4 bhat_glm = glm(equation,family=binomial(link=&quot;probit&quot;),data=gss2018) summary(bhat_glm) ## ## Call: ## glm(formula = equation, family = binomial(link = &quot;probit&quot;), data = gss2018) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3910 -1.0777 0.5492 0.8754 1.6541 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.167e+00 1.771e-01 -6.589 4.43e-11 *** ## age 2.527e-02 4.055e-03 6.232 4.60e-10 *** ## childs -4.884e-02 3.411e-02 -1.432 0.1522 ## government 2.722e-01 1.359e-01 2.003 0.0452 * ## education 2.949e-01 4.830e-02 6.106 1.02e-09 *** ## married 8.745e-02 1.062e-01 0.824 0.4101 ## income 1.132e-06 1.435e-06 0.789 0.4302 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 978.09 on 771 degrees of freedom ## Residual deviance: 857.62 on 765 degrees of freedom ## AIC: 871.62 ## ## Number of Fisher Scoring iterations: 4 "],["exercises-2.html", "7.3 Exercises", " 7.3 Exercises Consider the problem of choosing whether or not to purchase a hybrid vehicle (e.g., the Toyota Prius, Honda Civic Hybrid, Ford Escape). As an analyst, you assume that whether or not an individual purchases a hybrid depends upon the current price of gasoline (gas), the difference in purchase price of a hybrid vehicle compared to a comparably equipped vehicle (increment), college education which is represented by a dummy variable that equals 1 if individual has completed college and equals 0 otherwise (\\(college\\)), and a dummy variable that equals 1 if the individual is a member of an environmental organization, e.g., Nature Conservancy, National Audubon Society, (env). Answer the following questions contained in hybrid: Provide summary statistics (i.e., mean, minimum, and maximum) for each variables. Estimate a regression model that allows to calculate the probability that a person would buy a hybrid. Using your parameter estimates, compute the probability that the following ``types of individuals will buy a hybrid: Type I: gasoline price = 2.50; difference in purchase price = 1,500; college = 0; and member of an environmental organization = 0 Type II: gasoline price = 3.50; difference in purchase price = 500; college = 1; and member of an environmental organization = 1 Type III: gasoline price = 3.00; difference in purchase price = 1,000; college = 1; and member of an environmental organization = 0 Given the above probabilities, calculate the marginal effect that gasoline prices have on the probability that each of the three ``types of individuals will purchase a hybrid vehicle. Given the above probabilities, calculate the marginal effect that joining an environmental group on Type I and Type III individuals. EV Data: Using the , estimate a logit model which calculates the probability that a consumer would purchase a hybrid (choice=2), plug-in hybrid (choice=3), or electric vehicle (choice=4) as opposed to a gasoline car (choice=1). In a first step, you should create a new variable that is equal to 0 if the person purchases a gasoline vehicle and is equal to 1 for everything else. Include the following independent variables in your model: \\(age\\), \\(numcars\\), \\(politics\\), \\(female\\), \\(edu\\), and \\(income\\). We are interested in the determinants of voting based on data from the General Social Survey (GSS). The following variables are hypothesized to be a determinant of voting behavior: \\(age\\), \\(income\\), \\(gun ownership\\) (yes=1), \\(stance on death penalty\\) (in favor=1), and \\(education level\\) (higher number is associated with a higher education level). Run a probit and logit model and report the results. Calculate the marginal probabilities. \\end{enumerate} "],["qualitative-choice-models.html", "8 Qualitative Choice Models", " 8 Qualitative Choice Models The last chapter introduced binary choice models that answer questions such as: Did you vote during the last election? Does an individual recidivate after being released from prison? In this chapter, cases of dependent variables with more than two categories are considered. The categorical dependent variables can be with or without natural ordering. Here are some examples of natural ordering: Level of happiness: Very happy, happy, okay, or sad Intent to get vaccinated: Definitely yes, probably yes, probably no, definitely no Examples without natural ordering: Type of car to purchase: Passenger car, Pick-up truck, van, convertible Those models are also known as Discrete Choice Models. The packages required for this section are mlogit, erer, MASS, AER, glmmML, and nnet. The data required are fpdata and evdata in addition to Fishing and TravelMode which are part of the packages mlogit and AER. You can load them into R with the following commands: data(\"Fishing\",package=\"mlogit\") data(\"TravelMode\",package=\"AER\") There is also a YouTube Video associated with this chapter. "],["ordered-logit-model.html", "8.1 Ordered Logit Model", " 8.1 Ordered Logit Model Suppose that respondents are asked if they are going to get a COVID-19 vaccine. The choices are Definitely yes Probably yes Probably no Definitely no The ordered logit model assumes the presence of a latent (unobserved by the researcher) variable \\(y^*\\): \\[y_i^* = \\beta_0+\\beta_1 \\cdot x_i + \\epsilon_i\\] In the case of the vaccine model, this may be a measure of trust in vaccines. What the researcher does measure is an \\(m\\)-alternative ordered model: \\[y_i = j \\quad \\text{if} \\quad \\alpha_{j-1} &lt; y_i^* \\leq \\alpha_j \\quad \\text{for} \\quad j=1, \\dots,m\\] where \\(\\alpha_0 = - \\infty\\) and \\(\\alpha_m=\\infty\\). In this case, we have \\[ \\begin{align*} Pr(y = j) &amp;=Pr(\\alpha_{j-1} &lt; y_i^* \\leq \\alpha_j)\\\\ &amp;=Pr(\\alpha_{j-1} &lt; \\beta_0+\\beta_1 \\cdot x_i + \\epsilon_i \\leq \\alpha_j)\\\\ &amp;=Pr(\\alpha_{j-1}- \\beta_0-\\beta_1 \\cdot x_i &lt; \\epsilon_i \\leq \\alpha_j - \\beta_0-\\beta_1 \\cdot x_i)\\\\ &amp;=F(\\alpha_j - \\beta_0-\\beta_1 \\cdot x_i)-F(\\alpha_{j-1} - \\beta_0-\\beta_1 \\cdot x_i) \\end{align*}\\] For the ordered logit: \\(F(z)=exp(z)/(1+exp(z))\\). The cut-off example points for the graph below are \\(\\alpha_0 = -\\infty\\), \\(\\alpha_2 = -1.5\\), \\(\\alpha_3 = 2\\), and \\(\\alpha_4 = \\infty\\) 8.1.1 Ordered Logit Example: Organic Food Purchase The order logit model is illustrated with a survey on the purchase frequency of organic tomatoes and organic strawberries fpdata: Never (1), rarely (2), once per month (3), every 2 weeks (4), 1-2 times a week (5), almost daily (6) The independent variables included in the model are Age and female Education: High school (1), some college (2), bachelor (3), master (4), technical school diploma (5), doctorate (6) fpdata$strawberries_org = as.factor(fpdata$strawberries_org) strawdata = fpdata[c(&quot;strawberries_org&quot;,&quot;age&quot;,&quot;education&quot;,&quot;female&quot;,&quot;kidsunder12&quot;)] strawdata = na.omit(strawdata) bhat = polr(strawberries_org~age+education+female+kidsunder12,data=strawdata,Hess=TRUE) summary(bhat) ## Call: ## polr(formula = strawberries_org ~ age + education + female + ## kidsunder12, data = strawdata, Hess = TRUE) ## ## Coefficients: ## Value Std. Error t value ## age -0.02034 0.009838 -2.0676 ## education 0.01596 0.112028 0.1425 ## female -0.41533 0.280485 -1.4808 ## kidsunder12 0.28560 0.321778 0.8876 ## ## Intercepts: ## Value Std. Error t value ## 0|1 -1.4958 0.6497 -2.3022 ## 1|2 -0.4381 0.6434 -0.6810 ## 2|3 0.2084 0.6394 0.3259 ## 3|4 0.8352 0.6442 1.2964 ## 4|5 1.6314 0.6699 2.4353 ## ## Residual Deviance: 526.4547 ## AIC: 544.4547 For the organic purchases data, the cut-off points are under Intercepts and thus, we have (rounded coefficients): \\[z = -0.020 \\cdot age +0.0160 \\cdot education -0.415 \\cdot female +0.286 \\cdot kidsunder12\\] The cut-off points can be interpreted as follows: \\[\\begin{align*} Pr(y=1) &amp;= P(z+\\epsilon_i \\leq -1.4958)\\\\ Pr(y=2) &amp;= P(-1.4958 &lt; z+\\epsilon_i \\leq -0.4381)\\\\ Pr(y=3) &amp;= P(-0.4381 &lt; z+\\epsilon_i \\leq 0.2084)\\\\ Pr(y=4) &amp;= P(0.2084 &lt; z+\\epsilon_i \\leq 0.8352)\\\\ Pr(y=4) &amp;= P(0.8352 &lt; z+\\epsilon_i \\leq 1.6314)\\\\ Pr(y=6) &amp;= P(1.6314 \\leq z+\\epsilon_i ) \\end{align*}\\] In order to get the \\(p\\)-values displayed in the output, you have to execute two additional steps: bhat.coef = data.frame(coef(summary(bhat))) bhat.coef$pval = round((pnorm(abs(bhat.coef$t.value),lower.tail=FALSE)*2),2) bhat.coef ## Value Std..Error t.value pval ## age -0.02034185 0.009838336 -2.0676110 0.04 ## education 0.01595914 0.112027882 0.1424569 0.89 ## female -0.41533145 0.280485372 -1.4807598 0.14 ## kidsunder12 0.28560319 0.321777883 0.8875787 0.37 ## 0|1 -1.49575271 0.649708891 -2.3021891 0.02 ## 1|2 -0.43813109 0.643390358 -0.6809724 0.50 ## 2|3 0.20839939 0.639383205 0.3259382 0.74 ## 3|4 0.83515493 0.644195302 1.2964313 0.19 ## 4|5 1.63135404 0.669870994 2.4353257 0.01 8.1.2 Predicted Probability and Marginal Effects The predicted probability for each observation can be obtained as well assuming that the output of the polr command is stored in \\(bhat\\). bhat.pred = predict(bhat,type=&quot;probs&quot;) x = ocME(bhat) x$out$ME.all ## effect.0 effect.1 effect.2 effect.3 effect.4 effect.5 ## age 0.005 0.000 -0.001 -0.001 -0.001 -0.001 ## education -0.004 0.000 0.001 0.001 0.001 0.001 ## female 0.098 -0.003 -0.023 -0.024 -0.023 -0.026 ## kidsunder12 -0.067 0.001 0.015 0.017 0.016 0.018 "],["multinomial-logit-and-multinomial-probit-models.html", "8.2 Multinomial Logit and Multinomial Probit Models", " 8.2 Multinomial Logit and Multinomial Probit Models Revealed preferences: Observed choices of individuals Stated preference Hypothetical choice situations Economists modeling of choice Utility/happiness/satisfaction associated with multiple choice situations 8.2.1 Theoretical Aspects Travel choice model dependent on cost (\\(x\\)) and time (\\(z\\)): \\[V_j = \\alpha_j + \\beta_1 \\cdot x_j + \\beta_2 \\cdot z_j\\] Probability of choosing alternative \\(j\\) (assuming three choices): \\[\\begin{align*} P(1) &amp;= \\frac{e^{V_1}}{e^{V_1}+e^{V_2}+e^{V_3}}\\\\ P(2) &amp;= \\frac{e^{V_2}}{e^{V_1}+e^{V_2}+e^{V_3}}\\\\ P(3) &amp;= \\frac{e^{V_3}}{e^{V_1}+e^{V_2}+e^{V_3}} \\end{align*}\\] Note that \\(P(1)+P(2)+P(3) = 1\\) 8.2.2 Data Managment Long shape One row for each alternative Wide shape One row for each choice situation There are some very good resources on data management and the package in general: Estimation of Random Utility Models in R: The mlogit Package Travel Mode (long format) Travel Mode Choice Data mlogit.data(travelmode,choice=\"choice\",shape=\"long\",alt.levels=c(\"air\",\"train\",\"bus\",\"car\")) 8.2.3 Fishing Data The data is in wide format: Fishing modes: beach, pier, private, and charter Alternative-specific variables: price and catch Individual-specific variables: income Suitability of the wide format to store individual-specific variables The R parameter varying designates alternative specific variables In a first step, the model is only estimated using the individual-specific variable \\(income\\): data(&quot;Fishing&quot;,package=&quot;mlogit&quot;) fishing = mlogit.data(Fishing,shape=&quot;wide&quot;,varying=2:9,choice=&quot;mode&quot;) bhat = mlogit(mode~0|income,fishing) summary(bhat) ## ## Call: ## mlogit(formula = mode ~ 0 | income, data = fishing, method = &quot;nr&quot;) ## ## Frequencies of alternatives:choice ## beach boat charter pier ## 0.11337 0.35364 0.38240 0.15059 ## ## nr method ## 4 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 8.32E-07 ## gradient close to zero ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept):boat 7.3892e-01 1.9673e-01 3.7560 0.0001727 *** ## (Intercept):charter 1.3413e+00 1.9452e-01 6.8955 5.367e-12 *** ## (Intercept):pier 8.1415e-01 2.2863e-01 3.5610 0.0003695 *** ## income:boat 9.1906e-05 4.0664e-05 2.2602 0.0238116 * ## income:charter -3.1640e-05 4.1846e-05 -0.7561 0.4495908 ## income:pier -1.4340e-04 5.3288e-05 -2.6911 0.0071223 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1477.2 ## McFadden R^2: 0.013736 ## Likelihood ratio test : chisq = 41.145 (p.value = 6.0931e-09) fishing.fitted = fitted(bhat,outcome=FALSE) effects(bhat,covariate =&quot;income&quot;) ## beach boat charter pier ## 7.496226e-08 3.259851e-05 -1.201366e-05 -2.065981e-05 In a second step, alternative-specific variables are added: bhat = mlogit(mode~catch+price|income,data=fishing) summary(bhat) ## ## Call: ## mlogit(formula = mode ~ catch + price | income, data = fishing, ## method = &quot;nr&quot;) ## ## Frequencies of alternatives:choice ## beach boat charter pier ## 0.11337 0.35364 0.38240 0.15059 ## ## nr method ## 7 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 1.37E-05 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept):boat 5.2728e-01 2.2279e-01 2.3667 0.0179485 * ## (Intercept):charter 1.6944e+00 2.2405e-01 7.5624 3.952e-14 *** ## (Intercept):pier 7.7796e-01 2.2049e-01 3.5283 0.0004183 *** ## catch 3.5778e-01 1.0977e-01 3.2593 0.0011170 ** ## price -2.5117e-02 1.7317e-03 -14.5042 &lt; 2.2e-16 *** ## income:boat 8.9440e-05 5.0067e-05 1.7864 0.0740345 . ## income:charter -3.3292e-05 5.0341e-05 -0.6613 0.5084031 ## income:pier -1.2758e-04 5.0640e-05 -2.5193 0.0117582 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1215.1 ## McFadden R^2: 0.18868 ## Likelihood ratio test : chisq = 565.17 (p.value = &lt; 2.22e-16) fishing.fitted = fitted(bhat,outcome=FALSE) effects(bhat,covariate=&quot;income&quot;) ## beach boat charter pier ## -7.214167e-07 3.176132e-05 -2.173392e-05 -9.305980e-06 rm(bhat,Fishing,fishing.fitted) The mlogit package also allows for the estimation of a multinomial probit model. bhat = mlogit(mode~catch+price|income,data=fishing,probit=FALSE) summary(bhat) ## ## Call: ## mlogit(formula = mode ~ catch + price | income, data = fishing, ## probit = FALSE, method = &quot;nr&quot;) ## ## Frequencies of alternatives:choice ## beach boat charter pier ## 0.11337 0.35364 0.38240 0.15059 ## ## nr method ## 7 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 1.37E-05 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept):boat 5.2728e-01 2.2279e-01 2.3667 0.0179485 * ## (Intercept):charter 1.6944e+00 2.2405e-01 7.5624 3.952e-14 *** ## (Intercept):pier 7.7796e-01 2.2049e-01 3.5283 0.0004183 *** ## catch 3.5778e-01 1.0977e-01 3.2593 0.0011170 ** ## price -2.5117e-02 1.7317e-03 -14.5042 &lt; 2.2e-16 *** ## income:boat 8.9440e-05 5.0067e-05 1.7864 0.0740345 . ## income:charter -3.3292e-05 5.0341e-05 -0.6613 0.5084031 ## income:pier -1.2758e-04 5.0640e-05 -2.5193 0.0117582 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -1215.1 ## McFadden R^2: 0.18868 ## Likelihood ratio test : chisq = 565.17 (p.value = &lt; 2.22e-16) fishing.fitted = fitted(bhat,outcome=FALSE) effects(bhat,covariate=&quot;income&quot;) ## beach boat charter pier ## -7.214167e-07 3.176132e-05 -2.173392e-05 -9.305980e-06 rm(bhat,Fishing,fishing,fishing.fitted) ## Warning in rm(bhat, Fishing, fishing, fishing.fitted): object &#39;Fishing&#39; not found 8.2.4 Travel Data data(&quot;TravelMode&quot;,package=&quot;AER&quot;) travelmode = mlogit.data(TravelMode,choice=&quot;choice&quot;,shape=&quot;long&quot;,alt.var=&quot;mode&quot;) bhat = mlogit(choice~gcost+wait|income+size,data=travelmode,reflevel=&quot;car&quot;) summary(bhat) ## ## Call: ## mlogit(formula = choice ~ gcost + wait | income + size, data = travelmode, ## reflevel = &quot;car&quot;, method = &quot;nr&quot;) ## ## Frequencies of alternatives:choice ## car air train bus ## 0.28095 0.27619 0.30000 0.14286 ## ## nr method ## 5 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 1.66E-07 ## gradient close to zero ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept):air 7.8736084 0.9868475 7.9785 1.554e-15 *** ## (Intercept):train 5.5592051 0.6991387 7.9515 1.776e-15 *** ## (Intercept):bus 4.4331916 0.7783339 5.6957 1.228e-08 *** ## gcost -0.0196850 0.0054015 -3.6444 0.0002680 *** ## wait -0.1015659 0.0112306 -9.0436 &lt; 2.2e-16 *** ## income:air 0.0040710 0.0127247 0.3199 0.7490196 ## income:train -0.0551849 0.0144824 -3.8105 0.0001387 *** ## income:bus -0.0233237 0.0162973 -1.4311 0.1523914 ## size:air -1.0274229 0.2656569 -3.8675 0.0001100 *** ## size:train 0.3023954 0.2256155 1.3403 0.1801437 ## size:bus -0.0300096 0.3339774 -0.0899 0.9284023 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -177.45 ## McFadden R^2: 0.37463 ## Likelihood ratio test : chisq = 212.61 (p.value = &lt; 2.22e-16) tavel.fitted = fitted(bhat,outcome=FALSE) 8.2.5 Electric Vehicle Data evdata = mlogit.data(evdata,shape=&quot;wide&quot;,choice=&quot;choice&quot;) bhat = mlogit(choice~0|age+female+level2+numcars+edu+income+politics,data=evdata) summary(bhat) ## ## Call: ## mlogit(formula = choice ~ 0 | age + female + level2 + numcars + ## edu + income + politics, data = evdata, method = &quot;nr&quot;) ## ## Frequencies of alternatives:choice ## 1 2 3 4 ## 0.419355 0.265233 0.225806 0.089606 ## ## nr method ## 5 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 0.000603 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept):2 0.11810109 0.60622429 0.1948 0.8455384 ## (Intercept):3 0.90653782 0.63856959 1.4196 0.1557130 ## (Intercept):4 0.34528552 0.84542368 0.4084 0.6829675 ## age:2 -0.01298148 0.00731409 -1.7749 0.0759210 . ## age:3 -0.04216613 0.00867889 -4.8585 1.183e-06 *** ## age:4 -0.02314160 0.01139425 -2.0310 0.0422561 * ## female:2 0.17935742 0.21591603 0.8307 0.4061537 ## female:3 -0.02781447 0.23732560 -0.1172 0.9067019 ## female:4 0.00019196 0.32364088 0.0006 0.9995267 ## level2:2 0.13808856 0.25338230 0.5450 0.5857665 ## level2:3 0.87829370 0.25462065 3.4494 0.0005618 *** ## level2:4 1.14103750 0.33905869 3.3653 0.0007646 *** ## numcars:2 0.08340570 0.10845229 0.7691 0.4418611 ## numcars:3 0.02412224 0.11385000 0.2119 0.8322027 ## numcars:4 -0.35470532 0.15922101 -2.2278 0.0258969 * ## edu:2 0.09033371 0.08476675 1.0657 0.2865711 ## edu:3 0.21765011 0.09487551 2.2941 0.0217871 * ## edu:4 -0.00475892 0.12980749 -0.0367 0.9707550 ## income:2 -0.01070840 0.07000961 -0.1530 0.8784328 ## income:3 -0.04329121 0.07768626 -0.5573 0.5773519 ## income:4 -0.01112286 0.10371253 -0.1072 0.9145930 ## politics:2 -0.15253774 0.05211226 -2.9271 0.0034214 ** ## politics:3 -0.19656259 0.05687906 -3.4558 0.0005487 *** ## politics:4 -0.07567912 0.07405617 -1.0219 0.3068211 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -663.51 ## McFadden R^2: 0.062686 ## Likelihood ratio test : chisq = 88.75 (p.value = 2.6494e-10) evdata.fitted = fitted(bhat,outcome=FALSE) rm(bhat,evdata,evdata.fitted) "],["exercises-3.html", "8.3 Exercises", " 8.3 Exercises Happiness: Using the data set happy, generate an ordered logit regression model that regresses the dependent variable \\(happiness\\) on those variables that have the strongest potential causal relationship. For your model, interpret the R output and indicate why each independent variable that is included in the model would contribute to higher or lower happiness. Speak to the possibility of multicollinearity in the independent variables. Alternative Fuel Vehicle Choice: The data evdata contains data about the choice of consumers with respect to alternative fuel vehicles. For each consumer, you have the following variables: \\(age\\), \\(suv\\) (whether they are interested in buying a SUV), \\(level2\\) (indicating whether people have a fast charger for electric cars in their community), \\(own \\dots\\) (indicating whether the respondent currently has a gas, hybrid, plug- in hybrid, or battery electric vehicle), \\(gender\\) (1=female) and \\(numcars\\) (number of cars). Estimate a multinomial logit model that estimates the probability of a consumer to purchase a gasoline, hybrid, plug-in hybrid, or battery electric vehicle. Calculate the marginal probabilities as well. NHTS: Consider the data set hhpub from the 2017 National Household Travel Survey (NHTS). The data contains information about household characteristics and some of their travel means. For this question, you will focus on the following variables: \\(BIKE\\), \\(HBPPOPDN\\), \\(HHFAMINC\\), \\(HHVEHCNT\\), \\(HOMEOWN\\), and \\(URBRUR\\). You must read the codebook for this question and learn how the variables are coded. Go to the codebook and pick Household as the dataset (drop down menu). Before conducting the analysis, delete all entries that are not complete (i.e., all the negative values). Once you have final data set, estimate an ordered logit model with \\(BIKE\\) as the dependent variable and the other variables as the independent variables. What do you conclude from the model? Home heating: Consider the dataset Heating from the R package mlogit. You can load the data set into R by typing: data(\"Heating\",package=\"mlogit\"). The data contains the choice of heating systems in California homes. There are five types of heating systems: gas central (\\(gc\\)) gas room (\\(gr\\)) electric central (\\(ec\\)) electric room (\\(er\\)) heat pump (\\(hp\\)) There are 900 observations with the following variables: \\(idcase\\) gives the observation number (1-900), \\(depvar\\) identifies the chosen alternative (gc, gr, ec, er, hp), \\(ic.alt\\) is the installation cost for the 5 alternatives, \\(oc.alt\\) is the annual operating cost for the 5 alternatives, \\(income\\) is the annual income of the household, \\(agehed\\) is the age of the household head, \\(rooms\\) is the number of rooms in the house, \\(region\\) a factor with levels \\(ncostl\\) (northern coastal region), \\(scostl\\) (southern coastal region), \\(mountn\\) (mountain region), \\(valley\\) (central valley region). Estimate a multinomial logit model with installation and operating cost as the alternative-specific variables and income, age, and number of rooms as the individual-specific variables. "],["limited-dependent-variable-models.html", "9 Limited Dependent Variable Models", " 9 Limited Dependent Variable Models This chapter covers three regression models in which the dependent variable is somehow limited: Truncation: With truncated data, the researcher does not observe values past a particular point and those values are also not reported. Examples of truncation are low-income household studies, on-site visitation data, or time-of-use pricing experiments (excludes low-usage households). Censoring: In the case of censoring, values that are above or below a certain value are replaced by that value. For example, the demand for a particular class is not fully observed (absence of a waiting list). This is also called a Tobit model. Count dependent variable The following packages are necessary for this section: AER truncreg, censReg, and pscl. Truncation and censoring lead to a bias in the estimates. It is not always clear why or if the data is limited in its range. "],["truncation.html", "9.1 Truncation", " 9.1 Truncation In the case of truncation, a certain part of the data is not observed. In the graph below, the true parameters are \\(\\beta_0=-2\\) and \\(\\beta_1=0.5\\). Values \\(y&lt;0\\) are not reported in the data. The green regression line is correct whereas the red is the line obtained from a regression model which ignores the truncation. If all the data was observed, the correct regression model would give the following results: summary(bhat_real) ## ## Call: ## lm(formula = y_real ~ x, data = truncation) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9430 -0.7735 0.1659 0.6512 2.3772 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.50451 0.35042 -4.293 8.50e-05 *** ## x 0.42639 0.05861 7.275 2.79e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.109 on 48 degrees of freedom ## Multiple R-squared: 0.5244, Adjusted R-squared: 0.5145 ## F-statistic: 52.92 on 1 and 48 DF, p-value: 2.793e-09 The estimates are biased if truncation is ignored: summary(bhat_truncated) ## ## Call: ## lm(formula = y_obs ~ x, data = truncation) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9254 -0.7751 -0.2127 0.6512 2.1546 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.31318 0.53268 -0.588 0.56046 ## x 0.28021 0.07821 3.583 0.00105 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.046 on 34 degrees of freedom ## (14 observations deleted due to missingness) ## Multiple R-squared: 0.2741, Adjusted R-squared: 0.2527 ## F-statistic: 12.84 on 1 and 34 DF, p-value: 0.001051 To correct for the truncation, use the functions from the package truncreg which allows to reduce the bias of the coefficients: bhat_correct = truncreg(y_obs~x,data=truncation) summary(bhat_correct) ## ## Call: ## truncreg(formula = y_obs ~ x, data = truncation) ## ## BFGS maximization method ## 33 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 1.7E-08 ## ## ## ## Coefficients : ## Estimate Std. Error t-value Pr(&gt;|t|) ## (Intercept) -3.71422 2.35633 -1.5763 0.11496 ## x 0.63549 0.25539 2.4884 0.01283 * ## sigma 1.48284 0.36736 4.0364 5.427e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -43.289 on 3 Df "],["censoring.html", "9.2 Censoring", " 9.2 Censoring In the case of censoring, the values of the dependent variable are reported at a certain point if they are above or below a certain value. If all data was reported at the correct value, the following following regression model could be executed: summary(bhat_real) ## ## Call: ## lm(formula = y_real ~ x, data = censoring) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9280 -0.7849 -0.1564 0.6311 2.4465 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.32539 0.26128 -5.073 6.29e-06 *** ## x 0.43593 0.04634 9.407 1.80e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9921 on 48 degrees of freedom ## Multiple R-squared: 0.6483, Adjusted R-squared: 0.641 ## F-statistic: 88.49 on 1 and 48 DF, p-value: 1.803e-12 Ignoring censoring leads to biased results: summary(bhat_censored) ## ## Call: ## lm(formula = y_obs ~ x, data = censoring) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1565 -0.6446 -0.1244 0.3530 2.3600 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.44801 0.21654 -2.069 0.044 * ## x 0.32198 0.03841 8.383 5.84e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8222 on 48 degrees of freedom ## Multiple R-squared: 0.5942, Adjusted R-squared: 0.5857 ## F-statistic: 70.28 on 1 and 48 DF, p-value: 5.836e-11 Using the R package censReg) allows for the reduction of the bias: b_correct = censReg(y_obs~x,data=censoring) summary(b_correct) ## ## Call: ## censReg(formula = y_obs ~ x, data = censoring) ## ## Observations: ## Total Left-censored Uncensored Right-censored ## 50 19 31 0 ## ## Coefficients: ## Estimate Std. error t value Pr(&gt; t) ## (Intercept) -1.64928 0.40134 -4.109 3.97e-05 *** ## x 0.47892 0.06260 7.651 2.00e-14 *** ## logSigma 0.04512 0.13033 0.346 0.729 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Newton-Raphson maximisation, 6 iterations ## Return code 1: gradient close to zero (gradtol) ## Log-likelihood: -54.15188 on 3 Df "],["count-regression-models.html", "9.3 Count Regression Models", " 9.3 Count Regression Models Count regression models apply to cases in which the dependent variable represents discrete, integer count data. Here are some examples of count outcomes: What are the number of arrests for a person? What determines the number of credit cards a person owns? This section on count regression presents three models: Poisson Regression Model: The condition to use this model is the absence of overdispersion, i.e., the expected value of the dependent variable is equal to the variance. Quasi-Poisson Regression Model: Overdispersion occurs if the variance of the dependent variable is larger than its mean. In this case, the Poisson Regression Model leads to unreliable hypothesis tests regarding the coefficients. The Quasi-Poisson Model solves this issue. Negative Binomial Regression Model: The second possibility to deal with overdispersion is to use a Negative Binomial Regression Model. The main package used is pscl. There is also an additional resource with more theoretical details on the topic: Regression Models for Count Data in R. A more up-to-date version of the document may be found with the pscl package documentation. 9.3.1 Poisson Regression Model Recall that the Poisson distribution is written as: \\[Pr(Y=k)=\\frac{e^{-\\lambda} \\cdot \\lambda^k}{k!}\\] The important characteristics of the distribution is that the mean and variance are equal to \\(\\lambda\\), i.e., \\(E(Y)=\\lambda\\) and \\(Var(Y)=\\lambda\\), this is also known as the equidispersion property. The mean parameter is written as \\(\\lambda=exp(\\beta_0+\\beta_1 \\cdot x_1+ \\dots + \\beta_k \\cdot x_k)\\). The first examples uses the 2017 NHTS data contained in hhpub. In a first step, the data is prepared for the regression model, i.e., missing or unknown values are eliminated and income is measured in (thousand) dollar terms: hhpubdata = subset(hhpub,HHFAMINC %in% c(1:11) &amp; HOMEOWN %in% c(1,2) &amp; URBRUR %in% c(1,2) &amp; HHVEHCNT %in% c(0:12)) HHFAMINC = c(1:11) INCOME = c(10,12.5,20,30,42.5,57.5,82.5,112.5,137.5,175,200) INCOME = data.frame(HHFAMINC,INCOME) hhpubdata = merge(hhpubdata,INCOME) hhpubdata$RURAL = hhpubdata$URBRUR-1 hhpubdata$RENT = hhpubdata$HOMEOWN-1 The outcome of interest is the number of vehicles based on household income, home ownership, and urban/rural household location. Before executing the Poisson regression model, calculate the mean and variance of the outcome variable. ## [1] 1.981142 ## [1] 1.386027 The mean and the variance are similar and thus, estimating a Poisson Regression Model is an appropriate first step. The package AER also contains the function dispersiontest() which conducts a hypothesis test assuming no overdispersion. This test will be used after execution of the main regression. To estimate the model, the glm() function can be used by specifying family=poisson. bhat_pois = glm(HHVEHCNT~INCOME+RENT+RURAL,data=hhpubdata,family=poisson) summary(bhat_pois) ## ## Call: ## glm(formula = HHVEHCNT ~ INCOME + RENT + RURAL, family = poisson, ## data = hhpubdata) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.6889 -0.5568 -0.1558 0.3590 5.5063 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.654e-01 4.292e-03 108.43 &lt;2e-16 *** ## INCOME 2.986e-03 3.601e-05 82.93 &lt;2e-16 *** ## RENT -3.733e-01 5.797e-03 -64.39 &lt;2e-16 *** ## RURAL 2.224e-01 4.616e-03 48.19 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 86505 on 124400 degrees of freedom ## Residual deviance: 68533 on 124397 degrees of freedom ## AIC: 370161 ## ## Number of Fisher Scoring iterations: 5 All coefficients are statistically significant. The signs associated with the coefficients indicates the direction of influence on the outcome variable, i.e., the number of cars. As expected, higher income is associated with a higher number of cars and so is living in a rural environment. Renting is associated with a lower number of vehicles. Note that income and renting is possibly correlated. In general, the coefficient estimates are interpreted using \\(\\exp(\\beta)\\). That is, with every unit increase in \\(X\\), the predictor variable has a multiplicative effect of \\(\\exp(\\beta)\\) on the mean of \\(Y\\), i.e., \\(\\lambda\\): If \\(\\beta=0\\), then \\(\\exp(\\beta) = 1\\), and the expected count \\(\\mu= E(y) = \\exp(\\alpha)\\), and Y and X are not related. If \\(\\beta&gt;0\\), then \\(\\exp(\\beta) &gt; 1\\), and the expected count \\(E(y)\\) is \\(\\exp(\\beta)\\) times larger than when \\(X = 0\\) If \\(\\beta&lt;0\\), then \\(\\exp(\\beta) &lt; 1\\), and the expected count \\(E(y)\\) is \\(\\exp(\\beta)\\) times smaller than when \\(X = 0\\) The function overdispersion tests the null hypothesis of equidispersion: dispersiontest(bhat_pois) ## ## Overdispersion test ## ## data: bhat_pois ## z = -115.75, p-value = 1 ## alternative hypothesis: true dispersion is greater than 1 ## sample estimates: ## dispersion ## 0.5670593 Given the p-value, the null hypothesis cannot be rejected. If the data suggests overdispersion, two alternative regression models can be used: (1) Quasi-Poisson and (2) Negative Binomial. 9.3.2 Quasi-Poisson Regression Model The dataset blm used in this section as well as the one describing the negative binomial model is associated with the article Black Lives Matter: Evidence that Police-Caused Deaths Predict Protest Activity. Note that the paper includes a significant number of supplementary materials which allows for the replication of the results and much more. The dependent variable is the total number of protests in a city. In a first step, the mean and variance of the variable \\(totalprotests\\) are calclated: mean(blm$totprotests) ## [1] 0.4959529 var(blm$totprotests) ## [1] 6.35326 The variance is significantly higher than the mean which suggests overdispersion. In a first step, a regular Poisson model is estimated. eq1 = &quot;totprotests~log(pop)+log(popdensity)+percentblack+blackpovertyrate+I(blackpovertyrate^2)+percentbachelor+collegeenrollpc+demshare&quot; bhat1 = glm(eq1,data=blm,family=poisson) summary(bhat1) ## ## Call: ## glm(formula = eq1, family = poisson, data = blm) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.6571 -0.5238 -0.3008 -0.1632 6.5795 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.001e+01 6.327e-01 -31.625 &lt; 2e-16 *** ## log(pop) 1.129e+00 4.007e-02 28.170 &lt; 2e-16 *** ## log(popdensity) -1.831e-01 8.654e-02 -2.116 0.0343 * ## percentblack 1.697e-02 3.104e-03 5.467 4.59e-08 *** ## blackpovertyrate 1.461e-01 2.636e-02 5.541 3.02e-08 *** ## I(blackpovertyrate^2) -1.552e-03 3.985e-04 -3.895 9.82e-05 *** ## percentbachelor 3.893e-02 3.918e-03 9.935 &lt; 2e-16 *** ## collegeenrollpc 9.305e-03 2.377e-03 3.914 9.06e-05 *** ## demshare 4.301e-02 5.293e-03 8.126 4.43e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 3204.6 on 1225 degrees of freedom ## Residual deviance: 787.4 on 1217 degrees of freedom ## (133 observations deleted due to missingness) ## AIC: 1242.9 ## ## Number of Fisher Scoring iterations: 6 Note that the signs and statistical significance correspond to Model 1 (Table 3) in the original paper. The coefficients are different because the paper only include negative binomial regression models. The reason for not using a Poisson regression model is the presence of overdispersion: dispersiontest(bhat1) ## ## Overdispersion test ## ## data: bhat1 ## z = 1.4052, p-value = 0.07998 ## alternative hypothesis: true dispersion is greater than 1 ## sample estimates: ## dispersion ## 2.212733 Note that the null hypothesis is rejected at the 10% but not the 5% significance level. The Quasi-Poisson Regression Model handles overdispersion by adjusting the stand-errors but leaving the coefficicent estimates the same. bhat2 = glm(eq1,data=blm,family=quasipoisson) summary(bhat2) ## ## Call: ## glm(formula = eq1, family = quasipoisson, data = blm) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.6571 -0.5238 -0.3008 -0.1632 6.5795 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.001e+01 9.841e-01 -20.332 &lt; 2e-16 *** ## log(pop) 1.129e+00 6.232e-02 18.111 &lt; 2e-16 *** ## log(popdensity) -1.831e-01 1.346e-01 -1.360 0.173942 ## percentblack 1.697e-02 4.828e-03 3.515 0.000457 *** ## blackpovertyrate 1.461e-01 4.100e-02 3.562 0.000382 *** ## I(blackpovertyrate^2) -1.552e-03 6.198e-04 -2.504 0.012403 * ## percentbachelor 3.893e-02 6.094e-03 6.387 2.40e-10 *** ## collegeenrollpc 9.305e-03 3.697e-03 2.517 0.011975 * ## demshare 4.301e-02 8.233e-03 5.225 2.05e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 2.419275) ## ## Null deviance: 3204.6 on 1225 degrees of freedom ## Residual deviance: 787.4 on 1217 degrees of freedom ## (133 observations deleted due to missingness) ## AIC: NA ## ## Number of Fisher Scoring iterations: 6 Note that in this case, the population density is not statistically significant anymore. 9.3.3 Negative Binomial Regression Model The Negative Binomial Regression Model can be used in the presence of count data and overdispersion. Below, the results from the article Black Lives Matter: Evidence that Police-Caused Deaths Predict Protest Activity are recreated using the negative binomial models presented in the paper. The required package is MASS The first model is a basic model of resource mobilization and opportunity structure. bhat3 = glm.nb(eq1,data=blm,link=log) summary(bhat3) ## ## Call: ## glm.nb(formula = eq1, data = blm, link = log, init.theta = 1.559078735) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2523 -0.4543 -0.2673 -0.1506 5.7386 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.090e+01 1.117e+00 -18.719 &lt; 2e-16 *** ## log(pop) 1.292e+00 7.159e-02 18.047 &lt; 2e-16 *** ## log(popdensity) -3.130e-01 1.328e-01 -2.356 0.01848 * ## percentblack 2.249e-02 4.709e-03 4.777 1.78e-06 *** ## blackpovertyrate 1.319e-01 3.121e-02 4.227 2.37e-05 *** ## I(blackpovertyrate^2) -1.340e-03 4.768e-04 -2.810 0.00496 ** ## percentbachelor 4.462e-02 5.465e-03 8.163 3.26e-16 *** ## collegeenrollpc 1.051e-02 4.118e-03 2.553 0.01068 * ## demshare 4.077e-02 7.292e-03 5.591 2.26e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(1.5591) family taken to be 1) ## ## Null deviance: 1899.84 on 1225 degrees of freedom ## Residual deviance: 501.15 on 1217 degrees of freedom ## (133 observations deleted due to missingness) ## AIC: 1120.2 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 1.559 ## Std. Err.: 0.351 ## ## 2 x log-likelihood: -1100.187 In a second model, black deaths are added: bhat4 = glm.nb(totprotests~log(pop)+log(popdensity)+percentblack+blackpovertyrate+I(blackpovertyrate^2)+percentbachelor+collegeenrollpc+demshare+deathsblackpc,data=blm,link=log) summary(bhat4) ## ## Call: ## glm.nb(formula = totprotests ~ log(pop) + log(popdensity) + percentblack + ## blackpovertyrate + I(blackpovertyrate^2) + percentbachelor + ## collegeenrollpc + demshare + deathsblackpc, data = blm, link = log, ## init.theta = 1.685551835) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3170 -0.4549 -0.2707 -0.1529 5.3090 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.073e+01 1.101e+00 -18.824 &lt; 2e-16 *** ## log(pop) 1.281e+00 7.046e-02 18.178 &lt; 2e-16 *** ## log(popdensity) -3.054e-01 1.315e-01 -2.323 0.020201 * ## percentblack 1.801e-02 4.864e-03 3.704 0.000212 *** ## blackpovertyrate 1.283e-01 3.109e-02 4.127 3.67e-05 *** ## I(blackpovertyrate^2) -1.301e-03 4.743e-04 -2.744 0.006071 ** ## percentbachelor 4.372e-02 5.381e-03 8.125 4.47e-16 *** ## collegeenrollpc 1.005e-02 4.073e-03 2.466 0.013657 * ## demshare 4.069e-02 7.249e-03 5.613 1.98e-08 *** ## deathsblackpc 2.825e+00 9.312e-01 3.034 0.002414 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(1.6856) family taken to be 1) ## ## Null deviance: 1943.21 on 1225 degrees of freedom ## Residual deviance: 500.18 on 1216 degrees of freedom ## (133 observations deleted due to missingness) ## AIC: 1113.4 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 1.686 ## Std. Err.: 0.404 ## ## 2 x log-likelihood: -1091.353 And in the third model, the authors use all police-caused deaths instead (victims of any race): bhat5 = glm.nb(totprotests~log(pop)+log(popdensity)+percentblack+blackpovertyrate+I(blackpovertyrate^2)+percentbachelor+collegeenrollpc+demshare+deathspc,data=blm,link=log) summary(bhat5) ## ## Call: ## glm.nb(formula = totprotests ~ log(pop) + log(popdensity) + percentblack + ## blackpovertyrate + I(blackpovertyrate^2) + percentbachelor + ## collegeenrollpc + demshare + deathspc, data = blm, link = log, ## init.theta = 1.621799986) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3230 -0.4517 -0.2684 -0.1505 5.6311 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.080e+01 1.108e+00 -18.773 &lt; 2e-16 *** ## log(pop) 1.277e+00 7.122e-02 17.928 &lt; 2e-16 *** ## log(popdensity) -3.121e-01 1.321e-01 -2.363 0.01812 * ## percentblack 2.163e-02 4.705e-03 4.596 4.31e-06 *** ## blackpovertyrate 1.295e-01 3.110e-02 4.164 3.12e-05 *** ## I(blackpovertyrate^2) -1.315e-03 4.744e-04 -2.772 0.00558 ** ## percentbachelor 4.507e-02 5.472e-03 8.237 &lt; 2e-16 *** ## collegeenrollpc 1.040e-02 4.094e-03 2.540 0.01108 * ## demshare 4.121e-02 7.257e-03 5.678 1.36e-08 *** ## deathspc 9.564e-01 6.330e-01 1.511 0.13085 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(1.6218) family taken to be 1) ## ## Null deviance: 1921.82 on 1225 degrees of freedom ## Residual deviance: 502.82 on 1216 degrees of freedom ## (133 observations deleted due to missingness) ## AIC: 1119.8 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 1.622 ## Std. Err.: 0.374 ## ## 2 x log-likelihood: -1097.839 "],["hurdle-and-zero-inflation-models.html", "9.4 Hurdle and Zero-Inflation Models", " 9.4 Hurdle and Zero-Inflation Models Count data often includes many observations at 0 which can lead to problems using a Poisson or a Negative-Binomial Regression Model. The application of both models is first illustrated with the NMES1988 data from the package AER and then with the BLM protest data. The data NMES1988 contains 4406 observations of people on Medicare who are 66 years or older. The outcome of interest is the number of doctor \\(visits\\) as a function of \\(hospital\\) (number of hospital visits), \\(health\\) (self-indicated health status), \\(chronic\\) (number of chronic conditions), \\(gender\\), \\(school\\), and \\(insurance\\). data(&quot;NMES1988&quot;,package=&quot;AER&quot;) eq = visits~hospital+health+chronic+gender+school+insurance bhat_pois = glm(eq,data=NMES1988,family=poisson) summary(bhat_pois) ## ## Call: ## glm(formula = eq, family = poisson, data = NMES1988) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -8.4055 -1.9962 -0.6737 0.7049 16.3620 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.028874 0.023785 43.258 &lt;2e-16 *** ## hospital 0.164797 0.005997 27.478 &lt;2e-16 *** ## healthpoor 0.248307 0.017845 13.915 &lt;2e-16 *** ## healthexcellent -0.361993 0.030304 -11.945 &lt;2e-16 *** ## chronic 0.146639 0.004580 32.020 &lt;2e-16 *** ## gendermale -0.112320 0.012945 -8.677 &lt;2e-16 *** ## school 0.026143 0.001843 14.182 &lt;2e-16 *** ## insuranceyes 0.201687 0.016860 11.963 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 26943 on 4405 degrees of freedom ## Residual deviance: 23168 on 4398 degrees of freedom ## AIC: 35959 ## ## Number of Fisher Scoring iterations: 5 bhat_nb = glm(eq,data=NMES1988) summary(bhat_nb) ## ## Call: ## glm(formula = eq, data = NMES1988) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -20.582 -3.500 -1.402 1.693 73.341 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.63203 0.33480 4.875 1.13e-06 *** ## hospital 1.61976 0.13264 12.211 &lt; 2e-16 *** ## healthpoor 1.84532 0.31234 5.908 3.72e-09 *** ## healthexcellent -1.33140 0.36257 -3.672 0.000243 *** ## chronic 0.94440 0.07693 12.276 &lt; 2e-16 *** ## gendermale -0.63185 0.19454 -3.248 0.001171 ** ## school 0.14345 0.02726 5.262 1.49e-07 *** ## insuranceyes 1.10397 0.24362 4.532 6.01e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 40.02228) ## ## Null deviance: 201252 on 4405 degrees of freedom ## Residual deviance: 176018 on 4398 degrees of freedom ## AIC: 28769 ## ## Number of Fisher Scoring iterations: 2 bhat_hurdle = hurdle(eq,data=NMES1988,dist=&quot;negbin&quot;) summary(bhat_hurdle) ## ## Call: ## hurdle(formula = eq, data = NMES1988, dist = &quot;negbin&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -1.1718 -0.7080 -0.2737 0.3196 18.0092 ## ## Count model coefficients (truncated negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.197699 0.058973 20.309 &lt; 2e-16 *** ## hospital 0.211898 0.021396 9.904 &lt; 2e-16 *** ## healthpoor 0.315958 0.048056 6.575 4.87e-11 *** ## healthexcellent -0.331861 0.066093 -5.021 5.14e-07 *** ## chronic 0.126421 0.012452 10.152 &lt; 2e-16 *** ## gendermale -0.068317 0.032416 -2.108 0.0351 * ## school 0.020693 0.004535 4.563 5.04e-06 *** ## insuranceyes 0.100172 0.042619 2.350 0.0188 * ## Log(theta) 0.333255 0.042754 7.795 6.46e-15 *** ## Zero hurdle model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.043147 0.139852 0.309 0.757688 ## hospital 0.312449 0.091437 3.417 0.000633 *** ## healthpoor -0.008716 0.161024 -0.054 0.956833 ## healthexcellent -0.289570 0.142682 -2.029 0.042409 * ## chronic 0.535213 0.045378 11.794 &lt; 2e-16 *** ## gendermale -0.415658 0.087608 -4.745 2.09e-06 *** ## school 0.058541 0.011989 4.883 1.05e-06 *** ## insuranceyes 0.747120 0.100880 7.406 1.30e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta: count = 1.3955 ## Number of iterations in BFGS optimization: 16 ## Log-likelihood: -1.209e+04 on 17 Df "],["survival-analysis.html", "9.5 Survival Analysis", " 9.5 Survival Analysis "],["exercises-4.html", "9.6 Exercises", " 9.6 Exercises Aptitude Tobit Model: Consider the censored data set in tobit. The aptitude score is limited at 800. Estimate an appropriate model with \\(apt\\) as the dependent variable and \\(read\\), \\(math\\), and program as the independent variables. Make sure to convert \\(prog\\) into dummy variables first. Chicago Grocery Stores: One subdivision of Chicago are so-called Chicago Community Areas (CCA). The data in chicagogrocery includes data about the number of grocery stores (\\(stores\\)) in each CCA as well as demographic information. Estimate a Poisson and Negative Binomial Regression Model with \\(stores\\) as the dependent variable and the following independent variables: \\(income\\), \\(pop\\), unemployment rate (\\(unemployed/laborforce\\)) and percentage of blacks (\\(black/pop\\)). What do you conclude? Are the results what you would expect? "],["panel-data.html", "10 Panel Data", " 10 Panel Data This chapter introduces panel data which are observations of the same unit of analysis over time, e.g., people, cities, or countries. So far, only cross-sectional data was analyzed. This chapter on panel data starts to incorporate time aspects into the estimation procedure. The required packages are plm and lmtest. The plm also contains some excellent documentation on the theoretical aspects of panel data as well as on the use of the package. "],["overview-1.html", "10.1 Overview", " 10.1 Overview Two types of data must be distinguished: Pooled data: Combination of multiple cross-sectional data over time Two or more different observational units over time Grades in an economics class based on students concentration combined from multiple semesters American Community Survey (ACS) Panel data: Repeated measurement on the same individual \\(i\\) over time \\(t\\). Individual units can be people, states, firms, counties, countries, etc. National Longitudinal Survey (NLSY79): To access the data: Accessing Data &gt; Investigator &gt; Begin searching as guest. Necessary adjustments of standard error due to correlation across time. There are some necessary assumptions about linear panel models. Regular time intervals Errors are correlated Parameters may vary across individuals or time Intercept: Individual specific effects model (fixed or random) Note that the General Social Survey (GSS) is not a panel data set because different respondents are questioned every year. Besides the National Longitudinal Survey mentioned above, here are some additional examples of panel data sets: Panel Study of Income Dynamics (PSID): Data on approximately 5,000 families on various socioeconomic and demographic variables Survey of Income and Program Participation (SIPP): Interviews about economic condition of respondents Panel models have the advantage that they take into account heterogeneity among observational units, e.g., firms, states, counties. Those models also contribute to the better understanding on the dynamics of change for observational units over time. Panel data combines cross-sectional data with time series data leading to more complete behavioral models. Balanced versus unbalance panel: A balanced panel has the same number of time-series observations for each subject or observational unit, whereas an unbalanced panel does not. Short versus long panel: A short panel has a larger number of subjects or observational units than there are time periods. A long panel has a greater number of time periods than observational units. There are three types of regression models presented in this chapter: Pooled Ordinary Least Square model Fixed Effects Panel Data Model Random Effects Panel Data Model "],["pooled-ordinary-least-square-model.html", "10.2 Pooled Ordinary Least Square model", " 10.2 Pooled Ordinary Least Square model The first example is data from the General Social Survey (GSS). Recall that the GSS is not a panel data set since the respondents change from one year to the next. The years analyzed for this example are the even years between 1974 and 1984. Note that this data set is accompanying the book Introductory Econometrics: A Modern Approach by Jeffrey Wooldridge. bhat = lm(kids~educ+age+I(age^2)+east+northcentral+west+farm+otherrural +town+smallcity+y74+y76+y78+y80+y82+y84,data=fertil1) summary(bhat) ## ## Call: ## lm(formula = kids ~ educ + age + I(age^2) + east + northcentral + ## west + farm + otherrural + town + smallcity + y74 + y76 + ## y78 + y80 + y82 + y84, data = fertil1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.1437 -1.0481 -0.1082 0.9450 5.1055 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.785069 3.098715 -2.190 0.028758 * ## educ -0.129765 0.018653 -6.957 5.94e-12 *** ## age 0.499186 0.140592 3.551 0.000400 *** ## I(age^2) -0.005436 0.001589 -3.421 0.000648 *** ## east 0.060729 0.132538 0.458 0.646896 ## northcentral 0.219568 0.120638 1.820 0.069019 . ## west 0.050807 0.167982 0.302 0.762360 ## farm -0.109598 0.149353 -0.734 0.463217 ## otherrural -0.199208 0.178270 -1.117 0.264043 ## town 0.058579 0.126538 0.463 0.643502 ## smallcity 0.221122 0.162964 1.357 0.175095 ## y74 0.240846 0.175541 1.372 0.170334 ## y76 -0.139590 0.181902 -0.767 0.443012 ## y78 -0.104546 0.184622 -0.566 0.571324 ## y80 -0.086997 0.185803 -0.468 0.639718 ## y82 -0.414209 0.174412 -2.375 0.017723 * ## y84 -0.565326 0.177398 -3.187 0.001479 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.581 on 1112 degrees of freedom ## Multiple R-squared: 0.09941, Adjusted R-squared: 0.08645 ## F-statistic: 7.671 on 16 and 1112 DF, p-value: &lt; 2.2e-16 The evolution of fertility rates over time after controlling of other observable factors can be interpreted as follows: Base year: 1972 Negative coefficients indicate a drop in fertility in the early 1980s Coefficient of \\(y82\\) (-0.41) indicates that women had on average 0.41 less children, i.e., 100 women had 41 kids less than 1972. This drop is independent from education since we are controlling for education. More educated women have fewer children Assumes that the effect of each explanatory variable remains constant. The next example uses cps7885 and interacts year dummy with key explanatory variables to see if the effect of that variable has changed over time. That is, the following model is estimated: \\[\\ln(wage)=\\beta_0+\\gamma_0 \\cdot y85+\\beta_1 \\cdot educ+\\gamma_1 \\cdot y85 \\cdot educ+\\beta_2 \\cdot exper+ \\beta_3 \\cdot exper^2+\\beta_4 \\cdot union+\\beta_5 \\cdot female+\\gamma_5 \\cdot y85 \\cdot female\\] bhat = lm(formula=lwage~y85+educ+y85*educ+exper+expersq+union+female+y85fem,data=cps7885) summary(bhat) ## ## Call: ## lm(formula = lwage ~ y85 + educ + y85 * educ + exper + expersq + ## union + female + y85fem, data = cps7885) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.56098 -0.25828 0.00864 0.26571 2.11669 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.589e-01 9.345e-02 4.911 1.05e-06 *** ## y85 1.178e-01 1.238e-01 0.952 0.3415 ## educ 7.472e-02 6.676e-03 11.192 &lt; 2e-16 *** ## exper 2.958e-02 3.567e-03 8.293 3.27e-16 *** ## expersq -3.994e-04 7.754e-05 -5.151 3.08e-07 *** ## union 2.021e-01 3.029e-02 6.672 4.03e-11 *** ## female -3.167e-01 3.662e-02 -8.648 &lt; 2e-16 *** ## y85fem 8.505e-02 5.131e-02 1.658 0.0977 . ## y85:educ 1.846e-02 9.354e-03 1.974 0.0487 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4127 on 1075 degrees of freedom ## Multiple R-squared: 0.4262, Adjusted R-squared: 0.4219 ## F-statistic: 99.8 on 8 and 1075 DF, p-value: &lt; 2.2e-16 This model can be interpreted as follows: \\(\\beta_0\\) is the 1978 intercept \\(\\beta_0+\\gamma_0\\) is the 1985 intercept \\(\\beta_1\\) is the return to education in 1978 \\(\\beta_1 + \\gamma_1\\) is the return to education in 1985 \\(\\gamma_1\\) measures how the return to education has changed over the seven year period 1978 return to education: 7.47% 1985 return to education: 7.47%+1.85% = 9.32% 1978 gender gap: 31.67% 1985 gender gap: 31.67% - 8.51% = 23.16% The last example regarding pooled data illustrates how misleading a regression model can be if executed incorrectly. The data set is called kiel and is on home values near the location of an garbage incinerator. The important aspect of the data set is that there was no knowledge about the proposed incinerator in 1978. In a first step, the data is separated into the two years: kiel1978 = subset(kiel,year==1978) kiel1981 = subset(kiel,year==1981) Next, two regressions for each of the years are estimated. summary(lm(rprice~nearinc,data=kiel1981)) ## ## Call: ## lm(formula = rprice ~ nearinc, data = kiel1981) ## ## Residuals: ## Min 1Q Median 3Q Max ## -60678 -19832 -2997 21139 136754 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 101308 3093 32.754 &lt; 2e-16 *** ## nearinc -30688 5828 -5.266 5.14e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 31240 on 140 degrees of freedom ## Multiple R-squared: 0.1653, Adjusted R-squared: 0.1594 ## F-statistic: 27.73 on 1 and 140 DF, p-value: 5.139e-07 summary(lm(rprice~nearinc,data=kiel1978)) ## ## Call: ## lm(formula = rprice ~ nearinc, data = kiel1978) ## ## Residuals: ## Min 1Q Median 3Q Max ## -56517 -16605 -3193 8683 236307 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 82517 2654 31.094 &lt; 2e-16 *** ## nearinc -18824 4745 -3.968 0.000105 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29430 on 177 degrees of freedom ## Multiple R-squared: 0.08167, Adjusted R-squared: 0.07648 ## F-statistic: 15.74 on 1 and 177 DF, p-value: 0.0001054 The results can be used for a difference-in-difference estimator: -$30,688-(-$18,824)=-$11,864. Expressed differently: \\[\\hat{\\delta}_1 = (price_{81,near}-price_{81,far})-(price_{78,near}-price_{78,far})\\] where \\(\\hat{\\delta}_1\\) represents the difference over time in average differences in housing prices in the two locations. To determine statistical significance, the following model must be estimated: \\[price = \\beta_0 + \\gamma_0 \\cdot y81 + \\beta_1 \\cdot nearinc + \\gamma_1 \\cdot y81 \\cdot nearinc\\] The interpretation of the coefficients is as follows: \\(\\beta_0\\): Average home value which is not near the garbage incinerator \\(\\gamma_0 \\cdot y81\\): Average change in housing values for all homes \\(\\beta_1 \\cdot nearinc\\): Location effect that is not due to the incinerator \\(\\gamma_1\\): Decline in housing values due to incinerator Include \\(age\\) and \\(age^2\\) in the above equation to take advantage of the information provided in the data. Other variables such as \\(cbd\\), \\(rooms\\), \\(area\\), \\(land\\), and \\(baths\\) can be added as well. In general, the results show that homes have lost 9.3% in values when including additional independent variables and using the natural logarithm of price. "],["fixed-effects-panel-data-model.html", "10.3 Fixed Effects Panel Data Model", " 10.3 Fixed Effects Panel Data Model The two sections on fixed and random effects panel data model use a very old data set but which is standard in all panel data texts. The data set is called grunfeld and is part of the package plm. The data contains the following variables of 10 companies over the period 1935 to 1954: \\(inv\\): Investment \\(value\\): Value of the firm \\(capital\\): Capital stock The companies of interest for this chapter are GM (firm 1), U.S. Steel (firm 2), GE (firm 3), and Westinghouse (firm 8). grunfeld = subset(grunfeld,grunfeld$firm %in% c(1,2,3,8)) In a first step, a pooled model is executed, i.e., all cross-sectional and time series observations are combined into a single data set. \\[inv_i = \\beta_0 + \\beta_1 \\cdot value_i + \\beta_2 \\cdot capital_i\\] The general formulation of the pooled model: \\[y_{it}=\\beta_0+\\beta_1 \\cdot x_i + \\epsilon_i\\] There are multiple issues associated with a pooled OLS model: Ignores heterogeneity among the observations and time. Presence of heterogeneity: Correlation between independent variables and error term leads to biased and inconsistent coefficient estimates. Solution: Fixed effects model takes heterogeneity into account. \\(\\Rightarrow\\) Autocorrelation between error terms. Fix: Random effects model To use the functions from plm, define the data as a panel data set: grunfeld = pdata.frame(grunfeld,index=c(&quot;firm&quot;,&quot;year&quot;)) There are two possibilities to execute a pooled OLS Model. Use the regular lm() function or use plm() specifying the model as pooling. The outputs will be names grunwald.ols and grunwald.pooling. summary(plm(inv~value+capital,data=grunfeld,model=&quot;pooling&quot;)) ## Pooling Model ## ## Call: ## plm(formula = inv ~ value + capital, data = grunfeld, model = &quot;pooling&quot;) ## ## Balanced Panel: n = 4, T = 20, N = 80 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -319.6766 -99.9523 1.9647 65.9905 336.2072 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## (Intercept) -62.831841 29.725385 -2.1137 0.03778 * ## value 0.110521 0.013776 8.0230 9.186e-12 *** ## capital 0.300463 0.049399 6.0823 4.273e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 6410400 ## Residual Sum of Squares: 1572700 ## R-Squared: 0.75466 ## Adj. R-Squared: 0.74829 ## F-statistic: 118.424 on 2 and 77 DF, p-value: &lt; 2.22e-16 %======================================================================================================================================== summary(plm(inv~value+capital,data=grunfeld,model=&quot;within&quot;)) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = inv ~ value + capital, data = grunfeld, model = &quot;within&quot;) ## ## Balanced Panel: n = 4, T = 20, N = 80 ## ## Residuals: ## Min. 1st Qu. Median 3rd Qu. Max. ## -184.6581 -48.2612 9.3252 40.5471 197.6681 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## value 0.108400 0.017566 6.1711 3.3e-08 *** ## capital 0.345058 0.026708 12.9195 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 2171500 ## Residual Sum of Squares: 422220 ## R-Squared: 0.80556 ## Adj. R-Squared: 0.79242 ## F-statistic: 153.291 on 2 and 74 DF, p-value: &lt; 2.22e-16 %======================================================================================================================================== %======================================================================================================================================== %======================================================================================================================================== %======================================================================================================================================== %======================================================================================================================================== %======================================================================================================================================== %\\begin{frame}[fragile]{Testing for Heteroscedasticity} %\\begin{verbatim} %bptest(inv~value+capital+factor(firm),data=grunfeld) % % studentized Breusch-Pagan test % %data: inv ~ value + capital + factor(firm) %BP = 25.375, df = 5, p-value = 0.0001179 %\\end{verbatim} %If the p-value is below 0.05, then we face heteroscedasticity. %\\end{frame} %======================================================================================================================================== %\\begin{frame}[fragile]{Heteroscedasticity Consistent Coefficients and Standard Errors} %\\begin{verbatim} %coeftest(grunfeld.fixed,vcovHC) % %t test of coefficients: % % Estimate Std. Error t value Pr(&gt;|t|) %value 0.108400 0.014293 7.5839 7.902e-11 %capital 0.345058 0.031152 11.0765 &lt; 2.2e-16 % %Signif. codes: 0  0.001  0.01  0.05 . 0.1   1 % %coeftest(grunfeld.random,vcovHC) % %t test of coefficients: % % Estimate Std. Error t value Pr(&gt;|t|) %(Intercept) -73.084676 47.566742 -1.5365 0.1285 %value 0.108056 0.012149 8.8945 1.915e-13 %capital 0.344543 0.032276 10.6750 &lt; 2.2e-16 % %Signif. codes: 0  0.001  0.01  0.05 . 0.1   1 %\\end{verbatim} %\\end{frame} \\end{document} "],["exercises-5.html", "10.4 Exercises", " 10.4 Exercises NFL II: Consider the data set nfl which includes the performance, salary, and facial symmetry of NFL quarterbacks. The data includes the name and the year and thus, it can be estimated as a panel data model. In a first step, convert the data into a panel data set using the function pdata.frame(). Next, estimate three models: (a) Regular pooled OLS model, (2) fixed-effects model, and (3) random effects model. The regression equation is the same for each model: \\[\\ln(total) =\\beta_0 + \\beta_1 \\cdot yards+ \\beta_2 \\cdot att+ \\beta_3 \\cdot exp+ \\beta_4 \\cdot exp^2 + \\beta_5 \\cdot draft1+beta_6 \\cdot draft2\\\\ + \\beta_7 \\cdot veteran +\\beta_8 \\cdot changeteam+ \\beta_9 \\cdot pbowlever+ \\beta_{10} \\cdot symm\\] Report and interpret the output for all three models. What happens to the variable \\(symm\\) in the fixed effects model and why? How does the panel data model compare to the original model which does not incorportate the panel strucutre? Renewable Energy: This question is based on the paper The effect of the feed-in-system policy on renewable energy investments: Evidence from the EU countries by Alolo et al. (2020). + Guns and Crime: There is a fierce debate on the relationship between gun ownership and crime. On the one hand, there is the argument that more guns prevent crime due to a deterrence effect. On the other hand, more guns trickling into society increase the likelihood of crime being committed. This data set is one example on how the issue at hand can be analyzed. The data set which is used for this question is part of the AER and can be imported with the command data(\"Guns\",package=\"AER\"). Please read the description of the data set which is available as part of the package. The first regression model to be estimated is written as: \\[\\ln(violent)=\\beta_0+\\beta_1 \\cdot law+\\epsilon\\] Estimate a fixed effects panel model using the equation above and report the R output. Interpret the results. The second model include year and state fixed effects and is written as follows: \\[\\ln(violent)=\\beta_0+\\beta_1 \\cdot law + \\alpha_i + \\lambda_t+\\epsilon\\] In a third and last model, include the variables \\(prisoners\\), \\(density\\), \\(income\\), \\(population\\) \\(afam\\), \\(cauc\\), and \\(male\\). What does the model suggest about the opposing views mentioned at the beginning of the question? WDI: Previous questions used the wdi data set without incorporating the potential panel structure of the information. Create a panel data frame using the command pdata.frame(). Estimate the following panel model: \\[\\ln(mortrate)=\\beta_0+ \\beta_1 \\cdot gdp+ \\beta_2 \\cdot litrate +\\epsilon\\] First, estimate the equation as a regular OLS model for the years 1980 and 2015 (separately for the two years). Rent in College Towns: This "],["time-series.html", "11 Time Series", " 11 Time Series This chapter introduces time as a component in regression models. Times series represent a temporal ordering of the data. In the cross-section models seen so far, the observations could have been in any order. In this chapter, the ordering of the observations matters. It is usually assumed that there is a stochastic process generating the series and the important aspect is that only a single realization of the stochastic process is observed. The following topics are covered: Static regression model Trend and seasonality Finite distributed lag models (including past or lagged independent variables) Autoregressive model (including past or lagged dependent variables) also known as dynamic models: Forecasting "],["static-regression-model.html", "11.1 Static Regression Model", " 11.1 Static Regression Model A static regression model is used if the data represents various time periods but the independent variables \\(x_{i,t}\\) have an immediate effect on the dependent variable \\(y_t\\). For example, if you consider the per-capita chicken consumption as a function of the chicken price and real disposable income in the data meatdemand, then the following model can be estimated. summary(lm(qchicken~rdi+pchicken,data=meatdemand)) ## ## Call: ## lm(formula = qchicken ~ rdi + pchicken, data = meatdemand) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.6335 -2.4765 0.2693 2.2275 6.3905 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 47.8885914 17.7955903 2.691 0.0107 * ## rdi 0.0014297 0.0002049 6.977 3.52e-08 *** ## pchicken -0.1143327 0.0454294 -2.517 0.0164 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.204 on 36 degrees of freedom ## Multiple R-squared: 0.9545, Adjusted R-squared: 0.952 ## F-statistic: 377.8 on 2 and 36 DF, p-value: &lt; 2.2e-16 This assumes that there is an immediate effect of income and chicken price on consumption. "],["trend-and-seasonality.html", "11.2 Trend and Seasonality", " 11.2 Trend and Seasonality In general, trends in the data can be linear: \\[y_t = \\beta_0 + \\beta_1 \\cdot t + \\epsilon_t\\] or exponential: \\[\\ln(y_t) = \\beta_0 + \\beta_1 \\cdot t + \\epsilon_t\\] Note that \\(\\beta_1\\) in the exponential time trend model is the average annual growth rate (assuming \\(t\\) is in years). Often, data can be decomposed into three components: Trend Season Random component The seasonal component can be included via dummy variables. For example, for quarterly data the following model can be used: \\[y_t=\\beta_0+\\delta_1 \\cdot Q1_t+\\delta_2 \\cdot Q2_t+\\delta_3 \\cdot Q3_t+\\beta_1 \\cdot x_{1,t}+ \\cdots+\\beta_k \\cdot x_{k,t}+\\epsilon_t\\] One seasonal dummy must be dropped. That is, quarterly and yearly data require three and eleven dummy variables, respectively. Consider the retail data. retail$date = as.Date(retail$date,format=&quot;%Y-%m-%d&quot;) retail$month = format(retail$date,&quot;%m&quot;) retail$t = c(1:nrow(retail)) bhat = lm(retail~factor(month)+t,data=retail) retail$fit = predict.lm(bhat) ggplot(retail)+ geom_line(mapping=aes(x=date,y=retail),color=&quot;red&quot;)+ geom_line(mapping=aes(x=date,y=fit)) 11.2.1 Practice Exercise Consider the data in ez. It contains unemployment claims (\\(uclms\\)) from Anderson (IN) before and after the establishments of an enterprise zone (EZ). The purpose of an EZ is to provide incentives for business to invest in area that are usually plagued by economic distress. Execute two regression models with \\(\\ln(uclms)\\) as the dependent variable and including a time trend and monthly dummy variables as the independent variables: (1) without \\(EZ\\) and (2) with \\(EZ\\). What can be concluded in terms of trend, seasonality, and the effectiveness of the EZ. Consider the data in traffic which contains information on accidents, traffic laws, and other variables for California. The dependent variable of interest is the natural log of \\(totacc\\). In a first regression, use the time trend and monthly dummy variables as the independent variables. In a second regression, add \\(wkends\\), \\(unem\\), \\(spdlaw\\), and \\(beltlaw\\). What can be concluded in terms of trend, seasonality, and the effectiveness of the laws concerning speed limits and belt usage. Why would weekends and unemployment be important? "],["finitie-distributed-lag-models.html", "11.3 Finitie Distributed Lag Models", " 11.3 Finitie Distributed Lag Models Distributed-lag models include past or lagged independent variables: \\[y_t=\\alpha+\\beta_0 \\cdot x_t+\\beta_1 \\cdot x_{t-1}+\\beta_2 \\cdot x_{t-2}+\\dots \\beta_k \\cdot x_{t-k}+\\epsilon\\] There are many reasons to include lagged independent variables such as psychological (e.g., it is difficult to break a habit or adjust to a new situation), economic (e.g., contractual obligations), or political (e.g., effectiveness of policy builds up over time) reasons. The relationship between income and consumption is used to introduce distributed lag models. Assume the following relationship between income and consumption: \\[C_t=\\alpha+\\beta_0 \\cdot I_t+\\beta_1 \\cdot I_{t-1}+\\beta_2 \\cdot I_{t-2}\\] Assume that \\(\\alpha_0=100\\), \\(\\beta_0=0.4\\), \\(\\beta_1=0.3\\), and \\(\\beta_2=0.2\\). For this example, the following questions are of interest: What is the long-run consumption with $4,000? How does the consumption change if the income increases to $5000? Note that the sum of the \\(\\beta_i\\)s is 0.9. The long-run multiplier (or long-run propensity) is written as: \\[\\sum_{i=1}^k \\beta_i = \\beta_0+\\beta_1+\\beta_2 + \\dots +\\beta_k = \\beta\\] The question about how may lagged independent variables to include is a difficult to answer question. If the assumption is made that all \\(\\beta_k\\) are of the same sign, then the so-called Koyck transformation can be applied: \\[\\beta_k = \\beta_0 \\cdot \\lambda^k \\quad \\text{for} \\quad k=0,1,2,\\dots\\] Characteristics of this assumption: \\(\\lambda &lt;1\\) gives less weight to distant values of \\(\\beta\\) Long-run multiplier is finite, i.e., \\[\\sum_{k=0}^\\infty \\beta_k = \\beta_0 \\cdot \\left( \\frac{1}{1-\\lambda} \\right)\\] Given the above assumptions, the Koyck transformation can be applied to the regression model. The original model is written as: \\[y_t = \\alpha+\\beta_0 \\cdot x_t+\\beta_0 \\cdot \\lambda x_{t-1}+\\beta_0 \\cdot \\lambda^2 \\cdot x_{t-2}+\\dots+\\epsilon_t\\] The reformulated equation to be estimated is \\[y_t = \\alpha \\cdot (1-\\lambda) + \\beta_0 \\cdot x_t + \\lambda \\cdot y_{t-1} + \\upsilon_t\\] The notation of the error term has changed from \\(\\epsilon_t\\) to \\(\\upsilon_t\\) in order to highlight that the terms will be different. koyck = usdata koyck$year = substr(koyck$date,1,4) koyck = aggregate(koyck[c(&quot;income&quot;,&quot;consumption&quot;)],FUN=sum,by=list(koyck$year)) colnames(koyck) = c(&quot;year&quot;,&quot;income&quot;,&quot;consumption&quot;) bhat = lm(formula = consumption ~ income + Lag(consumption), data = koyck) summary(bhat) ## ## Call: ## lm(formula = consumption ~ income + Lag(consumption), data = koyck) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10377.3 -630.8 410.4 953.3 2820.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 498.54586 599.54815 0.832 0.4085 ## income 0.14927 0.07810 1.911 0.0601 . ## Lag(consumption) 0.84107 0.08657 9.715 1.31e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1783 on 70 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.998, Adjusted R-squared: 0.9979 ## F-statistic: 1.732e+04 on 2 and 70 DF, p-value: &lt; 2.2e-16 "],["autoregressive-model.html", "11.4 Autoregressive Model", " 11.4 Autoregressive Model An autoregressive model includes lagged dependent variables. One of the simplest model is an autoregressive model of order 1, i.e., an AR(1) Model \\[y_t = \\alpha + \\beta \\cdot y_{t-1} + \\epsilon_t\\] where \\(\\epsilon_t \\sim N(0,\\sigma^2)\\). Consider the data of earthquakes over magnitude 7 in the data set quakes: In a first step, a scatter plot is constructed of \\(Y_{t-1}\\) and \\(y_t\\). The easiest way is to use the function acf: ## [1] 0.2486712 The correlation coefficicent of 0.25 indicates a weak positive correlation between the number of earthquakes in periods \\(t\\) and \\(t-1\\). Remember that correlation is not causation. The AR(1) model can be estimated with the lm() function used previously: summary(lm(quakes~Lag(quakes),data=quakes)) ## ## Call: ## lm(formula = quakes ~ Lag(quakes), data = quakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.8331 -2.5881 -0.8132 2.0394 10.1470 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.51834 1.26910 7.500 3.05e-11 *** ## Lag(quakes) 0.25498 0.09596 2.657 0.00922 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.997 on 97 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.06785, Adjusted R-squared: 0.05824 ## F-statistic: 7.06 on 1 and 97 DF, p-value: 0.009216 Note that although the slope coefficient associated with the lagged term is statistically significant, the R-squared value is very low. A second example considers quarterly beer production in Australia. In a first step, the data is converted into a time series: beer = ts(beer$sales,start=c(1992,1),end=c(2014,4),frequency=4) Next, the data is plotted using a function from the package ggfortify. Additional documentation using the package is found under Plotting ts objects. autoplot(beer) Note that the beer now appears in a different category in the Global Environment, i.e., not under Data anymore. The function tslm from the package forecast is used next. The function fits a linear model including seasonality and a trend component (and a trend-squared component if desired). bhat = tslm(beer~trend+I(trend^2)+season) summary(bhat) ## ## Call: ## tslm(formula = beer ~ trend + I(trend^2) + season) ## ## Residuals: ## Min 1Q Median 3Q Max ## -677.24 -163.84 2.12 195.34 529.37 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.082e+03 9.865e+01 41.381 &lt; 2e-16 *** ## trend 3.975e+01 4.298e+00 9.249 1.53e-14 *** ## I(trend^2) 4.200e-01 4.477e-02 9.381 8.26e-15 *** ## season2 8.677e+02 7.987e+01 10.864 &lt; 2e-16 *** ## season3 1.043e+03 7.990e+01 13.061 &lt; 2e-16 *** ## season4 2.031e+03 7.993e+01 25.408 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 270.8 on 86 degrees of freedom ## Multiple R-squared: 0.9867, Adjusted R-squared: 0.9859 ## F-statistic: 1278 on 5 and 86 DF, p-value: &lt; 2.2e-16 The package forecast can be also be used to forcast: plot(forecast(bhat,h=20)) The third examples uses data on Japanese car production (jcars) to illustrate the concept of autocorrelation. The focus is on car production after 1963. In a first step, the data is visualized using ggplot: ggplot(jcars,aes(x=year,y=cars))+geom_line()+theme_bw()+theme(axis.title.x=element_blank()) The sample autocorrelation function (ACF) is the correlation between \\(y_t\\) and \\(y_{t-1}\\), \\(y_{t-2}\\), \\(y_{t-3}\\), and so on. It can be written as follows: \\[\\rho_j = \\frac{Cov(y_t,y_{t-j})}{\\sqrt{Var(y_t)\\cdot Var(y_{t-j})}}\\] The ACF can be used to identify a possible structure of time series either of the actual time series or the residuals of the regression. The autocorrelation function (ACF) is plotted using the function acf() in R. acf(jcars$cars) "],["data-sources.html", "12 Data Sources", " 12 Data Sources This chapter describes the data sets associated with the lecture notes. Some data sets are generic, i.e., randomly generated, to highlight a particular concept while other are either based on public sources or are taken from academic papers. In any case, the sources are clearly indicated. Sometimes modifications were made to the data for ease of use, e.g., removing missing values or unnecessary variables. All the data is contained in the file DataAnalysisPAData.RData. Some of the data sets are also associated with specific R packages which is indicated. accidents (\\(N=30\\)): Generic data on the number of \\(accidents\\) and the \\(temperature\\) in Fahrenheit. airlines (\\(N=189496\\)): Data on airline on-time statistics and delay causes from the Bureau of Transportation Statistics from January 2004 to May 2020. The variable names are mostly self-explanatory. The variables \\(arr\\_flights\\) and \\(arr\\_del15\\) are the total number of arriving flights and the number of flight delayed at least 15 minutes. The original data is available here. anscombe (\\(N=11\\)): Anscombes Quartet where \\(y_i\\) and \\(x_i\\) represent dependent and independent variable of set \\(i\\), respectively. This data set is widely available and is also included in R. blm (\\(N=1359\\)): The data is associated with the paper Black Lives Matter: Evidence that Police-Caused Deaths Predict Protest Activity. The variables include city (\\(geography\\)), total protests (\\(totprotests\\)), population (\\(pop\\)), population density (\\(popdensity\\)), percent black (\\(percentblack\\)), black poverty rate (\\(blackpovertyrate\\)), percent college-educated (\\(percentbachelor\\)), college students as a percent of the population (\\(collegeenrollpc\\)), Democratic vote share (\\(demshare\\)), police-caused deaths per 10,000 (\\(deathspc\\)), and black police-caused deaths per 10,000 (\\(deathsblackpc\\)). bmw (\\(N=30\\)): Data on the prices and miles of a particular BMW 5-series model in the Indianapolis area. Some of the models have rear-wheel drive (\\(allwheeldrive=0\\)) whereas some have all-wheel drive (\\(allwheeldrive=1\\)). chicagogrocery: The data set was represents the information from 2013 grocery stores and [Community Data Snapshots Raw Data from March 2014](https://datahub.cmap.illinois.gov/dataset/community-data-snapshots-raw-data]. It includes the following variables: \\(cca\\): Name of the Chicago Community Area \\(stores\\): Number of grocery stores \\(sqft\\): Sum of store square footage \\(income\\): Median income \\(homevalue\\): Median home value \\(pop\\): Population \\(whitepop\\): White population \\(laborforce\\): Labor Force \\(unemployed\\): Unemployed people \\(black\\): Blac population \\(acres\\): Area of the CCA coffee (\\(N=29\\)): Historical data about the coffee consumption in the United States. The variable \\(consumption\\) is measured in thousand 60-kg bags. The variable \\(price\\) represents the retail price of roasted coffee in US Dollars per pound. Both time series are obtained from the International Coffee Association. The variables \\(rdi\\) and \\(cpi\\)} represent real disposable income and the consumer price index, respectively. Those data series are obtained from the St. Louis FRED date base. The same is true for the varible \\(population\\). The variables \\(rprice\\) and \\(pcconsumption\\) represent the real coffee prices and the per-capita consumption of coffee. compactcars (\\(N=78\\)): Fuel efficiency of compact cars in 1995 and 2015. The fuel efficiency is expressed in miles per gallon in the columns and , respectively. discharge (\\(N=7\\)): Generic data set of pollutant discharge into a river measured in gallons. eggweights (\\(N=37\\)): Weight in grams of Large Grade A Brown Eggs from Whole Foods. eucrime (\\(N=387\\)): Intentional homicide data in Europe. Data series from Eurostat. evdata (\\(N=579\\)): Data about the choice of consumers with respect to alternative fuel vehicles. The variable represents the choice by the consumer for gasoline vehicles (\\(choice = 1\\)), conventional hybrids (\\(choice = 2\\)), plug-in hybrids (\\(choice = 3\\)), and electric vehicles (\\(choice = 4\\)). For each consumer, you have the following variables: , (indicating whether people have a fast charger for electric cars in their community), , and (number of cars). The variable is coded as follows: Under $15,000 (1), $15,000 to $24,999 (2), $25,000 to $34, 999 (3), $35,000 to $49,999 (4), $50,000 to $74,999 (5), $75,000 to $99,999 (6), $100,000 to $149,999 (7), $150,000 to $199,999 (8), $200,000 to $249,000 (9), above $250,000 (10). The variable is coded as follows: Less than High School (1), High School / GED (2), Some College (3), 2-year College Degree (4), 4-year College Degree (5), Masters Degree (6), Doctoral Degree (7). The variable politics is coded as follows: Extremely Liberal (1), Liberal (2), Slightly liberal (3), Moderate (4), Slightly conservative (5), Conservative (6), Extremely conservative (7), Other (8), None (9). See for more details. fertil1 (\\(N=1129\\)): Data from the General Social Survey. The data set is accompanying the book Introductory Econometrics: A Modern Approach by Jeffrey Wooldridge. The variables are as follows: year: 72 to 84, even educ: years of schooling meduc and feduc}: mothers and fathers education kids: number children ever born east, , and : 1 if lived in at 16 farm: 1 if on farm at 16 otherrural: 1 if other rural at 16 town: 1 if lived in town at 16 smallcity: 1 if in small city at 16 fpdata (\\(N=176\\)): gss2018 (\\(N=772\\)): 2018 data from the General Social Survey with the followin variables: gssgun (\\(N=1300\\)): gsssocialmedia (\\(N=1366\\)): happy (\\(N=631\\)): Data from the 2018 General Social Survey about happiness which includes the following variables: \\(sexfreq\\): Frequency of sex during last year \\(gun\\): Have gun in home \\(sclass\\): Subjective class identification \\(health1\\): Condition of health \\(happiness\\): General happiness \\(party\\): Political party affiliation. You may want to combine the Ind, near democrat'' andNot str democrat into the same category, e.g., ``lean democrat. Do the same for republicans. \\(education\\): Highest year of school completed \\(age\\): Age of respondent hdr (\\(N=189\\)): Human Development Indicator from 2019. heating (\\(N=24\\)): Contains 24 observations of natural gas usage (in hundreds of cubic feet) and the average outside temperature (in Fahrenheit). hhpub (\\(N=129696\\)): honda (\\(N=81\\)): hybrid (\\(N=1000\\)): indyhomes (\\(N=102\\)): meatdemand (\\(N=39\\)): Meet demand quantity (\\(q\\)) and real price (\\(p\\)) data from the U.S. Department of Agriculture. Prices and real disposable income (\\(rdi\\)) are in real terms. mh1 (\\(N=101\\)): Home values in the Meridian Hills area in Indianapolis. mh2 (\\(N=18\\)): Prices and characteristics of homes in the Meridian Hills area in Indianapolis. milk (\\(N=50\\)): Randomly generated milk container fillings in ounces. mpa (\\(N=18\\)): Randomly generated exam scores in a class of MPA students. nfl (\\(N=1009\\)): Data from which includes the following variables: ohioincome (\\(N=607\\)): Enrollment and median income in Ohio School districts for 2018/2019. identifies the school district. ohioscore (\\(N=608\\)): Performance and achievement scores of Ohio schools for 2018/2019. The data is obtained from the . identifies the school district. organic (\\(N=100\\)): Randomly generated data on the purchase behavior of organic food (binary choice) as a function of income. retail (\\(N=348\\)): obtained from the St. Louis FRED. rossi (\\(N=432\\)): week: week of first arrest after release, or censoring time. arrest: the event indicator, equal to 1 for those arrested during the period of the study and 0 for those who were not arrested. fin: a factor, with levels yes if the individual received financial aid after release from prison, and no if he did not; financial aid was a randomly assigned factor manipulated by the researchers. age: in years at the time of release. race: a factor with levels black and other. wexp: a factor with levels yes if the individual had full-time work experience prior to incarceration and no if he did not. mar: a factor with levels married if the individual was married at the time of release and not married if he was not. paro: a factor coded yes if the individual was released on parole and no if he was not. prio: number of prior convictions. educ: education, a categorical variable coded numerically, with codes 2 (grade 6 or less), 3 (grades 6 through 9), 4 (grades 10 and 11), 5 (grade 12), or 6 (some post-secondary). emp1  emp52: factors coded yes if the individual was employed in the corresponding week of the study and no otherwise. skewness (\\(N=500\\)): Randomly generated data from a beta distribution: soda (\\(N=25\\)): Randomly generated data of for soda cans (in milliliters). states (\\(N=10\\)): Generated income data for three states. usdata (\\(N=296\\)): Data from the St. Louis FRED data base. vehicles (\\(N=42702\\)): The data set contains data about the fuel efficiency of vehicles from model year 1984 to 2021 (\\(year\\)). It also includes \\(make\\) and \\(model\\) for the various manufacturers. The variable \\(displ\\) refers to engine displacement in liters, \\(VClass\\) is the vehicle class, \\(ghgScore\\) indicates the greenhouse gas (GHG) emissions, and \\(comb08U\\) is unrounded combined MPG. The data is obtained from the U.S. Environmental Protection Agency (EPA) webpage. vehpub (\\(N=256115\\)): The data contains vehicle data from the 2017 National Household Travel Survey (NHTS). Some columns were deleted but the detailed code book can be found here. The variables included are: \\(HOUSEID\\): Household Identifier \\(VEHID\\): Vehicle Identifier \\(VEHYEAR\\): Vehicle Year \\(MAKE\\): Vehicle Make \\(FUELTYPE\\): Fuel Type \\(VEHTYPE\\): Vehicle Type \\(OD\\_READ\\): Odometer Reading \\(HFUEL\\): Type of Hybrid Vehicle \\(HYBRID\\): Hybrid vehicle \\(HOMEOWN\\): Home Ownership \\(HHFAMINC\\): Household income \\(HHSTATE\\): Household state \\(URBRUR\\): Household in urban/rural area waterpressure (\\(N=30\\)): Randomly generated data on the water pressure in a citys water lines. wdi (\\(N=13237\\)): World Development Indicators (WDI) from the World Bank. The following variables are included in the data set. \\(iso2c\\): Country code \\(country\\): Country name \\(year\\): Year \\(gdp\\): GDP per capita (constant 2010 US$) \\(lifeexp\\): Life expectancy at birth, total (years) \\(litrate\\): Literacy rate, adult total (% of people ages 15 and above) \\(fertrate\\): Fertility rate, total (births per woman) \\(mortrate\\): Mortality rate, under 5 (per 1,000 live births) \\(region\\): Geographic region \\(incomeLevel\\): Income groupings are based on Gross National Income per-capita "]]
