[["index.html", "Data Analysis for Public Affairs 1 Introduction", " Data Analysis for Public Affairs Jerome Dumortier 2021-03-07 1 Introduction This book serves as an introduction to data analysis with R and RStudio. It can be subdivided into four main parts. Probability Statistics Regression Advanced Regression "],["topics-overview.html", "2 Topics Overview", " 2 Topics Overview This is my first tex code: \\(CO_2\\) emissions. height = c(71,77,70,73,66,69,73,73,75,76) sum(height) ## [1] 723 "],["introduction-to-r.html", "3 Introduction to R 3.1 Online Resources and Help 3.2 Opening RStudio 3.3 Functions 3.4 Data Management", " 3 Introduction to R Topics covered in this lecture Introduction to R and RStudio Data Management Plotting and Graphs with R Basic Statistics In-class exercises will be conducted throughout the Lecture. 3.1 Online Resources and Help Very large user community for R. Google search for ``Some topic R usually leads quickly to the desired help. Here are the links to a few online tutorials UCLA Institute for Digital Research and Education StatMethods Statistical tools for high-throughput data analysis Two online resources will provide you the solution to the vast majority of your R questions. Getting on those websites is usually the result of a Google search. Statistical Data Analysis R: This resource contains the function manual for R/RStudio including all packages. Example for a function boxplot. The most helpful part are the examples at the bottom of the page. Stack Overflow: Resources for developers. For example, a Google search for r ggplot two y axis may give you the following result Note that all questions on Stack Overflow have to be accompanied by a re-creatable dataset. Besides many online resources, there are also two useful textbooks Applied Econometrics with R by Christian Kleiber and Achim Zeileis. Introductory Statistics with R by Peter Dalgaard. An additional online tutorial is Using R for Introductory Statistics by John Verzani. 3.2 Opening RStudio Work in RStudio is done in four windows: Script Window This is were you type your R Script (.R) and where you execute commands. Comparable to do-file/editor in Stata. This window needs to be opened by File \\(\\Rightarrow\\) New File \\(\\Rightarrow\\) R Script. Console window Use of R interactively. Should only be used for quick calculations and not part of an analysis. Environment Lists all the variables, data frames, and user-created functions. It is tempting to use the ``Import Dataset function Dont. Plots/Packages/Help There is a base version of R that allows doing many calculations but the power of R comes through its packages. To use functions associated with a particular package, click ``Install in the packages window of RStudio and type in the name of the package. Or alternatively, use install.packages(\"ggplot2\") To use a package, you have to activate it by either checking the box in the window Packages or by including library(packagename). Those packages are updated on a regular basis by users. The \\# allows you to include comments in your script file that are not read by R. It is good practice to start any new script with clearing the memory using the command rm(list=ls()). Use the command get() to determine the current working directory or set the new working directory with the command setwd(), e.g., setwd(\"E:/\"). For file paths, replace \\(\\backslash\\) with \\(/\\). Next, you want to load all libraries necessary for your entire script file with the command library(). It is also good practice to save your R-script on a regular basis. The frontmatter, i.e., the top of a R-script file, could look as follows rm(list=ls()) load(&quot;DataAnalysisPAData.RData&quot;) library(openxlsx) 3.2.1 In-class Exercise 1 Create a R-script file with the following components: Two lines for the title and the date (use #) Clearing all current contents Setting the correct working directory This should be a folder to which you have downloaded all materials. Installing and loading the package openxlsx. 3.3 Functions At the core of R are functions that do things based on your input. The basic structure is object = functionname(argument1=value,argument2=value,...) The structure has the following components object: Output of the function will be assigned to object. functionname: Name of the system function. You can also create and use your own functions. More about this later. argument: Arguments are function specific. value: The value you want a particular argument to take. If a function is executed without an specific assignment, the output will be displayed in the console window. Before using a function, read the documentation. Many functions have default settings. Be aware of default values. In most cases, those defaults are set to values that satisfy most uses. For example, consider the help file for the function t.test. -t.test(x,y=NULL,...,mu=0,conf.level=0.95,...) For this function we have the following default values y=NULL mu=0 conf.level=0.95 3.4 Data Management 3.4.1 Data in R The main data types which can appear in the Environment window of R are: Vectors preselection = seq(1788,2016,4) midterm = seq(by=4,to=2018,from=1790) Matrix somematrix = matrix(8,10,4) Only numerical values are allowed. Data frames By far, the most common data type in R. Comparable to an Excel sheet. More on this later. Lists Collection of objects from of various types. myfirstlist = list(preselection,midterm,somematrix) 3.4.2 Using R as a Calculator Entering heights of people and storing it in a vector named : height = c(71,77,70,73,66,69,73,73,75,76) Calculating the sum, product, natural log, mean, and (element-wise) squaring is done with the following commands: sum(height) prod(height) log(height) # Default is the natural log meanheight = mean(height) heightsq = height^2 Removing (i.e., deleting) unused elements: rm(heightsq,meanheight) 3.4.3 Creating a Data Frame from Scratch Data frames are the most commonly used tables in R/RStudio. They are similar to an Excel sheet. Column names represent the variables and rows represent observations. Column names must be unique and without spaces. Suggestion: Use only lower-case variable names and objects. studentid = 1:10 studentnames = c(&quot;Andrew&quot;,&quot;Linda&quot;,&quot;William&quot;,&quot;Daniel&quot;,&quot;Gina&quot;, &quot;Mick&quot;,&quot;Sonny&quot;,&quot;Wilbur&quot;,&quot;Elisabeth&quot;,&quot;James&quot;) students = data.frame(studentid,studentnames,height) rm(studentid,height,studentnames) 3.4.4 In-class Exercise 2 Create a data frame called students containing the following information: Name Economics English Mindy 80.0 52.5 Gregory 60.0 60.0 Shubra 95.0 77.5 Keith 77.5 30.0 Louisa 97.5 95.0 Notes: Use as the column header for the students names. Once you have created the data frame, remove the unused vectors. "],["basic-multivariate-regression.html", "4 Basic Multivariate Regression 4.1 Dummy Variables", " 4 Basic Multivariate Regression Extension of the bivariate model to multivariate regression One dependent variable but multiple independent variables Topics associated with multivariate regression models covered in this lecture: Multicollinearity Dummy Variables Natural logarithm Functional forms Interaction Terms Bivariate regression model (one independent and one dependent variable) \\[\\begin{equation*} y = \\beta_0 + \\beta_1 x_1 + \\epsilon \\end{equation*}\\] Multivariate linear regression model (multiple independent variables) \\[\\begin{equation*} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + \\epsilon \\end{equation*}\\] Whether we consider the univariate or multivariate regression model, the objective is always to minimize the sum of squared errors, hence the name ordinary least square (OLS) model. The equation of a line can be determined using slope and intercept, we can write: \\[\\begin{equation*} E(y|x) = \\beta_0 + \\beta_1 x \\end{equation*}\\] A model with two independent variables (predictors) describes a plane. 4.1 Dummy Variables Dummy variables represent a single qualitative characteristic such as religion, gender, or nationality. Dummy variables are independent variables coded as either 0 or 1. For example, consider Example: Price (\\(Y_i\\)) of a car depending on miles (\\(X_i\\)) and AWD (\\(D_i\\)) \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 D_i + \\epsilon_i \\end{equation*}\\] with \\(D_i = 1\\) if AWD and \\(D_i = 0\\) if RWD. \\end{itemize} This regression can theoretically be separated into two single equations: Interpretation: "],["binary-choice.html", "5 Binary Choice 5.1 Stuff 5.2 Including Plots", " 5 Binary Choice Binary choice models (\\(y\\) takes two values: 0 or 1) Did you vote during the last election? Does an individual recidivate after being released from prison? Participation in the labor market Purchasing a home Model: \\(Pr(y=1|x)\\) Categorical dependent variable but naturally ordered What is your level of happiness? E.g., very happy, happy, ok, sad. Categorical dependent variable but no ordering How do you commute to campus? Did you vote for the democratic, republican, or independent candidate during the last election? bhat_logit = glm(buying~income,family=binomial(link=&quot;logit&quot;),data=organic) summary(bhat_logit) ## ## Call: ## glm(formula = buying ~ income, family = binomial(link = &quot;logit&quot;), ## data = organic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8451 -0.5293 -0.1423 0.4093 1.9154 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.87557 1.13842 -5.161 2.45e-07 *** ## income 0.11709 0.02247 5.211 1.87e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 138.469 on 99 degrees of freedom ## Residual deviance: 70.931 on 98 degrees of freedom ## AIC: 74.931 ## ## Number of Fisher Scoring iterations: 6 This is a test. 5.1 Stuff This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 5.2 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "],["violating-assumptions.html", "6 Violating Assumptions 6.1 Heteroscedasticity 6.2 Multicollinearity 6.3 Autocorrelation 6.4 Other Issues and Problems with Data 6.5 Exercises", " 6 Violating Assumptions This chapter introduces the detection and correction of problems with the estimation procedure due to the violation of the key assumptions necessary for the OLS model to work. The following R packages are needed for this chapter: car, lmtest, orcutt, and sandwich. 6.1 Heteroscedasticity A key assumption of the OLS model is homoscedasticity error terms. That is, the error variance is constant: \\[Var(\\epsilon_i) = \\sigma^2\\] With heteroscedasticity, the variance of the error term is not constant: \\[Var(\\epsilon_i) = \\sigma_i^2\\] For a bivariate regression model with heteroscedastic data, it can be shown that \\[Var(\\hat{\\beta_1}) = \\frac{\\sum x_i^2 \\sigma_i^2}{(\\sum x_i^2)^2}\\] This is different from the variance of the coefficient estimate under homoscedasticity: \\[Var(\\hat{\\beta_1}) = \\frac{\\sigma^2}{\\sum x_i^2}\\] Unbiasedness of the OLS estimator is not affected but the variance of \\(\\beta_1\\) will be larger compared to other estimators. Note that the measure of \\(R^2\\) is unaffected by heteroscedasticity. Homoscedasticity is needed to justify the t-test, F-test, and confidence intervals. The F-statistic does no longer have an F-distribution. In short, hypothesis tests on the \\(\\beta\\)-coefficients are no longer valid. If \\(\\sigma_i^2\\) was known, the use of a Generalized Least Squares (GLS) model would be appropriate: \\[y_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i\\] Dividing both sides by the known variance: \\[\\frac{y_i}{\\sigma_i}=\\beta_0 \\cdot \\frac{1}{\\sigma_i}+\\beta_1 \\frac{x_i}{\\sigma_i}+\\frac{\\epsilon_i}{\\sigma_i}\\] If \\(\\epsilon^*_i = \\epsilon_i / \\sigma_i\\), then it can be shown that \\(Var(\\epsilon^*_i)=1\\), i.e., constant. Under the usual OLS model: \\[\\sum_{i=1}^N e_i^2=\\sum_{i=1}^N \\left(y_i-\\hat{\\beta}_0+\\hat{\\beta}_1 \\cdot x_i \\right)^2\\] Under GLS model: \\[\\sum_{i=1}^N w_i e_i^2= \\sum_{i=1}^N w_i \\left(y_i-\\hat{\\beta}_0+\\hat{\\beta}_1 \\cdot x_i \\right)^2\\] That is, GLS minimizes the weighted sum of the residual squares. Since in reality, the variance of \\(\\sigma^2\\) is not known, other techniques have to be employed to obtain so-called heteroscedasticity-consistent (HC) standard errors. But first, two tests are introduced to detect heteroscedasticity. 6.1.1 Detecting Heteroscedasticity Two test are presented to detect heteroscedasticity: Goldfeld-Quandt Test (1965) Breusch-Pagan-Godfrey Test (1979) The steps necessary for the Goldfeld-Quandt Test are as follows: Sort observations by ascending order of the dependent variable. Pick C as the number of central observations to drop in the middle of the dependent variable. Run two separate regression equations, i.e., with the lower and upper part. Compute \\[\\lambda = \\frac{RSS_2/df}{RSS_1/df}\\] \\(\\lambda\\) follows an F-distribution. The Goldfeld-Quandt Test can be illustrated with gqtestdata. In a first step, the data is separated into two groups with \\(C=6\\). In a second step, both groups are used to run a regression. And lastly, \\(\\lambda\\) is calculated. gqtestdata1 = gqtestdata[1:22,] gqtestdata2 = gqtestdata[29:50,] bhat1 = lm(price_r~sqft_r,data=gqtestdata1) bhat2 = lm(price_r~sqft_r,data=gqtestdata2) lambda = sum(bhat2$residuals^2)/sum(bhat1$residuals^2) Of course, there is also a function in R called gqtest which simplifies the procedure. library(lmtest) bhat = lm(price~sqft,data=gqtestdata) gqtest(bhat,fraction = 6) ## ## Goldfeld-Quandt test ## ## data: bhat ## GQ = 4.8709, df1 = 20, df2 = 20, p-value = 0.0004223 ## alternative hypothesis: variance increases from segment 1 to 2 In any case, the hypothesis of homoscedasticity is rejected for gqtestdata. The Breusch-Pagan-Godfrey Test is an alternative and does not rely on choosing C as the number of central observations to be dropped. The steps include the following: Run a regular OLS model and obtain the residuals. Calculate \\[\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^N e^2_i}{N}\\] Construct the variable \\(p_i\\) as follows: \\(p_i = e^2_i / \\hat{\\sigma}^2\\) Regress \\(p_i\\) on the Xs as follows \\[p_i = \\alpha_0 + \\alpha_1 \\cdot x_{i1}+\\alpha_2 \\cdot x_{i2} + \\dots\\] Obtain the explained sum of squares (ESS) and define \\(\\Theta = 0.5 \\cdot ESS\\). Then \\(\\Theta \\sim \\chi^2_{m-1}\\). The much simpler procedure is to use the function bptest() in R. library(lmtest) bhat = lm(price~sqft,data=gqtestdata) bptest(bhat) ## ## studentized Breusch-Pagan test ## ## data: bhat ## BP = 8.2672, df = 1, p-value = 0.004037 6.1.2 Correcting Heteroscedasticity To correct for heteroscedasticity, robust standard errors must be obtained. bhat = lm(price~sqft,data=gqtestdata) summary(bhat) vcov = vcovHC(bhat) coeftest(bhat,vcov.=vcov) Note that there are multiple variations to calculate the standard error and thus, it is possible for slight variations among the results from different packages. \\[Var(\\hat{\\beta}_1) = \\frac{\\sum_{i=1}^N (x_i-\\bar{x})^2 e_i^2}{\\sum_{i=1}^N (x_i-\\bar{x})^2}\\] The square root of the following equation is called heteroscedastic robust standard error: \\[\\widehat{Var}(\\hat{\\beta}_j) = \\frac{\\sum_{i=1}^N \\hat{r}^2_{ij} e_i^2}{\\sum_{i=1}^N (x_i-\\bar{x})^2}\\] Standard errors can be either larger or smaller. Note that in this example, we do not know whether heteroscedasticity is present or not. 6.2 Multicollinearity Multicollinearity describes the situation in which two or more independent variables are linearly related. Under perfect multicollinearity: \\[\\lambda_1 x_1 + \\lambda_2 x_2 + \\dots +\\lambda_k x_k = 0\\] where \\(\\lambda_i\\) are constants that are not all zero simultaneously. For example, consider \\(x_1=\\{8,12,15,17\\}\\), \\(x_2=\\{24,36,45,51\\}\\), and \\(x_3=\\{2,3,3.75,4.25\\}\\). In this case, \\(\\lambda_1=1\\), \\(\\lambda_2=-1/5\\), and \\(\\lambda_3=2\\). Note, multicollinearity refers to linear relationships! Including a squared or cubed term is not an issue of multicollinearity. It can be shown that the variance of the estimator increases in the presence of multicollinearity. There are various indications that the data suffers from multicollinearity: High \\(R^2\\) but few significant variables Fail to reject the hypothesis for H\\(_0\\): \\(\\beta_i=0\\) based on t-values but rejection all slopes being simultaneously zero based on F-test. High correlation among explanatory variables Variation of statistically significant variables between models. 6.2.1 Variance Inflated Factors (VIF) Identifies possible correlation among multiple independent variables and not just two as in the case of a simple correlation coefficient. Consider the model: \\[y_i = \\beta_0 + \\beta_k x_{ik} + \\epsilon_i\\] The estimated variances of the coefficient \\(\\beta_k\\) is written as \\[Var(\\beta_k)^* = \\frac{\\sigma^2}{\\sum_{i=1}^N (x_{ik}-\\bar{x}_k)^2}\\] Without any multicollinearity, this variance is minimized. If some some independent variables are correlated with the independent variable \\(k\\), then \\[Var(\\beta_k) = \\frac{\\sigma^2}{\\sum_{i=1}^N (x_{ik}-\\bar{x}_k)^2} \\cdot \\frac{1}{1-R^2_k}\\] where \\(R^2_k\\) is the \\(R^2\\) if variable \\(x_k\\) is taken as the dependent variable. The VIF can be written as \\[\\frac{Var(\\beta_k)}{Var(\\beta_k)^*}=\\frac{1}{1-R^2_k}\\] If \\(VIF=1\\), then there is no relationship between the variable \\(x_k\\) and the remaining independent variables. Otherwise, \\(VIF&gt;1\\). In general, the interpretation is as follows: VIF of 4 warrants attention VIF of 10 indicates a serious problem. 6.2.2 Examples To illustrate the concept of multicollinearity, the data set from nfl is used (Berri et al. (2011)). The first model includes total salary as the dependent variable and the following independent variables: prior season passing yards, pass attempts, experience (squared) in the league, draft round pick, veteran (more than 3 years in the league), pro bowl appearance, and facial symmetry. bhat = lm(log(total)~yards+att+exp+exp2+draft1+draft2+veteran+ changeteam+pbowlever+symm,data=nfl) summary(bhat) After estimating the results, the function vif() from the package car is used: vif(bhat) ## yards att exp exp2 draft1 draft2 veteran changeteam pbowlever ## 32.547700 30.920282 39.889877 26.715342 1.621048 1.228091 5.253525 1.194254 1.581753 ## symm ## 1.056661 The results indicate multicollinearity for yards, att, and experience. Passings yards and attempts may be correlated and thus, one of them (att) is dropped. bhat = lm(log(total)~yards+exp+exp2+draft1+draft2+veteran+ changeteam+pbowlever+symm,data=nfl) summary(bhat) This improves the estimation but experience (and its squared term) are still problematic: vif(bhat) ## yards exp exp2 draft1 draft2 veteran changeteam pbowlever symm ## 1.460849 39.339639 26.162804 1.616171 1.227479 5.253502 1.141435 1.569621 1.052906 The last estimation removes experience and the VIF terms are now in the acceptable range. bhat = lm(log(total)~yards+draft1+draft2+veteran+ changeteam+pbowlever+symm,data=nfl) summary(bhat) vif(bhat) ## yards draft1 draft2 veteran changeteam pbowlever symm ## 1.406241 1.653634 1.229459 1.976506 1.101988 1.406095 1.010855 The important part is that the conclsion of the paper has not changed with regard to facial symmetry. 6.3 Autocorrelation The correlation of error terms is called autocorrelation. The issue usually arises if there is a time component in the data. Recall the main types of data available for research: Cross-sectional data (multiple observations at same time point) Time series data (one variable observed over time) Pooled data (multiple observations at different time points) Panel data (same observations at different time points) There is a distinction between serial correlation and autocorrelation Serial correlation: Correlation between two series Autocorrelation: Correlation with lagged variables The OLS estimator is still unbiased but there is no longer minimum variance since \\(E(\\epsilon_i \\epsilon_j) \\neq 0\\). Unlikely autocorrelation for cross-sectional data except in the case of spatial auto-correlation. One cause of autocorrelation could be inertia in economic variables. For example, variables such as income, production, or employment increase after a recession. But there are a number of other reasons for autocorrelation. Autocorrelation could be due to specification bias due to excluded variables or incorrect functional forms. In the case of excluded variables, assume that the correct equation is \\[q_{beef}=\\beta_0+\\beta_1 \\cdot p_{beef}+\\beta_2 \\cdot p_{income} + \\beta_3 \\cdot p_{pork}+\\epsilon_t\\] The estimated equation is: \\[q_{beef}=\\beta_0+\\beta_1 \\cdot p_{beef}+\\beta_2 \\cdot p_{income}+\\upsilon_t\\] This results in a systematic patters of \\(\\upsilon_t\\): \\[\\upsilon_t= \\beta_3 \\cdot p_{pork}+\\epsilon_t\\] Incorrect functional form Cobweb phenomenon \\[\\begin{equation*} supply_t = \\beta_0 + \\beta_1 \\cdot p_{t-1} \\end{equation*}\\] Lags \\[\\begin{equation*} consumption_t = \\beta_0 + \\beta_1 \\cdot income_t+\\beta_3 \\cdot consumption_{t-1}+\\epsilon_t \\end{equation*}\\] First-order Autoregressive Scheme Consider the model: \\[y_t=\\beta_0+\\beta_1 x_t + \\upsilon_t\\] Assume the following form of \\(\\upsilon\\): \\[\\upsilon_t = \\rho \\upsilon_{t-1} + \\epsilon_t\\] This is called a first-order autoregressive AR(1) scheme. An AR(2) would be written as \\begin{equation*} \\upsilon_t = \\rho_1 \\upsilon_{t-1} + \\rho_2 \\upsilon_{t-2} + \\epsilon_t \\end{equation*} This can be illustrated with simulated data. Consider the following model: \\[y_t=1+0.8 \\cdot x_t + \\upsilon_t\\] Assume the following form of \\(\\upsilon\\): \\[\\upsilon_t = 0.7 \\cdot \\upsilon_{t-1} + \\epsilon_t\\] Procedure Simulate the above model 100 times Compare variance of coefficients under different two different methods: (1) OLS and (2) Cochrane-Orcutt 6.3.1 Durbin Watson d-Test The test statistic of the Durbin-Watson test is written as: \\[d=\\frac{\\sum_{t=2}^N (e_t-e_{t-1})^2}{\\sum_{t=1}^N e_t^2}\\] Assumptions underlying the test are No intercept AR(1) process, i.e., \\(\\upsilon_t = \\rho \\upsilon_{t-1} + \\epsilon_t\\) No lagged independent variables Original papers derive lower (\\(d_L\\)) and upper (\\(d_U\\)) bounds, i.e., critical values, that depend on \\(N\\) and \\(k\\) only. \\(d \\approx 2 \\cdot (1-\\rho)\\) and since \\(-1 \\leq \\rho \\leq 1\\), we have \\(0 \\leq d \\leq 4\\). Rule of thumb indicates that \\(d=2\\) signals no problems. 6.3.2 Breusch-Godfrey Test Consider the following model \\(y_t = \\beta_0 + \\beta_1 x_t + \\upsilon_t\\) with the following error term structure: \\[\\upsilon_t = \\rho_1 \\upsilon_{t-1} + \\rho_2 \\upsilon_{t-2} + \\dots + \\rho_p \\upsilon_{t-p} + \\epsilon_t\\] The null hypothesis for the test is expressed as follows: \\(H_0\\): \\(\\rho_1 = \\rho_2 = \\dots = \\rho_p =0\\) When the following regression is executed: \\[\\hat{\\upsilon}_t = \\alpha_0 + \\alpha_1 x_t + \\hat{\\rho}_1 \\hat{\\upsilon}_{t-1} + \\hat{\\rho}_2 \\hat{\\upsilon}_{t-2} + \\dots + \\hat{\\rho}_p \\hat{\\upsilon}_{t-p} + \\epsilon_t\\] Then \\[(n-p)R^2 \\sim \\chi^2_p\\] 6.4 Other Issues and Problems with Data More serious problems than heteroscedasticity: Functional form mis-specification Measurement error Missing data: Estimating a standard regression model is not possible with missing values. All statistical software packages ignore missing data. Missing data is a minor problem if it is due to random error. Missing data can be problematic if it is systematically missing, e.g., missing education data for people with lower education Non-random samples Outliers 6.5 Exercises WDI and Heteroscedasticity: Using the data in wdi, estimate the following equation for the year 2018 and report the results: \\[\\begin{equation*} fertrate = \\beta_0 + \\beta_1 \\cdot gdp+ \\beta_2 \\cdot litrate \\end{equation*}\\] The variable represents fertility rate (birth per woman), represents the GDP per capita in in real terms, and is the literacy rate. Indy Homes: The data indyhomes contains home values of two ZIP codes in Indianapolis. The model estimates the home value (dependent variable) based on a set of independent variables. The variables levels and garage refers to the number of stories and garage spots, respectively. The remaining variables are self-explanatory. Create a dummy variable called northwest for the 46268 ZIP code. Report and interpret the results of the following regression equation: \\[\\ln(price)=\\beta_0+\\beta_1 \\cdot \\ln(sqft)+\\beta_2 \\cdot northwest+\\beta_3 \\cdot \\ln(lot)+\\beta_4 \\cdot bed+\\beta_5 \\cdot garage+\\beta_6 \\cdot levels+\\beta_7 \\cdot northwest \\cdot levels\\] What is the expected home value of a house in the 46228 ZIP code area with the following characteristics: 1900 sqft, 0.65 acres lot, 3 bedrooms, 3 bathrooms, 2 garage spots, and 2 story. Conduct a Breusch-Pagan-Godfrey test for heteroscedasticity. What do you conclude? Estimate the above model with heteroscedasticity-consistent (HC) standard errors. What changes compared to the model from Part b? WDI and Multicollinearity: Use the command subset() on the WDI data and to select the variables fertrate, gdp, litrate, lifeexp, and mortrate for the year 2015. Estimate the following model \\[fertrate=\\beta_0+\\beta_1 \\cdot gdp+\\beta_2 \\cdot litrate+\\beta_3 \\cdot lifeexp+\\beta_4 \\cdot mortrate\\] Interpret the results. What do you conclude in terms of statistical significance and the value of \\(R^2\\)? Use the function vif from the package car. What can you say about the issue of multicollinearity in this case? Correct the issue of multicollinearity by adjusting your model. In this exercise, you will estimate the per-capita pork demand as a function of pork prices and the prices of substitutes (beef and chicken) as well as real disposable income. Estimate the following equation and interpret the coefficients. Are the signs of the coefficients what you would expect? \\[\\begin{equation*} \\ln(q_{pork}) = \\beta_0 + \\beta_1 \\cdot \\ln(p_{pork}) + \\beta_2 \\cdot \\ln(p_{chicken}) + \\beta_3 \\cdot \\ln(p_{beef}) + \\beta_4 \\cdot \\ln(rdi) \\end{equation*}\\] "]]
