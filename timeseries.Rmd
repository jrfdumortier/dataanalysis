# Time Series
This chapter introduces time as a component in regression models. Times series represent a temporal ordering of the data. In the cross-section models seen so far, the observations could have been in any order. In this chapter, the ordering of the observations matters. It is usually assumed that there is a stochastic process generating the series and the important aspect is that only a single realization of the stochastic process is observed. The following topics are covered:

- Trend and seasonality
- Finite distributed lag models (including past or lagged independent variables)
- Autoregressive model (including past or lagged dependent variables) also known as dynamic models:
- Forecasting

The static regression model is presented before moving into proper time series analysis. The model is used if the data represents various time periods but the independent variables $x_{i,t}$ have an immediate effect on the dependent variable $y_t$. For example, if the per-capita chicken consumption is a function of the chicken price and real disposable income, then the following model can be estimated using the data in `meatdemand`. 

```{r}
summary(lm(qchicken~rdi+pchicken,data=meatdemand))
```

This assumes that there is an immediate effect of income and chicken price on consumption. 

## Trend and Seasonality
In general, trends in the data can be linear:
$$y_t = \beta_0 + \beta_1 \cdot t + \epsilon_t$$
or exponential: 
$$\ln(y_t) = \beta_0 + \beta_1 \cdot t + \epsilon_t$$
Note that $\beta_1$ in the exponential time trend model is the average annual growth rate (assuming $t$ is in years). Often, data can be decomposed into three components:

- Trend
- Season
- Random component

The seasonal component can be included via dummy variables. For example, for quarterly data the following model can be used:

$$y_t=\beta_0+\delta_1 \cdot Q1_t+\delta_2 \cdot Q2_t+\delta_3 \cdot Q3_t+\beta_1 \cdot x_{1,t}+ \cdots+\beta_k \cdot x_{k,t}+\epsilon_t$$
One seasonal dummy must be dropped. That is, quarterly and yearly data require three and eleven dummy variables, respectively. Consider the `retail` data. 

```{r}
retail$date  = as.Date(retail$date,format="%Y-%m-%d")
retail$month = format(retail$date,"%m")
retail$t     = c(1:nrow(retail))
bhat         = lm(retail~factor(month)+t,data=retail)
retail$fit   = predict.lm(bhat)
ggplot(retail)+
  geom_line(mapping=aes(x=date,y=retail),color="red")+
  geom_line(mapping=aes(x=date,y=fit))
```

A second example considers quarterly `beer` production in Australia. In a first step, the data is converted into a time series:

```{r}
beer = ts(beer$production,start=c(1992,1),end=c(2014,4),frequency=4)
```

Next, the data is plotted using a function from the package [ggfortify](https://cran.r-project.org/web/packages/ggfortify/index.html). Additional documentation using the package is found under [Plotting ts objects](https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_ts.html).
```{r}
autoplot(beer)
```

Note that the `beer` now appears in a different category in the Global Environment, i.e., not under "Data" anymore. The function `tslm` from the package [forecast](https://cran.r-project.org/web/packages/forecast/index.html) is used next. The function fits a linear model including seasonality and a trend component (and a trend-squared component if desired). 

```{r}
bhat = tslm(beer~trend+I(trend^2)+season)
summary(bhat)
```

The package [forecast](https://cran.r-project.org/web/packages/forecast/index.html) can be also be used to forcast:
```{r}
plot(forecast(bhat,h=20))
```

### Practice Exercise

1. Consider the data in `ez`. It contains unemployment claims ($uclms$) from Anderson (IN) before and after the establishments of an enterprise zone (EZ). The purpose of an EZ is to provide incentives for business to invest in area that are usually plagued by economic distress. Execute two regression models with $\ln(uclms)$ as the dependent variable and including a time trend and monthly dummy variables as the independent variables: (1) without $EZ$ and (2) with $EZ$. What can be concluded in terms of trend, seasonality, and the effectiveness of the EZ.

2. Consider the data in `traffic` which contains information on accidents, traffic laws, and other variables for California. The dependent variable of interest is the natural log of $totacc$. In a first regression, use the time trend and monthly dummy variables as the independent variables. In a second regression, add $wkends$, $unem$, $spdlaw$, and $beltlaw$. What can be concluded in terms of trend, seasonality, and the effectiveness of the laws concerning speed limits and belt usage. Why would weekends and unemployment be important?

## Finite Distributed Lag Models
Distributed-lag models include past or lagged independent variables:
$$y_t=\alpha+\beta_0 \cdot x_t+\beta_1 \cdot x_{t-1}+\beta_2 \cdot x_{t-2}+\dots \beta_k \cdot x_{t-k}+\epsilon$$
There are many reasons to include lagged independent variables such as psychological (e.g., it is difficult to break a habit or adjust to a new situation), economic (e.g., contractual obligations), or political (e.g., effectiveness of policy builds up over time) reasons. 

The relationship between income and consumption is used to introduce distributed lag models. Assume the following relationship between income and consumption:
$$C_t=\alpha+\beta_0 \cdot I_t+\beta_1 \cdot I_{t-1}+\beta_2 \cdot I_{t-2}$$
Assume that $\alpha_0=100$, $\beta_0=0.4$, $\beta_1=0.3$, and $\beta_2=0.2$. For this example, the following questions are of interest:

- What is the long-run consumption with \$4,000?
- How does the consumption change if the income increases to \$5000?

Note that the sum of the $\beta_i$'s is 0.9. 
The long-run multiplier (or long-run propensity) is written as:
$$\sum_{i=1}^k \beta_i = \beta_0+\beta_1+\beta_2 + \dots +\beta_k = \beta$$

The question about how may lagged independent variables to include is a difficult to answer question. If the assumption is made that all $\beta_k$ are of the same sign, then the so-called Koyck transformation can be applied:
$$\beta_k = \beta_0 \cdot \lambda^k \quad \text{for} \quad k=0,1,2,\dots$$
Characteristics of this assumption:

- $\lambda <1$ gives less weight to distant values of $\beta$
- Long-run multiplier is finite, i.e.,
    $$\sum_{k=0}^\infty \beta_k = \beta_0 \cdot \left( \frac{1}{1-\lambda} \right)$$

Given the above assumptions, the Koyck transformation can be applied to the regression model. The original model is written as:
$$y_t = \alpha+\beta_0 \cdot x_t+\beta_0 \cdot \lambda x_{t-1}+\beta_0 \cdot \lambda^2 \cdot x_{t-2}+\dots+\epsilon_t$$

The reformulated equation to be estimated is
$$y_t = \alpha \cdot (1-\lambda) + \beta_0 \cdot x_t + \lambda \cdot y_{t-1} + \upsilon_t$$
The notation of the error term has changed from $\epsilon_t$ to $\upsilon_t$ in order to highlight that the terms will be different. 

```{r}
koyck = usdata
koyck$year = substr(koyck$date,1,4)
koyck = aggregate(koyck[c("income","consumption")],FUN=sum,by=list(koyck$year))
colnames(koyck) = c("year","income","consumption")
bhat = lm(formula = consumption ~ income + Lag(consumption), data = koyck)
summary(bhat)
```

## Basic Theoretical Aspects of Time Series
A collection of random variables ordered in time is called a stochastic process. The observed value in a given time period is usually a particular realization of the stochastic process. An important concept is so-called stationary of a time series or stochastic process. A time series is stationary if mean, variance, and covariance between any lagged observation are constant:

- Constant mean: $E(y_t) = \mu$
- Constant variance: $Var(y_t) = \sigma^2$
- Constant covariance: $Cov(y_t,y_{t-h})$ depends on $h$ but not on $t$.

A time series with a trend is usually not stationary (or nonstationary). The concepts behind are explained latter but the issue can be illustrated with a simple simulation. 

```{r}
spurious            = data.frame(x=matrix(0,500,1),y=matrix(0,500,1))
spurious[1,1]       = 1
spurious[1,2]       = 2
for(i in 2:500){
     spurious[i,1] = spurious[i-1,1]+rnorm(1)
     spurious[i,2] = spurious[i-1,2]+rnorm(1)}
bhat = lm(y~x,data=spurious)
summary(bhat)
rm(spurious,bhat)
```

There is no relationship between $y$ and $x$ yet the regression results indicate statistical significance. 







Consider the following so-called autoregressive model. The model depends on lagged terms of the dependent variable:
$$y_t=\alpha+\phi_1 \cdot y_{t-1}+\epsilon_t$$
This is called an AR(1) because the $y$ is lagged by one period. The requirement for a stationary AR(1) is that $|\phi_1|<1$. The properties of the AR(1) process are:

- Mean of $y_t$: $\mu = \frac{\alpha}{1-\phi_1}$
- Variance: $Var(x_t) = \frac{\sigma^2_w}{1-\phi_1^2}$
- Correlation: $\rho_h = \phi^h_1$




## Autoregressive Model
An autoregressive model includes lagged dependent variables. One of the simplest model is an autoregressive model of order 1, i.e., an AR(1) Model
$$y_t = \alpha + \beta \cdot y_{t-1} + \epsilon_t$$
where $\epsilon_t \sim N(0,\sigma^2)$. Consider the data of earthquakes over magnitude 7 in the data set `quakes`: 
```{r, echo=FALSE}
ggplot(quakes,aes(x=time,y=quakes))+geom_line()+theme_bw()+theme(axis.title.x=element_blank())
```

In a first step, a scatter plot is constructed of $Y_{t-1}$ and $y_t$. The easiest way is to use the function `acf`:
```{r,echo=FALSE}
correlation = acf(quakes$quakes)
correlation$acf[2]
```
The correlation coefficicent of 0.25 indicates a weak positive correlation between the number of earthquakes in periods $t$ and $t-1$. Remember that correlation is not causation. The AR(1) model can be estimated with the `lm()` function used previously:
```{r}
summary(lm(quakes~Lag(quakes),data=quakes))
```

Note that although the slope coefficient associated with the lagged term is statistically significant, the R-squared value is very low. 
The third examples uses data on Japanese car production (`jcars`) to illustrate the concept of autocorrelation. The focus is on car production after 1963. 

```{r, echo=FALSE}
jcars = subset(jcars,year>1963)
```

In a first step, the data is visualized using `ggplot`:

```{r}
ggplot(jcars,aes(x=year,y=cars))+geom_line()+theme_bw()+theme(axis.title.x=element_blank())
```

The sample autocorrelation function (ACF) is the correlation between $y_t$ and $y_{t-1}$, $y_{t-2}$, $y_{t-3}$, and so on. It can be written as follows:
$$\rho_j = \frac{Cov(y_t,y_{t-j})}{\sqrt{Var(y_t)\cdot Var(y_{t-j})}}$$

The ACF can be used to identify a possible structure of time series either of the actual time series or the residuals of the regression. The autocorrelation function (ACF) is plotted using the function `acf()` in R.

```{r}
acf(jcars$cars)
```

## Moving Average Models
The moving average model presented in this chapter should not be confused with the concept of moving average (also known as rolling mean) which is familiar to many people. To avoid confusion, the term rolling mean is used for the latter. An example of a rolling mean is plotted below: 

```{r,warning=FALSE}
ggplot(covid,aes(x=date))+
     geom_line(aes(y=newcases/1000))+
     geom_line(aes(y=rollmean(newcases/1000,7,na.pad=TRUE,align="right")),
               color="red")+ylab("New Cases in Thousand")
```

A moving average term in a time series model is a past error (multiplied by a coefficient), e.g., MA(1):
$$x_t = \mu + w_t + \theta_1 \cdot w_{t-1}$$
where $w_t \sim N(0,\sigma_w^2)$. The MA(1) model is written as:
$$x_t = \mu + w_t + \theta_1 \cdot w_{t-1} + \theta_2 \cdot w_{t-2}$$
And the properties of an MA(1) model are as follows:

- $E[x_t] = \mu$
- $Var(x_t) = \sigma^2_w(1+\theta_1^2)$
- ACF is $\rho_1 = \theta_1/(1+\theta_1^2)$ and $\rho_h = 0$ for $h \geq 2$

## Random Walk
A time series with no autocorrelation is called White Noise. Consider the following plot of White Noise:

```{r}
autoplot(ts(rnorm(100)))+ggtitle("White Noise")+theme_bw()
```




Let $\epsilon_t$ be white noise then the random walk without drift is
$$y_t = y_{t-1} + \epsilon_t$$

This is called an autoregressive model of order 1 or AR(1). Example:
$$y_1 = y_0 + \epsilon_1$$
Consider a different model written as 
$$y_2 = y_1 + \epsilon_2 = y_0 + \epsilon_1 + \epsilon_2$$
This is not a stationary process and it can be shown that $E(y_t) = y_0$ and $Var(y_t) = t \cdot \sigma^2$. However
$$y_t - y_{t-1} = \Delta y_t = \epsilon_t$$

Let $\epsilon_t$ be white noise then the random walk with drift is
$$y_t = c+y_{t-1} + \epsilon_t$$
where $c$ is the drift parameter. It can be shown that $E(y_t) = y_0 + t \cdot c$ and $Var(y_t) = t \cdot \sigma^2$. An autoregressive model AR(p) can be written as
$$y_t = c+ \sum^p_{i=1} \phi_p y_{t-p} + \epsilon_t$$

## Forecasting Japanense Car Production
Model 1: Regular OLS Model
$$y_t = \beta_0 + \beta_1 t + \epsilon_t$$
Model 2: Autoregressive Model
$$ y_t = \beta_0 + \beta_1 t + n_t \quad \text{where} \quad n_t = \phi_1 n_{t-1} + \epsilon_t$$
```{r}
summary(lm(cars~year,data=jcars))
```

```{r}
auto.arima(jcars$cars)
```

```{r}
bhat = Arima(jcars$cars,order=c(1,0,0),include.constant=TRUE,include.drift = TRUE)
summary(bhat)
```
```{r}
plot(forecast(bhat))
```

## Exercises

1. ***Souvenir Shop*** (\*\*\*): The data in `souvenirs` represents the sales of a souvenir beach shop in Australia. The sales volume in the shop fluctuates with the number of tourists which are in town. A larger number of tourists is observed in December for the holiday season and for the local surfing festival in March. Over the years, the shop has expanded in terms of items offered and also in store area. 
    a. Plot the data. What do you observe in terms of trend, seasonality, and magnitude of fluctuations?
    b. Estimate a regular OLS Model with $sales$ as the dependent variable and a trend as well as monthly dummy variables as independent variables. 
    c. Plot the observed sales and predicted sales over time.
    d. Repeat the estimation and the plot from (b) and (c) but use the natural log of $sales$, i.e., $\ln(sales)$, as the dependent variable instead.
    e. Compare the two graphs you have generated. What is the difference and what is the reason for the difference? Which form of the dependent variable is more appropriate.
  
2. ***Retail Time Series*** (\*\*\*): Consider the data `retail` and the following model:
     $$\ln(retail_t)=\beta_0+\beta_1 \cdot t +\delta_1 \cdot Q1_t+\delta_2 \cdot Q2_t+\delta_3 \cdot Q3_t+\epsilon_t$$
    a. Estimate the above regression equation, report, and interpret the results. 
    b. How is using $\ln(retail)$ different from $retail$ as the dependent variable? How does it change the interpretation of the coefficients? 
    c. Re-estimate the model but this time using $retail$ as the dependent variable. 
    d. Plot (in the same graph) the observed values and both modeled series. What do you observe? Does one model fit better than the other? If yes, which one? 
  
3. ***WDI Per-Capita Consumption*** (\*\*): The data `wdi` includes per-capita income ($gdp$), overall household consumption ($consumption$), and $population$. For a country of your choice, do the following:
    a. Subset the data such that you are only left with the country of your choice. 
    b. Create a new column which contains the consumption per capita, i.e., consumption divided by population.
    c. Make sure that there are at least 10 years of observations for the country of your choice. 
    d. Estimate a Koyck-Model with consumption as the dependent variable and per-capita consumption as the dependent variable.

4. ***Cardiovascular Mortality*** (\*\*\*): Consider the data in `cmort` which represents weekly data of cardiovascular mortality in Los Angeles over the period 1970 to 1979. 
    a. Estimate an AR(2) model of the following form to the data:
      $$y_t = \alpha + \beta_1 \cdot y_{t-1} + \beta_2 \cdot y_{t-2} + \epsilon_t$$
      Make sure to use the function `ar.ols()` with the options `deman=FALSE` and `intercept=TRUE`.
    b. Given the above model, what are the predicted values for the last four weeks of data. Compare the forecast to the observed values. Is it a good fit?